{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AWS EDA Slurm Cluster View on GitHub Pages This repository contains an AWS Cloud Development Kit (CDK) application that creates a SLURM cluster that is suitable for running production EDA workloads on AWS. Key features are: Automatic scaling of AWS EC2 instances based on demand Use any AWS EC2 instance type including Graviton2 Use of spot instances Batch and interactive partitions (queues) Managed tool licenses as a consumable resource User and group fair share scheduling SLURM accounting database CloudWatch dashboard Job preemption Multi-cluster federation Manage on-premises compute nodes Configure partitions (queues) and nodes that are always on to support reserved instances RIs and savings plans. AWS Fault Injection Simulator (FIS) templates to test spot terminations Operating System and Processor Architecture Support This SLURM cluster supports the following OSes: Alma Linux 8 Amazon Linux 2 CentOS 7 RedHat 7 and 8 Rocky Linux 8 RedHat stopped supporting CentOS 8, so for a similar RedHat 8 binary compatible distribution we support Alma Linux and Rocky Linux as replacements for CentOS. This SLURM cluster supports both Intel/AMD (x86_64) based instances and ARM Graviton2 (arm64/aarch64) based instances. Graviton 2 instances require Amazon Linux 2, RedHat 8, AlmaLinux 8, or RockyLinux 8 operating systems. RedHat 7 and CentOS 7 do not support Graviton 2. This provides the following different combinations of OS and processor architecture. Alma Linux 8 and arm64 Alma Linux 8 and x86_64 Amazon Linux 2 and arm64 Amazon Linux 2 and x86_64 CentOS 7 and x86_64 RedHat 7 and x86_64 RedHat 8 and arm64 RedHat 8 and x86_64 Rocky Linux 8 and arm64 Rocky Linux 8 and x86_64 Documentation To view the docs, clone the repository and run mkdocs: The docs are in the docs directory. You can view them in an editor or using the mkdocs tool. I recommend installing mkdocs in a python virtual environment. python3 -m venv ~/.mkdocs_venv source ~/.mkdocs_venv/bin/activate pip install mkdocs Then run mkdocs. source ~/.mkdocs_venv/bin/activate mkdocs serve & firefox http://127.0.0.1:8000/ & Open a browser to: http://127.0.0.1:8000/ Security See CONTRIBUTING for more information. License This library is licensed under the MIT-0 License. See the LICENSE file.","title":"AWS EDA Slurm Cluster"},{"location":"#aws-eda-slurm-cluster","text":"View on GitHub Pages This repository contains an AWS Cloud Development Kit (CDK) application that creates a SLURM cluster that is suitable for running production EDA workloads on AWS. Key features are: Automatic scaling of AWS EC2 instances based on demand Use any AWS EC2 instance type including Graviton2 Use of spot instances Batch and interactive partitions (queues) Managed tool licenses as a consumable resource User and group fair share scheduling SLURM accounting database CloudWatch dashboard Job preemption Multi-cluster federation Manage on-premises compute nodes Configure partitions (queues) and nodes that are always on to support reserved instances RIs and savings plans. AWS Fault Injection Simulator (FIS) templates to test spot terminations","title":"AWS EDA Slurm Cluster"},{"location":"#operating-system-and-processor-architecture-support","text":"This SLURM cluster supports the following OSes: Alma Linux 8 Amazon Linux 2 CentOS 7 RedHat 7 and 8 Rocky Linux 8 RedHat stopped supporting CentOS 8, so for a similar RedHat 8 binary compatible distribution we support Alma Linux and Rocky Linux as replacements for CentOS. This SLURM cluster supports both Intel/AMD (x86_64) based instances and ARM Graviton2 (arm64/aarch64) based instances. Graviton 2 instances require Amazon Linux 2, RedHat 8, AlmaLinux 8, or RockyLinux 8 operating systems. RedHat 7 and CentOS 7 do not support Graviton 2. This provides the following different combinations of OS and processor architecture. Alma Linux 8 and arm64 Alma Linux 8 and x86_64 Amazon Linux 2 and arm64 Amazon Linux 2 and x86_64 CentOS 7 and x86_64 RedHat 7 and x86_64 RedHat 8 and arm64 RedHat 8 and x86_64 Rocky Linux 8 and arm64 Rocky Linux 8 and x86_64","title":"Operating System and Processor Architecture Support"},{"location":"#documentation","text":"To view the docs, clone the repository and run mkdocs: The docs are in the docs directory. You can view them in an editor or using the mkdocs tool. I recommend installing mkdocs in a python virtual environment. python3 -m venv ~/.mkdocs_venv source ~/.mkdocs_venv/bin/activate pip install mkdocs Then run mkdocs. source ~/.mkdocs_venv/bin/activate mkdocs serve & firefox http://127.0.0.1:8000/ & Open a browser to: http://127.0.0.1:8000/","title":"Documentation"},{"location":"#security","text":"See CONTRIBUTING for more information.","title":"Security"},{"location":"#license","text":"This library is licensed under the MIT-0 License. See the LICENSE file.","title":"License"},{"location":"CONTRIBUTING/","text":"Contributing Guidelines Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community. Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution. Reporting Bugs/Feature Requests We welcome you to use the GitHub issue tracker to report bugs or suggest features. When filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful: A reproducible test case or series of steps The version of our code being used Any modifications you've made relevant to the bug Anything unusual about your environment or deployment Contributing via Pull Requests Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that: You are working against the latest source on the main branch. You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already. You open an issue to discuss any significant work - we would hate for your time to be wasted. To send us a pull request, please: Fork the repository. Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change. Ensure local tests pass. Commit to your fork using clear commit messages. Send us a pull request, answering any default questions in the pull request interface. Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation. GitHub provides additional document on forking a repository and creating a pull request . Finding contributions to work on Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start. Code of Conduct This project has adopted the Amazon Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments. Security issue notifications If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page . Please do not create a public github issue. Licensing See the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution.","title":"Contributing Guidelines"},{"location":"CONTRIBUTING/#contributing-guidelines","text":"Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community. Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.","title":"Contributing Guidelines"},{"location":"CONTRIBUTING/#reporting-bugsfeature-requests","text":"We welcome you to use the GitHub issue tracker to report bugs or suggest features. When filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful: A reproducible test case or series of steps The version of our code being used Any modifications you've made relevant to the bug Anything unusual about your environment or deployment","title":"Reporting Bugs/Feature Requests"},{"location":"CONTRIBUTING/#contributing-via-pull-requests","text":"Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that: You are working against the latest source on the main branch. You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already. You open an issue to discuss any significant work - we would hate for your time to be wasted. To send us a pull request, please: Fork the repository. Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change. Ensure local tests pass. Commit to your fork using clear commit messages. Send us a pull request, answering any default questions in the pull request interface. Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation. GitHub provides additional document on forking a repository and creating a pull request .","title":"Contributing via Pull Requests"},{"location":"CONTRIBUTING/#finding-contributions-to-work-on","text":"Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start.","title":"Finding contributions to work on"},{"location":"CONTRIBUTING/#code-of-conduct","text":"This project has adopted the Amazon Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments.","title":"Code of Conduct"},{"location":"CONTRIBUTING/#security-issue-notifications","text":"If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page . Please do not create a public github issue.","title":"Security issue notifications"},{"location":"CONTRIBUTING/#licensing","text":"See the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution.","title":"Licensing"},{"location":"debug/","text":"Debug Slurm Controller If slurm commands hang, then it's likely a problem with the Slurm controller. Connect to the slurmctl instance using SSM Manager and switch to the root user. sudo su The first thing to do is to ensure that the Slurm controller daemon is running: systemctl status slurmctld If it isn't then first check for errors in the user data script. The following command will show the output: grep cloud-init /var/log/messages | less The most common problem is that the ansible playbook failed. Check the ansible log file to see what failed. less /var/log/ansible.log The following command will rerun the user data. It will download the playbooks from the S3 deployment bucket and then run it to configure the instance. /var/lib/cloud/instance/scripts/part-001 If the problem is with the ansible playbook, then you can edit it in /root/playbooks and then run your modified playbook by running the following command. /root/slurmctl_config.sh The daemon may also be failing because of soem other error. Check the slurmctld.log for errors. Log Files Logfile Description /var/log/ansible.log Ansible logfile /var/log/slurm/cloudwatch.log Logfile for the script that uploads CloudWatch events. /var/log/slurm/slurmctld.log slurmctld logfile /var/log/slurm/power_save.log Slurm plugin logfile with power saving scripts that start, stop, and terminated instances. /var/log/slurm/terminate_old_instances.log Logfile for the script that terminates stopped instances. Slurm Accounting Database If you are having problems with the slurm accounting database connect to the slurmdbd instance using SSM Manager. Check for cloud-init and ansible errors the same way as for the slurmctl instance. Also check the slurmdbd.log for errors. Log Files Logfile Description /var/log/ansible.log Ansible logfile /var/log/slurm/slurmdbd.log slurmctld logfile Compute Nodes If there are problems with the compute nodes, connect to them using SSM Manager. Check for cloud-init errors the same way as for the slurmctl instance. The compute nodes do not run ansible; their AMIs are configured using ansible. Also check the slurmd.log . Check that the slurm daemon is running. systemctl status slurmd Log Files Logfile Description /var/log/slurm/slurmd.log slurmctld logfile Job Stuck in Pending State You can use scontrol to get detailed information about a job. scontrol show job *jobid* Job Stuck in Completing State When a node starts it reports it's number of cores and free memory to the controller. If the memory is less than in slurm_node.conf then the controller will mark the node as invalid. You can confirm this by searching for the node in /var/log/slurm/slurmctld.log on the controller. If this happens, fix the memory in slurm_nodes.conf and restart slurmctld. systemctl restart slurmctld Then reboot the node. Another cause of this is a hung process on the compute node. To clear this out, connect to the slurm controller and mark the node down, resume, and then idle. scontrol update node NODENAME state=DOWN reason=hung scontrol update node NODENAME state=RESUME scontrol update node NODENAME state=IDLE","title":"Debug"},{"location":"debug/#debug","text":"","title":"Debug"},{"location":"debug/#slurm-controller","text":"If slurm commands hang, then it's likely a problem with the Slurm controller. Connect to the slurmctl instance using SSM Manager and switch to the root user. sudo su The first thing to do is to ensure that the Slurm controller daemon is running: systemctl status slurmctld If it isn't then first check for errors in the user data script. The following command will show the output: grep cloud-init /var/log/messages | less The most common problem is that the ansible playbook failed. Check the ansible log file to see what failed. less /var/log/ansible.log The following command will rerun the user data. It will download the playbooks from the S3 deployment bucket and then run it to configure the instance. /var/lib/cloud/instance/scripts/part-001 If the problem is with the ansible playbook, then you can edit it in /root/playbooks and then run your modified playbook by running the following command. /root/slurmctl_config.sh The daemon may also be failing because of soem other error. Check the slurmctld.log for errors.","title":"Slurm Controller"},{"location":"debug/#log-files","text":"Logfile Description /var/log/ansible.log Ansible logfile /var/log/slurm/cloudwatch.log Logfile for the script that uploads CloudWatch events. /var/log/slurm/slurmctld.log slurmctld logfile /var/log/slurm/power_save.log Slurm plugin logfile with power saving scripts that start, stop, and terminated instances. /var/log/slurm/terminate_old_instances.log Logfile for the script that terminates stopped instances.","title":"Log Files"},{"location":"debug/#slurm-accounting-database","text":"If you are having problems with the slurm accounting database connect to the slurmdbd instance using SSM Manager. Check for cloud-init and ansible errors the same way as for the slurmctl instance. Also check the slurmdbd.log for errors.","title":"Slurm Accounting Database"},{"location":"debug/#log-files_1","text":"Logfile Description /var/log/ansible.log Ansible logfile /var/log/slurm/slurmdbd.log slurmctld logfile","title":"Log Files"},{"location":"debug/#compute-nodes","text":"If there are problems with the compute nodes, connect to them using SSM Manager. Check for cloud-init errors the same way as for the slurmctl instance. The compute nodes do not run ansible; their AMIs are configured using ansible. Also check the slurmd.log . Check that the slurm daemon is running. systemctl status slurmd","title":"Compute Nodes"},{"location":"debug/#log-files_2","text":"Logfile Description /var/log/slurm/slurmd.log slurmctld logfile","title":"Log Files"},{"location":"debug/#job-stuck-in-pending-state","text":"You can use scontrol to get detailed information about a job. scontrol show job *jobid*","title":"Job Stuck in Pending State"},{"location":"debug/#job-stuck-in-completing-state","text":"When a node starts it reports it's number of cores and free memory to the controller. If the memory is less than in slurm_node.conf then the controller will mark the node as invalid. You can confirm this by searching for the node in /var/log/slurm/slurmctld.log on the controller. If this happens, fix the memory in slurm_nodes.conf and restart slurmctld. systemctl restart slurmctld Then reboot the node. Another cause of this is a hung process on the compute node. To clear this out, connect to the slurm controller and mark the node down, resume, and then idle. scontrol update node NODENAME state=DOWN reason=hung scontrol update node NODENAME state=RESUME scontrol update node NODENAME state=IDLE","title":"Job Stuck in Completing State"},{"location":"deploy/","text":"Deploy the Cluster Configure AWS CLI Credentials You will needs AWS credentials that provide admin access to deploy the cluster. Clone or Download the Repository Clone or download the repository to your system. Subscribe to AWS MarketPlace AMIs Subscribe to the MarketPlace AMIs you will use in your cluster. Examples are: Alma Linux 8 - arm64 Alma Linux 8 - x86_64 AWS FPGA Developer AMI - CentOS 7 -x86_64 AWS FPGA Developer AMI - Amazon Linux 2 - x86_64 CentOS 7 - x86_64 Rocky Linux 8 - arm64 Rocky Linux 8 - x86_64 Create SNS Topic for Error Notifications The Slurm cluster allows you to specify an SNS notification that will be notified when an error is detected. You can provide the ARN for the topic in the config file or on the command line. You can use the SNS notification in various ways. The simplest is to subscribe your email address to the topic so that you get an email when there is an error. You could also use it to trigger a CloudWatch alarm that could be used to trigger a lambda to do automatic remediation or create a support ticket. Quick Minimal Deployment The install script can create a minimal Slurm cluster using the default configuration file in the repository and it will prompt you for the required parameters. You will first need to configure your AWS credentials. cd edaslurmcluster ./install.sh --prompt --cdk-cmd create Install Cloud Development Kit (CDK) (Optional) The install script will attempt to install all of the prerequisites for you. If the install script fails on your system then you can refer to this section for instructions on how to install or update CDK. This cluster uses Cloud Development Kit (CDK) and Python 3 to deploy the cluster. Install packages used by the installer. sudo yum -y install curl gcc-c++ make nfs-utils python3 tcl unzip wget The following link documents how to setup for CDK. Follow the instructions for Python. https://docs.aws.amazon.com/cdk/v2/guide/getting_started.html#getting_started_prerequisites Note that CDK requires a pretty new version of nodejs which you may have to download from, for example, https://nodejs.org/dist/v16.13.1/node-v16.13.1-linux-x64.tar.xz sudo yum -y install wget wget https://nodejs.org/dist/v16.13.1/node-v16.13.1-linux-x64.tar.xz tar -xf node-v16.13.1-linux-x64.tar.xz ~ Add the nodjs bin directory to your path. https://docs.aws.amazon.com/cdk/v2/guide/getting_started.html#getting_started_install Note that the version of aws-cdk changes frequently. The version that has been tested is in the CDK_VERSION variable in the install script. The install script will try to install the prerequisites if they aren't already installed. Configuration File The first step in deploying your cluster is to create a configuration file. A default configuration file is found in source/resources/config/default_config.yml . You should create a new config file and update the parameters for your cluster. The schema for the config file along with its default values can be found in source/cdk/config_schema.py . The schema is defined in python, but the actual config file should be in yaml format. The following are key parameters that you will need to update. If you do not have the required parameters in your config file then the installer script will fail unless you specify the --prompt option. You should save your selections in the config file. Parameter Description Valid Values Default StackName The cloudformation stack that will deploy the cluster. None VpcId The vpc where the cluster will be deployed. vpc-* None Region Region where VPC is located $AWS_DEFAULT_REGION SshKeyPair EC2 Keypair to use for instances None slurm/SubmitterSecurityGroupIds Existing security groups that can submit to the cluster. For SOCA this is the ComputeNodeSG* resource. sg-* None ErrorSnsTopicArn ARN of an SNS topic that will be notified of errors arn:aws:sns:{{region}}:{AccountId}:{TopicName} None The defaults for the following parameters are generally acceptable, but may be modified based on your requirements. Parameter Description Valid Values Default slurm/SlurmDbd Create a slurmdbd instance connected to an RDS Serverless database. No database. InstanceConfig Configures the instance families and types that the cluster can use. See default_config.yml NumberOfControllers For high availability you can have 2 or 3 controllers. 1-3 1 SuspendAction What to do to an idle instance. Stopped instances will restart faster, but still incur EBS charges while stopped. stop or terminate stop MaxStoppedDuration You can configure how long instances can be stopped before they are automatically terminated. The default is set to 1 hour. This is checked at least hourly. The format uses the ISO 8601 duration format. PnYnMnDTnHnMnS P0Y0M0DT1H0M0S CloudWatchPeriod Default: 5. Set to 1 for finer metric resolution. Configure the Compute Instances The InstanceConfig configuration parameter configures the base operating systems, CPU architectures, instance families, and instance types that the SLURM cluster should support. The supported OSes and CPU architectures are: Base OS CPU Architectures Alma Linux 8 x86_64, arm64 Amazon Linux 2 x86_64, arm64 CentOS 7 x86_64 RedHat 7 x86_64 RedHat 8 x86_64, arm64 Rocky Linux 8 x86_64, arm64 You can exclude instances types by family or specific instance type. By default the InstanceConfig excludes older generation instance families. You can include instances by family or specific instance type. If no includes are specified then all non-excluded instance types will be used. You can also choose to only include the largest instance size within a family. The advantage of using the max instance size is that jobs running on the instance have the highest network bandwidth for that family and fewer instances are required to run the same number of jobs. This may help jobs run faster and allow jobs to wait less time for a new instance to start. The disadvantage is higher cost if the instance is lightly loaded. The default InstanceConfig includes all supported base OSes and architectures and burstable and general purpose instance types. Note that instance types and families are python regular expressions. InstanceConfig: DefaultPartition: CentOS_7_x86_64 BaseOsArchitecture: AlmaLinux: 8: [x86_64, arm64] CentOS: 7: [x86_64] Amazon: {2: [x86_64, arm64]} RedHat: 7: [x86_64] 8: [x86_64, arm64] Include: MaxSizeOnly: false InstanceFamilies: - t3.* - t4g - m5.* InstanceTypes: [] Exclude: InstanceTypes: - '.+\\.(micro|nano)' # Not enough memory - '.*\\.metal' The following InstanceConfig configures instance types recommended for EDA workloads running on CentOS. InstanceConfig: DefaultPartition: CentOS_7_x86_64 BaseOsArchitecture: AlmaLinux: {8: [x86_64, arm64]} CentOS: 7: [x86_64] Include: MaxSizeOnly: false InstanceFamilies: - c5.* - c6g.* - f1 - m5.* - m6g.* - r5.* - r6g.* - x2gd - z1d InstanceTypes: [] Exclude: InstanceTypes: - '.*\\.metal' Update to Latest Base Operating System AMIs (Optional) The default configuration includes the latest AMIs that have been tested. If you want to use the latest base OS AMIs, then configure your AWS cli credentials and run the following script. Note : Updating the AMIs to a newer version may break deployment if repositories and package versions have changed from the tested version. source/create-ami-map.py > source/resources/config/ami_map.yml Use Your Own AMIs (Optional) You may already have base AMIs that are configured for your environment. To use them update the SlurmNodeAmis configuration parameter. The parameter is a map with the keys being the region, base OS, and CPU architecture as the keys. So, for example, to use a custom CentOS 7 AMI in the us-east-1 region you would have: slurm: SlurmNodeAmis: us-east-1: CentOS: 7: x86_64: ami-xxxxxxxxxxxxxxxxx Another example is to use the AWS FPGA Developer AMI as the base AMI for your compute nodes so that you can use the Xilinx Vivado tools for FPGA development for AWS F1 instances. By default the EBS volumes are created with the same sizes as in the AMI. You can increase the size of the root EBS volume as shown in this example. This is useful if the root volume needs additional space to install additional packages or tools. SlurmNodeAmis: # Customized AMIs with file system mounts, packages, etc. configured. BaseAmis: us-east-1: Amazon: {2: {x86_64: {ImageId: ami-0efdec76678df9a64, RootDeviceSize: '+5'}}} CentOS: {7: {x86_64: {ImageId: ami-02155c6289e76719a, RootDeviceSize: '+5'}}} Configure Fair Share Scheduling (Optional) SLURM supports fair share scheduling , but it requires the fair share policy to be configured. By default, all users will be put into a default group that has a low fair share. The configuration file is at source/resources/playbooks/roles/SlurmCtl/templates/tools/slurm/etc/accounts.yml.example in the repository and is deployed to /opt/slurm/{{ClusterName}}/conf/accounts.yml . The file is a simple yaml file that allows you to configure groups, the users that belong to the group, and a fair share weight for the group. Refer to the SLURM documentation for details on how the fair share weight is calculated. The scheduler can be configured so that users who aren't getting their fair share of resources get higher priority. The following shows 3 top level groups. Note that the fairshare weights aren't a percentage. They are just a relative weight. In this example, the projects have 9 times higher weight than the jenkins group. jenkins: fairshare: 10 users: - jenkins project1: fairshare: 90 project2: fairshare: 90 The allocation of top level groups can be further subdivided to control the relative priority of jobs within that group. For example, a project may have design verification (dv), rtl design (rtl), physical design (dv), and formal verification (fv) teams. The following example shows how the project's allocation can be prioritized for the different teams. If a group is using more than it's fair share then its jobs will have lower priority than jobs whose users aren't getting their fair share. project1-dv: parent: project1 fairshare: 80 users: - dvuser1 project1-pd: parent: project1 fairshare: 10 users: - pduser1 project1-rtl: parent: project1 fairshare: 10 users: - rtluser1 project1-fv: parent: project1 fairshare: 10 users: - fvuser1 The scheduler uses the priority/multifactor plugin to calculate job priorities. Fair share is just one of the factors. Read the Multifactor Priority Plugin documentation for the details. This is the default configuration in slurm.conf. The partition weight is set the highest so that jobs in the interactive partition always have the highest priority. Fairshare and QOS are the next highest weighted factors. The next factor is the job age, which means all else being equal the jobs run in FIFO order with the jobs that have been waiting the longest getting higher priority. PriorityType=priority/multifactor PriorityWeightPartition=100000 PriorityWeightFairshare=10000 PriorityWeightQOS=10000 PriorityWeightAge=1000 PriorityWeightAssoc=0 PriorityWeightJobSize=0 These weights can be adjusted based on your needs to control job priorities. Configure Licenses SLURM supports configuring licenses as a consumable resource . It will keep track of how many running jobs are using a license and when no more licenses are available then jobs will stay pending in the queue until a job completes and frees up a license. Combined with the fairshare algorithm, this can prevent users from monopolizing licenses and preventing others from being able to run their jobs. The configuration file is at source/resources/playbooks/roles/SlurmCtl/templates/tools/slurm/etc/accounts.yml.example in the repository and is deployed to /opt/slurm/{{ClusterName}}/conf/accounts.yml . The example configuration shows how the number of licenses can be configured as just a comma separated list. In this example, the cluster will manage 800 vcs licenses and 1 ansys license. Users must request a license using the -L or --licenses options. Licenses=vcs:800,ansys:1 Create the Cluster To install the cluster run the install script. You can override some parameters in the config file with command line arguments, however it is better to specify all of the parameters in the config file. ./install.sh --config-file <config-file> --stack-name <stack-name> --cdk-cmd create Use the Cluster Configuring your environment for users requires root privileges. The configuration commands are found in the outputs of the SLURM cloudformation stack. Configure SLURM Users and Groups The SLURM cluster needs to configure the users and groups of your environment. For efficiency, it does this by capturing the users and groups from your environment and saves them in a json file. When the compute nodes start they create local unix users and groups using this json file. Choose a single instance in your VPC that will always be running and that is joined to a domain so that it can list all users and groups. For SOCA this would be the Scheduler instance. Connect to that instance and run the commands in the MountCommand and ConfigureSyncSlurmUsersGroups outputs of the SLURM stack. These commands will mount the SLURM file system at /opt/slurm/{{ClusterName}} and then create a cron job that runs every 5 minutes and updates /opt/slurm/{{ClusterName}}/config/users_groups.json . Configure SLURM Submitter Instances Instances that need to submit to SLURM need to have their security group IDs in the SubmitterSecurityGroupIds configuration parameter so that the security groups allow communication between the submitter instances and the SLURM cluster. They also need to be configured by mounting the file system with the SLURM tools and configuring their environment. Connect to the submitter instance and run the commands in the MountCommand and ConfigureSubmitterCommand outputs of the SLURM stack. If all users need to use the cluster then it is probably best to create a custom AMI that is configured with the configuration commands. Run Your First Job Run the following command in a shell to configure your environment to use your slurm cluster. module load {{ClusterName}} To submit a job run the following command. sbatch /opt/slurm/$SLURM_CLUSTER_NAME/test/job_simple_array.sh To check the status run the following command. squeue To open an interactive shell on a slurm node. srun --pty /bin/bash Slurm Documentation https://slurm.schedmd.com","title":"Deploy the Cluster"},{"location":"deploy/#deploy-the-cluster","text":"","title":"Deploy the Cluster"},{"location":"deploy/#configure-aws-cli-credentials","text":"You will needs AWS credentials that provide admin access to deploy the cluster.","title":"Configure AWS CLI Credentials"},{"location":"deploy/#clone-or-download-the-repository","text":"Clone or download the repository to your system.","title":"Clone or Download the Repository"},{"location":"deploy/#subscribe-to-aws-marketplace-amis","text":"Subscribe to the MarketPlace AMIs you will use in your cluster. Examples are: Alma Linux 8 - arm64 Alma Linux 8 - x86_64 AWS FPGA Developer AMI - CentOS 7 -x86_64 AWS FPGA Developer AMI - Amazon Linux 2 - x86_64 CentOS 7 - x86_64 Rocky Linux 8 - arm64 Rocky Linux 8 - x86_64","title":"Subscribe to AWS MarketPlace AMIs"},{"location":"deploy/#create-sns-topic-for-error-notifications","text":"The Slurm cluster allows you to specify an SNS notification that will be notified when an error is detected. You can provide the ARN for the topic in the config file or on the command line. You can use the SNS notification in various ways. The simplest is to subscribe your email address to the topic so that you get an email when there is an error. You could also use it to trigger a CloudWatch alarm that could be used to trigger a lambda to do automatic remediation or create a support ticket.","title":"Create SNS Topic for Error Notifications"},{"location":"deploy/#quick-minimal-deployment","text":"The install script can create a minimal Slurm cluster using the default configuration file in the repository and it will prompt you for the required parameters. You will first need to configure your AWS credentials. cd edaslurmcluster ./install.sh --prompt --cdk-cmd create","title":"Quick Minimal Deployment"},{"location":"deploy/#install-cloud-development-kit-cdk-optional","text":"The install script will attempt to install all of the prerequisites for you. If the install script fails on your system then you can refer to this section for instructions on how to install or update CDK. This cluster uses Cloud Development Kit (CDK) and Python 3 to deploy the cluster. Install packages used by the installer. sudo yum -y install curl gcc-c++ make nfs-utils python3 tcl unzip wget The following link documents how to setup for CDK. Follow the instructions for Python. https://docs.aws.amazon.com/cdk/v2/guide/getting_started.html#getting_started_prerequisites Note that CDK requires a pretty new version of nodejs which you may have to download from, for example, https://nodejs.org/dist/v16.13.1/node-v16.13.1-linux-x64.tar.xz sudo yum -y install wget wget https://nodejs.org/dist/v16.13.1/node-v16.13.1-linux-x64.tar.xz tar -xf node-v16.13.1-linux-x64.tar.xz ~ Add the nodjs bin directory to your path. https://docs.aws.amazon.com/cdk/v2/guide/getting_started.html#getting_started_install Note that the version of aws-cdk changes frequently. The version that has been tested is in the CDK_VERSION variable in the install script. The install script will try to install the prerequisites if they aren't already installed.","title":"Install Cloud Development Kit (CDK) (Optional)"},{"location":"deploy/#configuration-file","text":"The first step in deploying your cluster is to create a configuration file. A default configuration file is found in source/resources/config/default_config.yml . You should create a new config file and update the parameters for your cluster. The schema for the config file along with its default values can be found in source/cdk/config_schema.py . The schema is defined in python, but the actual config file should be in yaml format. The following are key parameters that you will need to update. If you do not have the required parameters in your config file then the installer script will fail unless you specify the --prompt option. You should save your selections in the config file. Parameter Description Valid Values Default StackName The cloudformation stack that will deploy the cluster. None VpcId The vpc where the cluster will be deployed. vpc-* None Region Region where VPC is located $AWS_DEFAULT_REGION SshKeyPair EC2 Keypair to use for instances None slurm/SubmitterSecurityGroupIds Existing security groups that can submit to the cluster. For SOCA this is the ComputeNodeSG* resource. sg-* None ErrorSnsTopicArn ARN of an SNS topic that will be notified of errors arn:aws:sns:{{region}}:{AccountId}:{TopicName} None The defaults for the following parameters are generally acceptable, but may be modified based on your requirements. Parameter Description Valid Values Default slurm/SlurmDbd Create a slurmdbd instance connected to an RDS Serverless database. No database. InstanceConfig Configures the instance families and types that the cluster can use. See default_config.yml NumberOfControllers For high availability you can have 2 or 3 controllers. 1-3 1 SuspendAction What to do to an idle instance. Stopped instances will restart faster, but still incur EBS charges while stopped. stop or terminate stop MaxStoppedDuration You can configure how long instances can be stopped before they are automatically terminated. The default is set to 1 hour. This is checked at least hourly. The format uses the ISO 8601 duration format. PnYnMnDTnHnMnS P0Y0M0DT1H0M0S CloudWatchPeriod Default: 5. Set to 1 for finer metric resolution.","title":"Configuration File"},{"location":"deploy/#configure-the-compute-instances","text":"The InstanceConfig configuration parameter configures the base operating systems, CPU architectures, instance families, and instance types that the SLURM cluster should support. The supported OSes and CPU architectures are: Base OS CPU Architectures Alma Linux 8 x86_64, arm64 Amazon Linux 2 x86_64, arm64 CentOS 7 x86_64 RedHat 7 x86_64 RedHat 8 x86_64, arm64 Rocky Linux 8 x86_64, arm64 You can exclude instances types by family or specific instance type. By default the InstanceConfig excludes older generation instance families. You can include instances by family or specific instance type. If no includes are specified then all non-excluded instance types will be used. You can also choose to only include the largest instance size within a family. The advantage of using the max instance size is that jobs running on the instance have the highest network bandwidth for that family and fewer instances are required to run the same number of jobs. This may help jobs run faster and allow jobs to wait less time for a new instance to start. The disadvantage is higher cost if the instance is lightly loaded. The default InstanceConfig includes all supported base OSes and architectures and burstable and general purpose instance types. Note that instance types and families are python regular expressions. InstanceConfig: DefaultPartition: CentOS_7_x86_64 BaseOsArchitecture: AlmaLinux: 8: [x86_64, arm64] CentOS: 7: [x86_64] Amazon: {2: [x86_64, arm64]} RedHat: 7: [x86_64] 8: [x86_64, arm64] Include: MaxSizeOnly: false InstanceFamilies: - t3.* - t4g - m5.* InstanceTypes: [] Exclude: InstanceTypes: - '.+\\.(micro|nano)' # Not enough memory - '.*\\.metal' The following InstanceConfig configures instance types recommended for EDA workloads running on CentOS. InstanceConfig: DefaultPartition: CentOS_7_x86_64 BaseOsArchitecture: AlmaLinux: {8: [x86_64, arm64]} CentOS: 7: [x86_64] Include: MaxSizeOnly: false InstanceFamilies: - c5.* - c6g.* - f1 - m5.* - m6g.* - r5.* - r6g.* - x2gd - z1d InstanceTypes: [] Exclude: InstanceTypes: - '.*\\.metal'","title":"Configure the Compute Instances"},{"location":"deploy/#update-to-latest-base-operating-system-amis-optional","text":"The default configuration includes the latest AMIs that have been tested. If you want to use the latest base OS AMIs, then configure your AWS cli credentials and run the following script. Note : Updating the AMIs to a newer version may break deployment if repositories and package versions have changed from the tested version. source/create-ami-map.py > source/resources/config/ami_map.yml","title":"Update to Latest Base Operating System AMIs (Optional)"},{"location":"deploy/#use-your-own-amis-optional","text":"You may already have base AMIs that are configured for your environment. To use them update the SlurmNodeAmis configuration parameter. The parameter is a map with the keys being the region, base OS, and CPU architecture as the keys. So, for example, to use a custom CentOS 7 AMI in the us-east-1 region you would have: slurm: SlurmNodeAmis: us-east-1: CentOS: 7: x86_64: ami-xxxxxxxxxxxxxxxxx Another example is to use the AWS FPGA Developer AMI as the base AMI for your compute nodes so that you can use the Xilinx Vivado tools for FPGA development for AWS F1 instances. By default the EBS volumes are created with the same sizes as in the AMI. You can increase the size of the root EBS volume as shown in this example. This is useful if the root volume needs additional space to install additional packages or tools. SlurmNodeAmis: # Customized AMIs with file system mounts, packages, etc. configured. BaseAmis: us-east-1: Amazon: {2: {x86_64: {ImageId: ami-0efdec76678df9a64, RootDeviceSize: '+5'}}} CentOS: {7: {x86_64: {ImageId: ami-02155c6289e76719a, RootDeviceSize: '+5'}}}","title":"Use Your Own AMIs (Optional)"},{"location":"deploy/#configure-fair-share-scheduling-optional","text":"SLURM supports fair share scheduling , but it requires the fair share policy to be configured. By default, all users will be put into a default group that has a low fair share. The configuration file is at source/resources/playbooks/roles/SlurmCtl/templates/tools/slurm/etc/accounts.yml.example in the repository and is deployed to /opt/slurm/{{ClusterName}}/conf/accounts.yml . The file is a simple yaml file that allows you to configure groups, the users that belong to the group, and a fair share weight for the group. Refer to the SLURM documentation for details on how the fair share weight is calculated. The scheduler can be configured so that users who aren't getting their fair share of resources get higher priority. The following shows 3 top level groups. Note that the fairshare weights aren't a percentage. They are just a relative weight. In this example, the projects have 9 times higher weight than the jenkins group. jenkins: fairshare: 10 users: - jenkins project1: fairshare: 90 project2: fairshare: 90 The allocation of top level groups can be further subdivided to control the relative priority of jobs within that group. For example, a project may have design verification (dv), rtl design (rtl), physical design (dv), and formal verification (fv) teams. The following example shows how the project's allocation can be prioritized for the different teams. If a group is using more than it's fair share then its jobs will have lower priority than jobs whose users aren't getting their fair share. project1-dv: parent: project1 fairshare: 80 users: - dvuser1 project1-pd: parent: project1 fairshare: 10 users: - pduser1 project1-rtl: parent: project1 fairshare: 10 users: - rtluser1 project1-fv: parent: project1 fairshare: 10 users: - fvuser1 The scheduler uses the priority/multifactor plugin to calculate job priorities. Fair share is just one of the factors. Read the Multifactor Priority Plugin documentation for the details. This is the default configuration in slurm.conf. The partition weight is set the highest so that jobs in the interactive partition always have the highest priority. Fairshare and QOS are the next highest weighted factors. The next factor is the job age, which means all else being equal the jobs run in FIFO order with the jobs that have been waiting the longest getting higher priority. PriorityType=priority/multifactor PriorityWeightPartition=100000 PriorityWeightFairshare=10000 PriorityWeightQOS=10000 PriorityWeightAge=1000 PriorityWeightAssoc=0 PriorityWeightJobSize=0 These weights can be adjusted based on your needs to control job priorities.","title":"Configure Fair Share Scheduling (Optional)"},{"location":"deploy/#configure-licenses","text":"SLURM supports configuring licenses as a consumable resource . It will keep track of how many running jobs are using a license and when no more licenses are available then jobs will stay pending in the queue until a job completes and frees up a license. Combined with the fairshare algorithm, this can prevent users from monopolizing licenses and preventing others from being able to run their jobs. The configuration file is at source/resources/playbooks/roles/SlurmCtl/templates/tools/slurm/etc/accounts.yml.example in the repository and is deployed to /opt/slurm/{{ClusterName}}/conf/accounts.yml . The example configuration shows how the number of licenses can be configured as just a comma separated list. In this example, the cluster will manage 800 vcs licenses and 1 ansys license. Users must request a license using the -L or --licenses options. Licenses=vcs:800,ansys:1","title":"Configure Licenses"},{"location":"deploy/#create-the-cluster","text":"To install the cluster run the install script. You can override some parameters in the config file with command line arguments, however it is better to specify all of the parameters in the config file. ./install.sh --config-file <config-file> --stack-name <stack-name> --cdk-cmd create","title":"Create the Cluster"},{"location":"deploy/#use-the-cluster","text":"Configuring your environment for users requires root privileges. The configuration commands are found in the outputs of the SLURM cloudformation stack.","title":"Use the Cluster"},{"location":"deploy/#configure-slurm-users-and-groups","text":"The SLURM cluster needs to configure the users and groups of your environment. For efficiency, it does this by capturing the users and groups from your environment and saves them in a json file. When the compute nodes start they create local unix users and groups using this json file. Choose a single instance in your VPC that will always be running and that is joined to a domain so that it can list all users and groups. For SOCA this would be the Scheduler instance. Connect to that instance and run the commands in the MountCommand and ConfigureSyncSlurmUsersGroups outputs of the SLURM stack. These commands will mount the SLURM file system at /opt/slurm/{{ClusterName}} and then create a cron job that runs every 5 minutes and updates /opt/slurm/{{ClusterName}}/config/users_groups.json .","title":"Configure SLURM Users and Groups"},{"location":"deploy/#configure-slurm-submitter-instances","text":"Instances that need to submit to SLURM need to have their security group IDs in the SubmitterSecurityGroupIds configuration parameter so that the security groups allow communication between the submitter instances and the SLURM cluster. They also need to be configured by mounting the file system with the SLURM tools and configuring their environment. Connect to the submitter instance and run the commands in the MountCommand and ConfigureSubmitterCommand outputs of the SLURM stack. If all users need to use the cluster then it is probably best to create a custom AMI that is configured with the configuration commands.","title":"Configure SLURM Submitter Instances"},{"location":"deploy/#run-your-first-job","text":"Run the following command in a shell to configure your environment to use your slurm cluster. module load {{ClusterName}} To submit a job run the following command. sbatch /opt/slurm/$SLURM_CLUSTER_NAME/test/job_simple_array.sh To check the status run the following command. squeue To open an interactive shell on a slurm node. srun --pty /bin/bash","title":"Run Your First Job"},{"location":"deploy/#slurm-documentation","text":"https://slurm.schedmd.com","title":"Slurm Documentation"},{"location":"f1-ami/","text":"SLURM AMI Based On FPGA Developer AMI This tutorial shows how to create an AMI based on the AWS FPGA Developer AMI. The FPGA Developer AMI has the Xilinx Vivado tools that can be used free of additional charges when run on AWS EC2 instances to develop FPGA images that can be run on AWS F1 instances. Subscribe To the AMI First subscribe to the FPGA developer AMI in the AWS Marketplace. There are 2 versions, one for CentOS 7 and the other for Amazon Linux 2 . Add the AMI to Your Config File slurm: SlurmNodeAmis: # Customized AMIs with file system mounts, packages, etc. configured. # If these aren't defined then the generic base AMIs are used. # Example in the comment below is the AWS FPGA Developer AMI BaseAmis: us-east-1: Amazon: {2: {x86_64: {ImageId: ami-0efdec76678df9a64, RootDeviceSize: '+5'}}} CentOS: {7: {x86_64: {ImageId: ami-02155c6289e76719a, RootDeviceSize: '+5'}}} Deploy the Cluster With the config updated the AMIs for the compute nodes will be built using the specified base AMIs.","title":"SLURM AMI Based On FPGA Developer AMI"},{"location":"f1-ami/#slurm-ami-based-on-fpga-developer-ami","text":"This tutorial shows how to create an AMI based on the AWS FPGA Developer AMI. The FPGA Developer AMI has the Xilinx Vivado tools that can be used free of additional charges when run on AWS EC2 instances to develop FPGA images that can be run on AWS F1 instances.","title":"SLURM AMI Based On FPGA Developer AMI"},{"location":"f1-ami/#subscribe-to-the-ami","text":"First subscribe to the FPGA developer AMI in the AWS Marketplace. There are 2 versions, one for CentOS 7 and the other for Amazon Linux 2 .","title":"Subscribe To the AMI"},{"location":"f1-ami/#add-the-ami-to-your-config-file","text":"slurm: SlurmNodeAmis: # Customized AMIs with file system mounts, packages, etc. configured. # If these aren't defined then the generic base AMIs are used. # Example in the comment below is the AWS FPGA Developer AMI BaseAmis: us-east-1: Amazon: {2: {x86_64: {ImageId: ami-0efdec76678df9a64, RootDeviceSize: '+5'}}} CentOS: {7: {x86_64: {ImageId: ami-02155c6289e76719a, RootDeviceSize: '+5'}}}","title":"Add the AMI to Your Config File"},{"location":"f1-ami/#deploy-the-cluster","text":"With the config updated the AMIs for the compute nodes will be built using the specified base AMIs.","title":"Deploy the Cluster"},{"location":"federation/","text":"Federation To maximize performance, EDA workloads should run in a single AZ. If you need to run jobs in more than one AZ then you can use the federation feature of Slurm so that you can run jobs on multiple clusters. The config directory has example configuration files that demonstrate how deploy federated cluster into 3 AZs. source/config/slurm_eda_az1.yml source/config/slurm_eda_az2.yml source/config/slurm_eda_az3.yml These clusters should be deployed sequentially. The first cluster creates a cluster and a slurmdbd instance. The other 2 clusters are deployed into their own AZ by configuring the SubnetId of the cluster. They reuse the same slurmdbd instance so that they can reuse a common pool of licenses that is managed by the slurmdbd instance. The config files for the 2nd and 3rd clusters provide the stack names from the others so that the security groups can be updated to allow the required network traffic between the clusters. The following shows an example of the configuration. slurm_eda_az1: Federation: Name: slurmeda FederatedClusterStackNames: [] slurm_eda_az2: Federation: Name: slurmeda FederatedClusterStackNames: - slurmedaaz1 slurm_eda_az3: Federation: Name: slurmeda FederatedClusterStackNames: - slurmedaaz1 - slurmedaaz2","title":"Federation"},{"location":"federation/#federation","text":"To maximize performance, EDA workloads should run in a single AZ. If you need to run jobs in more than one AZ then you can use the federation feature of Slurm so that you can run jobs on multiple clusters. The config directory has example configuration files that demonstrate how deploy federated cluster into 3 AZs. source/config/slurm_eda_az1.yml source/config/slurm_eda_az2.yml source/config/slurm_eda_az3.yml These clusters should be deployed sequentially. The first cluster creates a cluster and a slurmdbd instance. The other 2 clusters are deployed into their own AZ by configuring the SubnetId of the cluster. They reuse the same slurmdbd instance so that they can reuse a common pool of licenses that is managed by the slurmdbd instance. The config files for the 2nd and 3rd clusters provide the stack names from the others so that the security groups can be updated to allow the required network traffic between the clusters. The following shows an example of the configuration. slurm_eda_az1: Federation: Name: slurmeda FederatedClusterStackNames: [] slurm_eda_az2: Federation: Name: slurmeda FederatedClusterStackNames: - slurmedaaz1 slurm_eda_az3: Federation: Name: slurmeda FederatedClusterStackNames: - slurmedaaz1 - slurmedaaz2","title":"Federation"},{"location":"implementation/","text":"Implementation Details Slurm Infrastructure All hosts in the cluster must share a uniform user and group namespace. The munged service must be running before starting any slurm daemons. Directory Structure All of the configuration files, scripts, and logs can be found under the following directory. /opt/slurm/{{ClusterName}} CloudWatch Metrics CloudWatch metrics are published by the following sources, but the code is all in SlurmPlugin.py . Slurm power saving scripts /opt/slurm/{{ClusterName}}/bin/slurm_ec2_resume.py /opt/slurm/{{ClusterName}}/bin/slurm_ec2_resume_fail.py /opt/slurm/{{ClusterName}}/bin/slurm_ec2_stop.py /opt/slurm/{{ClusterName}}/bin/slurm_ec2_terminate.py Spot monitor running on compute nodes /opt/slurm/{{ClusterName}}/bin/spot_monitor.py Cron jobs running on the Slurm controller /opt/slurm/{{ClusterName}}/bin/slurm_ec2_publish_cw.py /opt/slurm/{{ClusterName}}/bin/terminate_old_instances.py Down Node Handling If a node has a problem running jobs then Slurm can mark it DOWN. This includes if the resume script cannot start an instance for any reason include insufficient EC2 capacity. This can create 2 issues. First, if the compute node is running then it is wasting EC2 costs. Second, the node will be unavailable for scheduling which reduces the configured capacity of the cluster. The cluster is configured to periodically check for DOWN nodes so that they aren't left running and wasting compute costs. This is done by /opt/slurm/{{ClusterName}}/bin/slurm_down_nodes_clean.sh . The script is called every day by a systemd service: /etc/systemd/system/slurm_down_nodes_clean.service This service is run at boot and once a day as defined in /etc/systemd/system/slurm_down_nodes_clean.timer Insufficient Capacity Exception (ICE) Handling When Slurm schedules a powered down node it calls the ResumeScript defined in slurm.conf . This is in /opt/slurm/{{ClusterName}}/bin/slurm_ec2_resume.py . The script will attempt to start an EC2 instance and if it receives and InsufficientCapacityException (ICE) then the node will be marked down and Slurm will requeue the job. However, this is inadequate because if there are a large number of instances of that instance type configured then Slurm will schedule them and try to start them with the same result. Eventually all of the powered down nodes will be marked DOWN and depending on the job requirements the job will be allocated to a node with a different instance type or it will fail. This can take a substantial amount of time so SlurmPlugin.py does the following when it receives an ICE. Mark the node as DRAIN so no new jobs are scheduled on it. Find all other nodes of the same type and mark them DOWN so that they won't be scheduled after this node is marked DOWN. Nodes that are running will be left alone. Requeue jobs on the node that failed to resume because of ICE. Mark the node DOWN. Power down the node. This is so that Slurm knows that the node is powered down so that when it is marked IDLE it will be powered up when a job is scheduled on it. A cron job periodically finds all DOWN Slurm nodes, powers them down, and then marks them IDLE so that they can have jobs scheduled on them. This will allow Slurm to attempt to use more nodes of the instance type in the hopes that there is more capacity. If not, then the cycle repeats.","title":"Implementation Details"},{"location":"implementation/#implementation-details","text":"","title":"Implementation Details"},{"location":"implementation/#slurm-infrastructure","text":"All hosts in the cluster must share a uniform user and group namespace. The munged service must be running before starting any slurm daemons.","title":"Slurm Infrastructure"},{"location":"implementation/#directory-structure","text":"All of the configuration files, scripts, and logs can be found under the following directory. /opt/slurm/{{ClusterName}}","title":"Directory Structure"},{"location":"implementation/#cloudwatch-metrics","text":"CloudWatch metrics are published by the following sources, but the code is all in SlurmPlugin.py . Slurm power saving scripts /opt/slurm/{{ClusterName}}/bin/slurm_ec2_resume.py /opt/slurm/{{ClusterName}}/bin/slurm_ec2_resume_fail.py /opt/slurm/{{ClusterName}}/bin/slurm_ec2_stop.py /opt/slurm/{{ClusterName}}/bin/slurm_ec2_terminate.py Spot monitor running on compute nodes /opt/slurm/{{ClusterName}}/bin/spot_monitor.py Cron jobs running on the Slurm controller /opt/slurm/{{ClusterName}}/bin/slurm_ec2_publish_cw.py /opt/slurm/{{ClusterName}}/bin/terminate_old_instances.py","title":"CloudWatch Metrics"},{"location":"implementation/#down-node-handling","text":"If a node has a problem running jobs then Slurm can mark it DOWN. This includes if the resume script cannot start an instance for any reason include insufficient EC2 capacity. This can create 2 issues. First, if the compute node is running then it is wasting EC2 costs. Second, the node will be unavailable for scheduling which reduces the configured capacity of the cluster. The cluster is configured to periodically check for DOWN nodes so that they aren't left running and wasting compute costs. This is done by /opt/slurm/{{ClusterName}}/bin/slurm_down_nodes_clean.sh . The script is called every day by a systemd service: /etc/systemd/system/slurm_down_nodes_clean.service This service is run at boot and once a day as defined in /etc/systemd/system/slurm_down_nodes_clean.timer","title":"Down Node Handling"},{"location":"implementation/#insufficient-capacity-exception-ice-handling","text":"When Slurm schedules a powered down node it calls the ResumeScript defined in slurm.conf . This is in /opt/slurm/{{ClusterName}}/bin/slurm_ec2_resume.py . The script will attempt to start an EC2 instance and if it receives and InsufficientCapacityException (ICE) then the node will be marked down and Slurm will requeue the job. However, this is inadequate because if there are a large number of instances of that instance type configured then Slurm will schedule them and try to start them with the same result. Eventually all of the powered down nodes will be marked DOWN and depending on the job requirements the job will be allocated to a node with a different instance type or it will fail. This can take a substantial amount of time so SlurmPlugin.py does the following when it receives an ICE. Mark the node as DRAIN so no new jobs are scheduled on it. Find all other nodes of the same type and mark them DOWN so that they won't be scheduled after this node is marked DOWN. Nodes that are running will be left alone. Requeue jobs on the node that failed to resume because of ICE. Mark the node DOWN. Power down the node. This is so that Slurm knows that the node is powered down so that when it is marked IDLE it will be powered up when a job is scheduled on it. A cron job periodically finds all DOWN Slurm nodes, powers them down, and then marks them IDLE so that they can have jobs scheduled on them. This will allow Slurm to attempt to use more nodes of the instance type in the hopes that there is more capacity. If not, then the cycle repeats.","title":"Insufficient Capacity Exception (ICE) Handling"},{"location":"mkdocs/","text":"mkdocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"mkdocs"},{"location":"mkdocs/#mkdocs","text":"For full documentation visit mkdocs.org .","title":"mkdocs"},{"location":"mkdocs/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"mkdocs/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"multi-region/","text":"Multi-AZ and Multi-Region Support Set up Route53 private hosted zone. Create A records for all file systems that must be accessed by compute nodes. The Amazon DNS names will NOT be available in other VPCs so cannot use CNAME records; they must be A records with the IP address of the file system. Make sure that hosted zone is connected to all VPCs in all regions that are used by the slurm cluster. Make sure that file system security groups allow access from all slurm VPCs. Set up VPC peering connections between all VPCs. Create routes between the VPCs. All of the compute nodes in the cluster should share the same file directory structure and have network connectivity to all other nodes and to the slurm controller and slurmdbd instances. The slurm file system contains all of the configuration files and executables, so it must be accessible by all nodes. The easiest way to accomplish this is to simply mount the file system across the network. Initial file accesses may be slow, but subsequent accesses should be cached in the nfs clients so it shouldn't be an issue for performance. Working directories can also be mounted across AZs and regions, but the performance implications may be significant. File system performance issues related to network latency can be resolved by using NetApp FlexCache volumes in AZs and regions. Since the file system architecture can be very implementation specific, it is not handled automatically. What is done, is that the required structures are provided to allow you to easily configure the cluster to use whatever file systems and volumes you have created. The main mechanism is the creation of an Amazon Route53 hosted zone in each region with entries for the slurm controllers, the slurmdbd instance, and all file systems used by the cluster. This allows compute node AMIs to be created in the origin region and then copied to the other regions. The DNS entries can be updated after deployment to resolve to local FlexCache volumes if they are created. The other regional resources required to launch instances in other regions are ssh key pairs and security groups. IAM roles are not regional so they can be shared across regions. Regional resources that must be provided: * Subnet IDs * EC2 Keypairs Regional resources that will be created: * Route53 private hosted zone * ComputeNodeSecurityGroup ComputeNodeSecurityGroup","title":"Multi-AZ and Multi-Region Support"},{"location":"multi-region/#multi-az-and-multi-region-support","text":"Set up Route53 private hosted zone. Create A records for all file systems that must be accessed by compute nodes. The Amazon DNS names will NOT be available in other VPCs so cannot use CNAME records; they must be A records with the IP address of the file system. Make sure that hosted zone is connected to all VPCs in all regions that are used by the slurm cluster. Make sure that file system security groups allow access from all slurm VPCs. Set up VPC peering connections between all VPCs. Create routes between the VPCs. All of the compute nodes in the cluster should share the same file directory structure and have network connectivity to all other nodes and to the slurm controller and slurmdbd instances. The slurm file system contains all of the configuration files and executables, so it must be accessible by all nodes. The easiest way to accomplish this is to simply mount the file system across the network. Initial file accesses may be slow, but subsequent accesses should be cached in the nfs clients so it shouldn't be an issue for performance. Working directories can also be mounted across AZs and regions, but the performance implications may be significant. File system performance issues related to network latency can be resolved by using NetApp FlexCache volumes in AZs and regions. Since the file system architecture can be very implementation specific, it is not handled automatically. What is done, is that the required structures are provided to allow you to easily configure the cluster to use whatever file systems and volumes you have created. The main mechanism is the creation of an Amazon Route53 hosted zone in each region with entries for the slurm controllers, the slurmdbd instance, and all file systems used by the cluster. This allows compute node AMIs to be created in the origin region and then copied to the other regions. The DNS entries can be updated after deployment to resolve to local FlexCache volumes if they are created. The other regional resources required to launch instances in other regions are ssh key pairs and security groups. IAM roles are not regional so they can be shared across regions. Regional resources that must be provided: * Subnet IDs * EC2 Keypairs Regional resources that will be created: * Route53 private hosted zone * ComputeNodeSecurityGroup","title":"Multi-AZ and Multi-Region Support"},{"location":"multi-region/#computenodesecuritygroup","text":"","title":"ComputeNodeSecurityGroup"},{"location":"onprem/","text":"On-Premises Integration The slurm cluster can also be configured to manage on-premises compute nodes. The user must configure the on-premises compute nodes and then give the configuration information. On-Premises Network and Compute Nodes The on-prem network must have a CIDR range that doesn't overlap the Slurm cluster's VPC and the two networks need to be connected using VPN or AWS Direct Connect. The on-prem firewall must allow ingress and egress from the VPC. The ports are used to connect to the file systems, slurm controllers, and allow traffic between virtual desktops and compute nodes. Local network DNS must have an entry for the slurm controller or have a forwarding rule to the AWS provided DNS in the Slurm VPC. All of the compute nodes in the cluster, including the on-prem nodes, must have file system mounts that replicate the same directory structure. This can involve mounting filesystems across VPN or Direct Connect or synchronizing file systems using tools like rsync or NetApp FlexCache or SnapMirror. Performance will dictate the architecture of the file system. Slurm Configuration of On-Premises Compute Nodes The slurm cluster's configuration file allows the configuration of on-premises compute nodes. The Slurm cluster will not provision any of the on-prem nodes, network, or firewall, but it will configure the cluster's resources to be used by the on-prem nodes. All that needs to be configured are the configuration file for the on-prem nodes and the CIDR block. InstanceConfig: UseSpot: true DefaultPartition: CentOS_7_x86_64_spot NodesPerInstanceType: 10 BaseOsArchitecture: CentOS: {7: [x86_64]} Include: MaxSizeOnly: false InstanceFamilies: - t3 InstanceTypes: [] Exclude: InstanceFamilies: [] InstanceTypes: - '.+\\.(micro|nano)' # Not enough memory - '.*\\.metal' OnPremComputeNodes: ConfigFile: 'slurm_nodes_on_prem.conf' CIDR: '10.1.0.0/16' slurm_nodes_on_prem.conf # # ON PREMISES COMPUTE NODES # # Config file with list of statically provisioned on-premises compute nodes that # are managed by this cluster. # # These nodes must be addressable on the network and firewalls must allow access on all ports # required by slurm. # # The compute nodes must have mounts that mirror the compute cluster including mounting the slurm file system # or a mirror of it. NodeName=Default State=DOWN NodeName=onprem-c7-x86-t3-2xl-0 NodeAddr=onprem-c7-x86-t3-2xl-0.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-1 NodeAddr=onprem-c7-x86-t3-2xl-1.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-2 NodeAddr=onprem-c7-x86-t3-2xl-2.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-3 NodeAddr=onprem-c7-x86-t3-2xl-3.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-4 NodeAddr=onprem-c7-x86-t3-2xl-4.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-5 NodeAddr=onprem-c7-x86-t3-2xl-5.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-6 NodeAddr=onprem-c7-x86-t3-2xl-6.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-7 NodeAddr=onprem-c7-x86-t3-2xl-7.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-8 NodeAddr=onprem-c7-x86-t3-2xl-8.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-9 NodeAddr=onprem-c7-x86-t3-2xl-9.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 # # # OnPrem Partition # # The is the default partition and includes all nodes from the 1st OS. # PartitionName=onprem Default=YES PriorityTier=20000 Nodes=\\ onprem-c7-x86-t3-2xl-[0-9] # # Always on partitions # SuspendExcParts=onprem Simulating an On-Premises Network Using AWS Create a new VPC with public and private subnets and NAT gateways. To simulate the latency between an AWS region and on-prem you can create the VPC in a different region in your account. The CIDR must not overlap with the Slurm VPC. Create a VPC peering connection to your Slurm VPC and accept the connection in the Slurm VPC. Create routes in the private subnets for the CIDR of the peered VPC and route it to the vpc peering connection. Add the on-prem VPC to the Slurm VPC's Route53 private local zone. Create a Route53 private hosted zone for the on-prem compute nodes and add it to the onprem VPC and the slurm VPC so that onprem compute nodes can be resolved. Copy the Slurm AMIs to the region of the on-prem VPC. Create an instance using the copied AMI. Connect to the instance and confirm that the mount points mounted correctly. You will probably have to change the DNS names for the file systems to IP addresses. I created A records in the Route53 zone for the file systems so that if the IP addresses ever change in the future I can easily update them in one place without having to create a new AMI or updated any instances. Create a new AMI from the instance. Create compute node instances from the new AMI and run the following commands on them get the slurmd daemon running so they can join the slurm cluster. # Instance specific variables hostname=onprem-c7-x86-t3-2xl-0 # Domain specific variables onprem_domain=onprem.com source /etc/profile.d/instance_vars.sh # munge needs to be running before calling scontrol /usr/bin/cp /opt/slurm/$ClusterName/config/munge.key /etc/munge/munge.key systemctl enable munged systemctl start munged ipaddress=$(hostname -I) $SLURM_ROOT/bin/scontrol update nodename=${hostname} nodeaddr=$ipaddress # Set hostname hostname_fqdn=${hostname}.${onprem_domain} if [ $(hostname) != $hostname_fqdn ]; then hostnamectl --static set-hostname $hostname_fqdn hostnamectl --pretty set-hostname $hostname fi if [ -e /opt/slurm/${ClusterName}/config/users_groups.json ] && [ -e /opt/slurm/${ClusterName}/bin/create_users_groups.py ]; then /opt/slurm/${ClusterName}/bin/create_users_groups.py -i /opt/slurm/${ClusterName}/config/users_groups.json fi # Create directory for slurmd.log logs_dir=/opt/slurm/${ClusterName}/logs/nodes/${hostname} if [[ ! -d $logs_dir ]]; then mkdir -p $logs_dir fi if [[ -e /var/log/slurm ]]; then rm -rf /var/log/slurm fi ln -s $logs_dir /var/log/slurm systemctl enable slurmd systemctl start slurmd # Restart so that log file goes to file system systemctl restart spot_monitor","title":"On-Premises Integration"},{"location":"onprem/#on-premises-integration","text":"The slurm cluster can also be configured to manage on-premises compute nodes. The user must configure the on-premises compute nodes and then give the configuration information.","title":"On-Premises Integration"},{"location":"onprem/#on-premises-network-and-compute-nodes","text":"The on-prem network must have a CIDR range that doesn't overlap the Slurm cluster's VPC and the two networks need to be connected using VPN or AWS Direct Connect. The on-prem firewall must allow ingress and egress from the VPC. The ports are used to connect to the file systems, slurm controllers, and allow traffic between virtual desktops and compute nodes. Local network DNS must have an entry for the slurm controller or have a forwarding rule to the AWS provided DNS in the Slurm VPC. All of the compute nodes in the cluster, including the on-prem nodes, must have file system mounts that replicate the same directory structure. This can involve mounting filesystems across VPN or Direct Connect or synchronizing file systems using tools like rsync or NetApp FlexCache or SnapMirror. Performance will dictate the architecture of the file system.","title":"On-Premises Network and Compute Nodes"},{"location":"onprem/#slurm-configuration-of-on-premises-compute-nodes","text":"The slurm cluster's configuration file allows the configuration of on-premises compute nodes. The Slurm cluster will not provision any of the on-prem nodes, network, or firewall, but it will configure the cluster's resources to be used by the on-prem nodes. All that needs to be configured are the configuration file for the on-prem nodes and the CIDR block. InstanceConfig: UseSpot: true DefaultPartition: CentOS_7_x86_64_spot NodesPerInstanceType: 10 BaseOsArchitecture: CentOS: {7: [x86_64]} Include: MaxSizeOnly: false InstanceFamilies: - t3 InstanceTypes: [] Exclude: InstanceFamilies: [] InstanceTypes: - '.+\\.(micro|nano)' # Not enough memory - '.*\\.metal' OnPremComputeNodes: ConfigFile: 'slurm_nodes_on_prem.conf' CIDR: '10.1.0.0/16' slurm_nodes_on_prem.conf # # ON PREMISES COMPUTE NODES # # Config file with list of statically provisioned on-premises compute nodes that # are managed by this cluster. # # These nodes must be addressable on the network and firewalls must allow access on all ports # required by slurm. # # The compute nodes must have mounts that mirror the compute cluster including mounting the slurm file system # or a mirror of it. NodeName=Default State=DOWN NodeName=onprem-c7-x86-t3-2xl-0 NodeAddr=onprem-c7-x86-t3-2xl-0.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-1 NodeAddr=onprem-c7-x86-t3-2xl-1.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-2 NodeAddr=onprem-c7-x86-t3-2xl-2.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-3 NodeAddr=onprem-c7-x86-t3-2xl-3.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-4 NodeAddr=onprem-c7-x86-t3-2xl-4.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-5 NodeAddr=onprem-c7-x86-t3-2xl-5.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-6 NodeAddr=onprem-c7-x86-t3-2xl-6.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-7 NodeAddr=onprem-c7-x86-t3-2xl-7.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-8 NodeAddr=onprem-c7-x86-t3-2xl-8.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-9 NodeAddr=onprem-c7-x86-t3-2xl-9.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 # # # OnPrem Partition # # The is the default partition and includes all nodes from the 1st OS. # PartitionName=onprem Default=YES PriorityTier=20000 Nodes=\\ onprem-c7-x86-t3-2xl-[0-9] # # Always on partitions # SuspendExcParts=onprem","title":"Slurm Configuration of On-Premises Compute Nodes"},{"location":"onprem/#simulating-an-on-premises-network-using-aws","text":"Create a new VPC with public and private subnets and NAT gateways. To simulate the latency between an AWS region and on-prem you can create the VPC in a different region in your account. The CIDR must not overlap with the Slurm VPC. Create a VPC peering connection to your Slurm VPC and accept the connection in the Slurm VPC. Create routes in the private subnets for the CIDR of the peered VPC and route it to the vpc peering connection. Add the on-prem VPC to the Slurm VPC's Route53 private local zone. Create a Route53 private hosted zone for the on-prem compute nodes and add it to the onprem VPC and the slurm VPC so that onprem compute nodes can be resolved. Copy the Slurm AMIs to the region of the on-prem VPC. Create an instance using the copied AMI. Connect to the instance and confirm that the mount points mounted correctly. You will probably have to change the DNS names for the file systems to IP addresses. I created A records in the Route53 zone for the file systems so that if the IP addresses ever change in the future I can easily update them in one place without having to create a new AMI or updated any instances. Create a new AMI from the instance. Create compute node instances from the new AMI and run the following commands on them get the slurmd daemon running so they can join the slurm cluster. # Instance specific variables hostname=onprem-c7-x86-t3-2xl-0 # Domain specific variables onprem_domain=onprem.com source /etc/profile.d/instance_vars.sh # munge needs to be running before calling scontrol /usr/bin/cp /opt/slurm/$ClusterName/config/munge.key /etc/munge/munge.key systemctl enable munged systemctl start munged ipaddress=$(hostname -I) $SLURM_ROOT/bin/scontrol update nodename=${hostname} nodeaddr=$ipaddress # Set hostname hostname_fqdn=${hostname}.${onprem_domain} if [ $(hostname) != $hostname_fqdn ]; then hostnamectl --static set-hostname $hostname_fqdn hostnamectl --pretty set-hostname $hostname fi if [ -e /opt/slurm/${ClusterName}/config/users_groups.json ] && [ -e /opt/slurm/${ClusterName}/bin/create_users_groups.py ]; then /opt/slurm/${ClusterName}/bin/create_users_groups.py -i /opt/slurm/${ClusterName}/config/users_groups.json fi # Create directory for slurmd.log logs_dir=/opt/slurm/${ClusterName}/logs/nodes/${hostname} if [[ ! -d $logs_dir ]]; then mkdir -p $logs_dir fi if [[ -e /var/log/slurm ]]; then rm -rf /var/log/slurm fi ln -s $logs_dir /var/log/slurm systemctl enable slurmd systemctl start slurmd # Restart so that log file goes to file system systemctl restart spot_monitor","title":"Simulating an On-Premises Network Using AWS"},{"location":"run_jobs/","text":"Run Jobs This page is to give some basic instructions on how to run and monitor jobs on SLURM. SLURM provides excellent man pages for all of its commands, so if you have questions refer to the man pages. Set Up Load the environment module for slurm to configure your PATH and SLURM related environment variables. module load {{ClusterName}} The modulefile sets environment variables that control the defaults for Slurm commands. These are documented in the man pages for each command. If you don't like the defaults then you can set them in your environment (for example, your .bashrc) and the modulefile won't change any variables that are already set. The environment variables can always be overridden by the command line options. For example, the SQUEUE_FORMAT2 and SQUEUE_SORT environment variables are set so that the default output format is easier to read and contains useful information that isn't in the default format. Key SLURM Commands The key SLURM commands are Command Description Example sbatch Submit a batch script sbatch -c 1 --mem 1G -C 'sport&GHz:3.1' script srun Run a job within an allocation. srun --pty bin/bash squeue Get job status scancel Cancel a job scancel jobid sinfo Get info about slurm node status sinfo -p all scontrol sstat Display various status information about a running job/step sshare Tool for listing fair share information sprio View the factors that comprise a job's scheduling priority sacct Display accounting data for jobs sreport Generate reports from the Slurm accounting data. sview Graphical tool for viewing cluster state sbatch The most common options for sbatch are listed here. For more details run man sbatch . Options Description Default -p, --partition= partition-names Select the partition/partitions to run job on. Set by slurm.InstanceConfig.DefaultPartition in config file. -t, --time= time Set a limit on total run time of the job. SBATCH_TIMELIMIT=\"1:0:0\" (1 hour) -c, --cpus-per-task= ncpus Number of cores. Default is 1. --mem= size[units] Amount of memory. Default unit is M. Valid units are [K|M|G|T]. SBATCH_MEM_PER_NODE=100M -L, --licenses= license Licenses used by the job. -a, --array= indexes Submit job array -C, --constraint= list Features required by the job. Multiple constraints can be specified with AND(&) and OR( ). -d, --dependency= dependency-list Don't start the job until the dependencies have been completed. -D, --chdir= directory Set the working directory of the job --wait Do not exit until the job finishes, Exit code of sbatch will be the same as the exit code of the job. --wrap Wrap shell commands in a batch script. Run a simulation build followed by a regression build_jobid=$(sbatch -c 4 --mem 4G -L vcs_build -C 'GHz:4|GHz:4.5' -t 30:0 sim-build.sh) if sbatch -d \"afterok:$build_jobid\" -c 1 --mem 100M --wait submit-regression.sh; then echo \"Regression Passed\" else echo \"Regression Failed\" fi srun The srun is usually used to open a pseudo terminal on a compute node for you to run interactive jobs. It accepts most of the same options as sbatch to request cpus, memory, and node features. To open up a pseudo terminal in your shell on a compute node with 4 cores and 16G of memory, execute the following command. srun -c 4 --mem 8G --pty /bin/bash This will queue a job and when it is allocated to a node and the node runs the job control will be returned to your shell, but stdin and stdout will be on the compute node. If you set your DISPLAY environment variable and allow external X11 connections you can use this to run interactive GUI jobs on the compute node and have the windows on your instance. xhost + export DISPLAY=$(hostname):$(echo $DISPLAY | cut -d ':' -f 2) srun -c 4 --mem 8G --pty /bin/bash emacs . # Or whatever gui application you want to run. Should open a window. squeue The squeue command shows the status of jobs. The output format can be customized using the --format or --Format options and you can configure the default output format using the corresponding SQUEUE_FORMAT or SQUEUE_FORMAT2 environment variables. squeue sprio Use sprio to get information about a job's priority. This can be useful to figure out why a job is scheduled before or after another job. sprio -j10,11 sacct Display accounting information about jobs. For example, it can be used to get the requested CPU and memory and see the CPU time and memory actually used. sacct -o JobID,User,JobName,AllocCPUS,State,ExitCode,Elapsed,CPUTime,MaxRSS,MaxVMSize,ReqCPUS,ReqMem,SystemCPU,TotalCPU,UserCPU -j 44 This shows more details. sacct --allclusters --allusers --federation --starttime 1970-01-01 --format 'Submit,Start,End,jobid%15,State%15,user,account,cluster%15,AllocCPUS,AllocNodes,ExitCode,ReqMem,MaxRSS,MaxVMSize,MaxPages,Elapsed,CPUTime,UserCPU,SystemCPU,TotalCPU' | less For more information: man sacct sreport The sreport command can be used to generate report from the Slurm database. Other SLURM Commands Use man command to get information about these less commonly used SLURM commands. Command Description sacctmgr View/modify Slurm account information salloc Obtain a slurm job allocation sattach Attach to a job step sbcast Transmit a file to the nodes allocated to a Slurm job. scrontab Manage slurm crontab files sdiag Diagnostic tool for Slurm. Shows information related to slurmctld execution. seff sgather Transmit a file from the nodes allocated to a Slurm job. sh5util Tool for merging HDF5 files from the acct_gather_profile plugin that gathers detailed data for jobs. sjobexitmod Modify derived exit code of a job strigger Set, get, or clear Slurm trigger information","title":"Run Jobs"},{"location":"run_jobs/#run-jobs","text":"This page is to give some basic instructions on how to run and monitor jobs on SLURM. SLURM provides excellent man pages for all of its commands, so if you have questions refer to the man pages.","title":"Run Jobs"},{"location":"run_jobs/#set-up","text":"Load the environment module for slurm to configure your PATH and SLURM related environment variables. module load {{ClusterName}} The modulefile sets environment variables that control the defaults for Slurm commands. These are documented in the man pages for each command. If you don't like the defaults then you can set them in your environment (for example, your .bashrc) and the modulefile won't change any variables that are already set. The environment variables can always be overridden by the command line options. For example, the SQUEUE_FORMAT2 and SQUEUE_SORT environment variables are set so that the default output format is easier to read and contains useful information that isn't in the default format.","title":"Set Up"},{"location":"run_jobs/#key-slurm-commands","text":"The key SLURM commands are Command Description Example sbatch Submit a batch script sbatch -c 1 --mem 1G -C 'sport&GHz:3.1' script srun Run a job within an allocation. srun --pty bin/bash squeue Get job status scancel Cancel a job scancel jobid sinfo Get info about slurm node status sinfo -p all scontrol sstat Display various status information about a running job/step sshare Tool for listing fair share information sprio View the factors that comprise a job's scheduling priority sacct Display accounting data for jobs sreport Generate reports from the Slurm accounting data. sview Graphical tool for viewing cluster state","title":"Key SLURM Commands"},{"location":"run_jobs/#sbatch","text":"The most common options for sbatch are listed here. For more details run man sbatch . Options Description Default -p, --partition= partition-names Select the partition/partitions to run job on. Set by slurm.InstanceConfig.DefaultPartition in config file. -t, --time= time Set a limit on total run time of the job. SBATCH_TIMELIMIT=\"1:0:0\" (1 hour) -c, --cpus-per-task= ncpus Number of cores. Default is 1. --mem= size[units] Amount of memory. Default unit is M. Valid units are [K|M|G|T]. SBATCH_MEM_PER_NODE=100M -L, --licenses= license Licenses used by the job. -a, --array= indexes Submit job array -C, --constraint= list Features required by the job. Multiple constraints can be specified with AND(&) and OR( ). -d, --dependency= dependency-list Don't start the job until the dependencies have been completed. -D, --chdir= directory Set the working directory of the job --wait Do not exit until the job finishes, Exit code of sbatch will be the same as the exit code of the job. --wrap Wrap shell commands in a batch script.","title":"sbatch"},{"location":"run_jobs/#run-a-simulation-build-followed-by-a-regression","text":"build_jobid=$(sbatch -c 4 --mem 4G -L vcs_build -C 'GHz:4|GHz:4.5' -t 30:0 sim-build.sh) if sbatch -d \"afterok:$build_jobid\" -c 1 --mem 100M --wait submit-regression.sh; then echo \"Regression Passed\" else echo \"Regression Failed\" fi","title":"Run a simulation build followed by a regression"},{"location":"run_jobs/#srun","text":"The srun is usually used to open a pseudo terminal on a compute node for you to run interactive jobs. It accepts most of the same options as sbatch to request cpus, memory, and node features. To open up a pseudo terminal in your shell on a compute node with 4 cores and 16G of memory, execute the following command. srun -c 4 --mem 8G --pty /bin/bash This will queue a job and when it is allocated to a node and the node runs the job control will be returned to your shell, but stdin and stdout will be on the compute node. If you set your DISPLAY environment variable and allow external X11 connections you can use this to run interactive GUI jobs on the compute node and have the windows on your instance. xhost + export DISPLAY=$(hostname):$(echo $DISPLAY | cut -d ':' -f 2) srun -c 4 --mem 8G --pty /bin/bash emacs . # Or whatever gui application you want to run. Should open a window.","title":"srun"},{"location":"run_jobs/#squeue","text":"The squeue command shows the status of jobs. The output format can be customized using the --format or --Format options and you can configure the default output format using the corresponding SQUEUE_FORMAT or SQUEUE_FORMAT2 environment variables. squeue","title":"squeue"},{"location":"run_jobs/#sprio","text":"Use sprio to get information about a job's priority. This can be useful to figure out why a job is scheduled before or after another job. sprio -j10,11","title":"sprio"},{"location":"run_jobs/#sacct","text":"Display accounting information about jobs. For example, it can be used to get the requested CPU and memory and see the CPU time and memory actually used. sacct -o JobID,User,JobName,AllocCPUS,State,ExitCode,Elapsed,CPUTime,MaxRSS,MaxVMSize,ReqCPUS,ReqMem,SystemCPU,TotalCPU,UserCPU -j 44 This shows more details. sacct --allclusters --allusers --federation --starttime 1970-01-01 --format 'Submit,Start,End,jobid%15,State%15,user,account,cluster%15,AllocCPUS,AllocNodes,ExitCode,ReqMem,MaxRSS,MaxVMSize,MaxPages,Elapsed,CPUTime,UserCPU,SystemCPU,TotalCPU' | less For more information: man sacct","title":"sacct"},{"location":"run_jobs/#sreport","text":"The sreport command can be used to generate report from the Slurm database.","title":"sreport"},{"location":"run_jobs/#other-slurm-commands","text":"Use man command to get information about these less commonly used SLURM commands. Command Description sacctmgr View/modify Slurm account information salloc Obtain a slurm job allocation sattach Attach to a job step sbcast Transmit a file to the nodes allocated to a Slurm job. scrontab Manage slurm crontab files sdiag Diagnostic tool for Slurm. Shows information related to slurmctld execution. seff sgather Transmit a file from the nodes allocated to a Slurm job. sh5util Tool for merging HDF5 files from the acct_gather_profile plugin that gathers detailed data for jobs. sjobexitmod Modify derived exit code of a job strigger Set, get, or clear Slurm trigger information","title":"Other SLURM Commands"},{"location":"soca_integration/","text":"SOCA Integration Integration with SOCA is straightforward. Set the following parameters in your config file. Parameter Description Value VpcId VPC id for the SOCA cluster vpc-xxxxxx SubmitterSecurityGroupIds The ComputeNode security group name and id cluster-id - ComputeNodeSG : sg-xxxxxxxx ExtraMounts Add the mount parameters for the /apps and /data directories. This is required for access to the home directory. Deploy your slurm cluster. Connect to the SOCA Scheduler instance and run the commands in the MountCommand and ConfigureSyncSlurmUsersGroups outputs of the SLURM stack as root. These commands will mount the SLURM file system at /opt/slurm/{{ClusterName}} and then create a cron job that runs every 5 minutes and updates /opt/slurm/{{ClusterName}}/config/users_groups.json . Connect to a remote desktop instance and run the commands in the MountCommand and ConfigureSubmitterCommand outputs of the SLURM stack. If all users need to use the cluster then it is probably best to create a custom AMI that is configured with the configuration commands. You are now ready to run jobs.","title":"SOCA Integration"},{"location":"soca_integration/#soca-integration","text":"Integration with SOCA is straightforward. Set the following parameters in your config file. Parameter Description Value VpcId VPC id for the SOCA cluster vpc-xxxxxx SubmitterSecurityGroupIds The ComputeNode security group name and id cluster-id - ComputeNodeSG : sg-xxxxxxxx ExtraMounts Add the mount parameters for the /apps and /data directories. This is required for access to the home directory. Deploy your slurm cluster. Connect to the SOCA Scheduler instance and run the commands in the MountCommand and ConfigureSyncSlurmUsersGroups outputs of the SLURM stack as root. These commands will mount the SLURM file system at /opt/slurm/{{ClusterName}} and then create a cron job that runs every 5 minutes and updates /opt/slurm/{{ClusterName}}/config/users_groups.json . Connect to a remote desktop instance and run the commands in the MountCommand and ConfigureSubmitterCommand outputs of the SLURM stack. If all users need to use the cluster then it is probably best to create a custom AMI that is configured with the configuration commands. You are now ready to run jobs.","title":"SOCA Integration"},{"location":"todo/","text":"To Do List List of tasks to be completed. Create a configuration to emulate an on-prem cluster Use it to test burting from a static on-prem compute cluster to an AWS federation an multi-AZ/region. Configure always on instances for RIs or savings plans. Support multi-AZ/region for a single cluster. Instead of federating clusters, add support for compute nodes in multiple availability zones and regions Assumes that networking is configured between VPCs if multiple regions are used Assumes that the storage architecture supports a consistent file system view for all compute nodes. This could be shared file systems that are mounted across the AZs and regions or AZ/region specific storage with some kind of data synchronization strategy. Support multiple clusters (federation) Implementation is complete and is mostly working but has issues. I see jobs running in other clusters when there are still resources in the primary cluster. Job stuck in pending state with FedJobLock. Can't cancel or requeue, lower priority jobs stuck behind it in PENDING state. This completely hung cluster 1 and 3. Configure Preemption https://slurm.schedmd.com/preempt.html Configure preemption Document how to use preemption Example is interactive jobs should preempt regression jobs so that they can get a license and start running without waiting for a job to complte. 1/21/22: It has been configured, but not tested Use EC2ImageBuilder to create AMIs Add a CustomResource to deconfigure the cluster when the stack is deleted Run an SSM command on all instances in the VPC that runs the DeconfigureClusterCommand before the file systems are deleted. I think that the ordering can be done by making the CustomResource dependent on the file systems. This has been added but not tested. Put slurm logs on the file system so that they persist and can be accessed from other instances. Turn deletion_protection on for database. Have it turned off during testing to ease deletion of test stacks. Configure remote licenses that are stored on slurmdbd. https://slurm.schedmd.com/licenses.html Licenses can be configured with sacctmgr and assigned to clusters. Create separate script that saves EC2 instance info to a json file that is read by the Slurm plugin. Currently this information is read from the EC2 API at every invocation. This would reduce the API calls and speed up the scripts. Create script to update license configuration based on actual available licenses on license server. Use a customizable API for querying the license server and use the returned values to update the slurm license configuration. Document heterogeneous Job support usage How is srun used https://slurm.schedmd.com/heterogeneous_jobs.html Investigate removing memory as a consumable resource and allocate 1 job per node. This is more scalable according to https://slurm.schedmd.com/big_sys.html because the scheduler doesn't have to keep track of the cores and memory on each node. Add support for nss_slurm plugin https://slurm.schedmd.com/nss_slurm.html Removes the need of the user_groups.json file and creating local users and groups. 1/21/22: Implemented but when I remove the local user the username isn't found inside the job. The uid and gid exist, but without names. This results in \"I have no name\" in an interactive shell. May be acceptable to some, but stilling with the previous method of creating users and groups.","title":"To Do List"},{"location":"todo/#to-do-list","text":"List of tasks to be completed. Create a configuration to emulate an on-prem cluster Use it to test burting from a static on-prem compute cluster to an AWS federation an multi-AZ/region. Configure always on instances for RIs or savings plans. Support multi-AZ/region for a single cluster. Instead of federating clusters, add support for compute nodes in multiple availability zones and regions Assumes that networking is configured between VPCs if multiple regions are used Assumes that the storage architecture supports a consistent file system view for all compute nodes. This could be shared file systems that are mounted across the AZs and regions or AZ/region specific storage with some kind of data synchronization strategy. Support multiple clusters (federation) Implementation is complete and is mostly working but has issues. I see jobs running in other clusters when there are still resources in the primary cluster. Job stuck in pending state with FedJobLock. Can't cancel or requeue, lower priority jobs stuck behind it in PENDING state. This completely hung cluster 1 and 3. Configure Preemption https://slurm.schedmd.com/preempt.html Configure preemption Document how to use preemption Example is interactive jobs should preempt regression jobs so that they can get a license and start running without waiting for a job to complte. 1/21/22: It has been configured, but not tested Use EC2ImageBuilder to create AMIs Add a CustomResource to deconfigure the cluster when the stack is deleted Run an SSM command on all instances in the VPC that runs the DeconfigureClusterCommand before the file systems are deleted. I think that the ordering can be done by making the CustomResource dependent on the file systems. This has been added but not tested. Put slurm logs on the file system so that they persist and can be accessed from other instances. Turn deletion_protection on for database. Have it turned off during testing to ease deletion of test stacks. Configure remote licenses that are stored on slurmdbd. https://slurm.schedmd.com/licenses.html Licenses can be configured with sacctmgr and assigned to clusters. Create separate script that saves EC2 instance info to a json file that is read by the Slurm plugin. Currently this information is read from the EC2 API at every invocation. This would reduce the API calls and speed up the scripts. Create script to update license configuration based on actual available licenses on license server. Use a customizable API for querying the license server and use the returned values to update the slurm license configuration. Document heterogeneous Job support usage How is srun used https://slurm.schedmd.com/heterogeneous_jobs.html Investigate removing memory as a consumable resource and allocate 1 job per node. This is more scalable according to https://slurm.schedmd.com/big_sys.html because the scheduler doesn't have to keep track of the cores and memory on each node. Add support for nss_slurm plugin https://slurm.schedmd.com/nss_slurm.html Removes the need of the user_groups.json file and creating local users and groups. 1/21/22: Implemented but when I remove the local user the username isn't found inside the job. The uid and gid exist, but without names. This results in \"I have no name\" in an interactive shell. May be acceptable to some, but stilling with the previous method of creating users and groups.","title":"To Do List"}]}