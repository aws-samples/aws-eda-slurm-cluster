{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AWS EDA Slurm Cluster This repository contains an AWS Cloud Development Kit (CDK) application that creates a Slurm cluster that is suitable for running production EDA workloads on AWS. The original (legacy) version of this repo used a custom Python plugin to integrate Slurm with AWS. The latest version of the repo uses AWS ParallelCluster for the core Slurm infrastructure and AWS integration. The big advantage of moving to AWS ParallelCluster is that it is a supported AWS service. Currently, some of the features of the legacy version are not supported in the ParallelCluster version, but work continues to add features to ParallelCluster so that those features can be supported in the future. Key features are supported by both versions are: Automatic scaling of AWS EC2 instances based on demand Use any AWS EC2 instance type including Graviton2 Use of spot instances Handling of spot terminations Handling of insufficient capacity exceptions Batch and interactive partitions (queues) Manages tool licenses as a consumable resource User and group fair share scheduling Slurm accounting database CloudWatch dashboard Job preemption Manage on-premises compute nodes Configure partitions (queues) and nodes that are always on to support reserved instances (RIs) and savings plans (SPs). Features in the legacy version and not in the ParallelCluster version: Heterogenous clusters with mixed OSes and CPU architectures on compute nodes. Multi-AZ support. Supported by ParallelCluster, but not currently implemented. Multi-region support AWS Fault Injection Simulator (FIS) templates to test spot terminations Support for MungeKeySsmParameter Multi-cluster federation ParallelCluster Limitations Number of \"Compute Resources\" (CRs) is limited to 50 which limits the number of instance types allowed in a cluster. ParallelCluster can have multiple instance types in a CR, but with memory based scheduling enabled, they must all have the same number of cores and amount of memory. All Slurm instances must have the same OS and CPU architecture. Stand-alone Slurm database daemon instance. Prevents federation. Multi-region support. This is unlikely to change because multi-region services run against our archiectural philosophy. Federation may be an option but its current implementation limits scheduler performance and doesn't allow cluster prioritization so jobs land on random clusters. Slurm Limitations Job preemption based on licenses Federation doesn't support prioritizing federated clusters for job scheduling. Result is jobs scattered across the federated clusters. Operating System and Processor Architecture Support This Slurm cluster supports the following OSes: ParallelCluster: Amazon Linux 2 CentOS 7 RedHat 7 and 8 Legacy: Alma Linux 8 Amazon Linux 2 CentOS 7 RedHat 7 and 8 Rocky Linux 8 RedHat stopped supporting CentOS 8, so for a similar RedHat 8 binary compatible distribution we support Alma Linux and Rocky Linux as replacements for CentOS. These RHEL 8 downstreams are not currently supported by ParallelCluster. This Slurm cluster supports both Intel/AMD (x86_64) based instances and ARM Graviton2 (arm64/aarch64) based instances. Graviton instances require Amazon Linux 2, RedHat 8, AlmaLinux 8, or RockyLinux 8 operating systems. RedHat 7 and CentOS 7 do not support Graviton 2. This provides the following different combinations of OS and processor architecture. ParallelCluster: Amazon Linux 2 and arm64 Amazon Linux 2 and x86_64 CentOS 7 and x86_64 RedHat 7 and x86_64 RedHat 8 and arm64 RedHat 8 and x86_64 Legacy: Alma Linux 8 and arm64 Alma Linux 8 and x86_64 Amazon Linux 2 and arm64 Amazon Linux 2 and x86_64 CentOS 7 and x86_64 RedHat 7 and x86_64 RedHat 8 and arm64 RedHat 8 and x86_64 Rocky Linux 8 and arm64 Rocky Linux 8 and x86_64 Note that in the ParallelCluster version, all compute nodes must have the same OS and architecture. Documentation View on GitHub Pages You can also view the docs locally, The docs are in the docs directory. You can view them in an editor or using the mkdocs tool. I recommend installing mkdocs in a python virtual environment. python3 -m venv ~/.mkdocs_venv source ~/.mkdocs_venv/bin/activate pip install mkdocs Then run mkdocs. source ~/.mkdocs_venv/bin/activate mkdocs serve & firefox http://127.0.0.1:8000/ & Open a browser to: http://127.0.0.1:8000/ Or you can simply let make do this for you. make local-docs Security See CONTRIBUTING for more information. License This library is licensed under the MIT-0 License. See the LICENSE file.","title":"AWS EDA Slurm Cluster"},{"location":"#aws-eda-slurm-cluster","text":"This repository contains an AWS Cloud Development Kit (CDK) application that creates a Slurm cluster that is suitable for running production EDA workloads on AWS. The original (legacy) version of this repo used a custom Python plugin to integrate Slurm with AWS. The latest version of the repo uses AWS ParallelCluster for the core Slurm infrastructure and AWS integration. The big advantage of moving to AWS ParallelCluster is that it is a supported AWS service. Currently, some of the features of the legacy version are not supported in the ParallelCluster version, but work continues to add features to ParallelCluster so that those features can be supported in the future. Key features are supported by both versions are: Automatic scaling of AWS EC2 instances based on demand Use any AWS EC2 instance type including Graviton2 Use of spot instances Handling of spot terminations Handling of insufficient capacity exceptions Batch and interactive partitions (queues) Manages tool licenses as a consumable resource User and group fair share scheduling Slurm accounting database CloudWatch dashboard Job preemption Manage on-premises compute nodes Configure partitions (queues) and nodes that are always on to support reserved instances (RIs) and savings plans (SPs). Features in the legacy version and not in the ParallelCluster version: Heterogenous clusters with mixed OSes and CPU architectures on compute nodes. Multi-AZ support. Supported by ParallelCluster, but not currently implemented. Multi-region support AWS Fault Injection Simulator (FIS) templates to test spot terminations Support for MungeKeySsmParameter Multi-cluster federation ParallelCluster Limitations Number of \"Compute Resources\" (CRs) is limited to 50 which limits the number of instance types allowed in a cluster. ParallelCluster can have multiple instance types in a CR, but with memory based scheduling enabled, they must all have the same number of cores and amount of memory. All Slurm instances must have the same OS and CPU architecture. Stand-alone Slurm database daemon instance. Prevents federation. Multi-region support. This is unlikely to change because multi-region services run against our archiectural philosophy. Federation may be an option but its current implementation limits scheduler performance and doesn't allow cluster prioritization so jobs land on random clusters. Slurm Limitations Job preemption based on licenses Federation doesn't support prioritizing federated clusters for job scheduling. Result is jobs scattered across the federated clusters.","title":"AWS EDA Slurm Cluster"},{"location":"#operating-system-and-processor-architecture-support","text":"This Slurm cluster supports the following OSes: ParallelCluster: Amazon Linux 2 CentOS 7 RedHat 7 and 8 Legacy: Alma Linux 8 Amazon Linux 2 CentOS 7 RedHat 7 and 8 Rocky Linux 8 RedHat stopped supporting CentOS 8, so for a similar RedHat 8 binary compatible distribution we support Alma Linux and Rocky Linux as replacements for CentOS. These RHEL 8 downstreams are not currently supported by ParallelCluster. This Slurm cluster supports both Intel/AMD (x86_64) based instances and ARM Graviton2 (arm64/aarch64) based instances. Graviton instances require Amazon Linux 2, RedHat 8, AlmaLinux 8, or RockyLinux 8 operating systems. RedHat 7 and CentOS 7 do not support Graviton 2. This provides the following different combinations of OS and processor architecture. ParallelCluster: Amazon Linux 2 and arm64 Amazon Linux 2 and x86_64 CentOS 7 and x86_64 RedHat 7 and x86_64 RedHat 8 and arm64 RedHat 8 and x86_64 Legacy: Alma Linux 8 and arm64 Alma Linux 8 and x86_64 Amazon Linux 2 and arm64 Amazon Linux 2 and x86_64 CentOS 7 and x86_64 RedHat 7 and x86_64 RedHat 8 and arm64 RedHat 8 and x86_64 Rocky Linux 8 and arm64 Rocky Linux 8 and x86_64 Note that in the ParallelCluster version, all compute nodes must have the same OS and architecture.","title":"Operating System and Processor Architecture Support"},{"location":"#documentation","text":"View on GitHub Pages You can also view the docs locally, The docs are in the docs directory. You can view them in an editor or using the mkdocs tool. I recommend installing mkdocs in a python virtual environment. python3 -m venv ~/.mkdocs_venv source ~/.mkdocs_venv/bin/activate pip install mkdocs Then run mkdocs. source ~/.mkdocs_venv/bin/activate mkdocs serve & firefox http://127.0.0.1:8000/ & Open a browser to: http://127.0.0.1:8000/ Or you can simply let make do this for you. make local-docs","title":"Documentation"},{"location":"#security","text":"See CONTRIBUTING for more information.","title":"Security"},{"location":"#license","text":"This library is licensed under the MIT-0 License. See the LICENSE file.","title":"License"},{"location":"CONTRIBUTING/","text":"Contributing Guidelines Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community. Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution. Reporting Bugs/Feature Requests We welcome you to use the GitHub issue tracker to report bugs or suggest features. When filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful: A reproducible test case or series of steps The version of our code being used Any modifications you've made relevant to the bug Anything unusual about your environment or deployment Contributing via Pull Requests Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that: You are working against the latest source on the main branch. You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already. You open an issue to discuss any significant work - we would hate for your time to be wasted. To send us a pull request, please: Fork the repository. Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change. Ensure local tests pass. Commit to your fork using clear commit messages. Send us a pull request, answering any default questions in the pull request interface. Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation. GitHub provides additional document on forking a repository and creating a pull request . Finding contributions to work on Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start. Code of Conduct This project has adopted the Amazon Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments. Security issue notifications If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page . Please do not create a public github issue. Licensing See the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution.","title":"Contributing Guidelines"},{"location":"CONTRIBUTING/#contributing-guidelines","text":"Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community. Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.","title":"Contributing Guidelines"},{"location":"CONTRIBUTING/#reporting-bugsfeature-requests","text":"We welcome you to use the GitHub issue tracker to report bugs or suggest features. When filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful: A reproducible test case or series of steps The version of our code being used Any modifications you've made relevant to the bug Anything unusual about your environment or deployment","title":"Reporting Bugs/Feature Requests"},{"location":"CONTRIBUTING/#contributing-via-pull-requests","text":"Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that: You are working against the latest source on the main branch. You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already. You open an issue to discuss any significant work - we would hate for your time to be wasted. To send us a pull request, please: Fork the repository. Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change. Ensure local tests pass. Commit to your fork using clear commit messages. Send us a pull request, answering any default questions in the pull request interface. Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation. GitHub provides additional document on forking a repository and creating a pull request .","title":"Contributing via Pull Requests"},{"location":"CONTRIBUTING/#finding-contributions-to-work-on","text":"Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start.","title":"Finding contributions to work on"},{"location":"CONTRIBUTING/#code-of-conduct","text":"This project has adopted the Amazon Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments.","title":"Code of Conduct"},{"location":"CONTRIBUTING/#security-issue-notifications","text":"If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page . Please do not create a public github issue.","title":"Security issue notifications"},{"location":"CONTRIBUTING/#licensing","text":"See the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution.","title":"Licensing"},{"location":"debug/","text":"Debug Log Files on File System Most of the key log files are stored on the Slurm file system so that they can be accessed from any instance with the file system mounted. Logfile Description /opt/slurm/{{ClusterName}}/logs/nodes/{{node-name}}/slurmd.log Slurm daemon (slurmd) logfile /opt/slurm/{{ClusterName}}/logs/nodes/{{node-name}}/spot_monitor.log Spot monitor logfile /opt/slurm/{{ClusterName}}/logs/slurmctl[1-2]/cloudwatch.log Cloudwatch cron (slurm_ec2_publish_cw.py) logfile /opt/slurm/{{ClusterName}}/logs/slurmctl[1-2]/power_save.log Power saving API logfile /opt/slurm/{{ClusterName}}/logs/slurmctl[1-2]/slurmctld.log Slurm controller daemon (slurmctld) logfile /opt/slurm/{{ClusterName}}/logs/slurmctl[1-2]/terminate_old_instances.log Terminate old instances cron (terminate_old_instances.py) logfile /opt/slurm/{{ClusterName}}/logs/slurmdbd/slurmdbd.log Slurm database daemon (slurmdbd) logfile Slurm AMI Nodes The Slurm AMI nodes build the Slurm binaries for all of the configured operating system (OS) variants. The Amazon Linux 2 build is a prerequisite for the Slurm controllers and slurmdbd instances. The other builds are prerequisites for compute nodes and submitters. First check for errors in the user data script. The following command will show the output: grep cloud-init /var/log/messages | less The most common problem is that the ansible playbook failed. Check the ansible log file to see what failed. less /var/log/ansible.log The following command will rerun the user data. It will download the playbooks from the S3 deployment bucket and then run it to configure the instance. /var/lib/cloud/instance/scripts/part-001 If the problem is with the ansible playbook, then you can edit it in /root/playbooks and then run your modified playbook by running the following command. /root/slurm_node_ami_config.sh Slurm Controller If slurm commands hang, then it's likely a problem with the Slurm controller. The first thing to check is the controller's logfile which is stored on the Slurm file system. /opt/slurm/{{ClusterName}}/logs/nodes/slurmctl[1-2]/slurmctld.log If the logfile doesn't exist or is empty then you will need to connect to the slurmctl instance using SSM Manager or ssh and switch to the root user. sudo su The first thing to do is to ensure that the Slurm controller daemon is running: systemctl status slurmctld If it isn't then first check for errors in the user data script. The following command will show the output: grep cloud-init /var/log/messages | less The most common problem is that the ansible playbook failed. Check the ansible log file to see what failed. less /var/log/ansible.log The following command will rerun the user data. It will download the playbooks from the S3 deployment bucket and then run it to configure the instance. /var/lib/cloud/instance/scripts/part-001 If the problem is with the ansible playbook, then you can edit it in /root/playbooks and then run your modified playbook by running the following command. /root/slurmctl_config.sh The daemon may also be failing because of some other error. Check the slurmctld.log for errors. Another way to debug the slurmctld daemon is to launch it interactively with debug set high. The first thing to do is get the path to the slurmctld binary. slurmctld=$(cat /etc/systemd/system/slurmctld.service | awk -F '=' '/ExecStart/ {print $2}') Then you can run slurmctld: $slurmctld -D -vvvvv Slurm Controller Log Files Logfile Description /var/log/ansible.log Ansible logfile /var/log/slurm/cloudwatch.log Logfile for the script that uploads CloudWatch events. /var/log/slurm/slurmctld.log slurmctld logfile /var/log/slurm/power_save.log Slurm plugin logfile with power saving scripts that start, stop, and terminated instances. /var/log/slurm/terminate_old_instances.log Logfile for the script that terminates stopped instances. Slurm Accounting Database (slurmdbd) If you are having problems with the slurm accounting database connect to the slurmdbd instance using SSM Manager. Check for cloud-init and ansible errors the same way as for the slurmctl instance. Also check the slurmdbd.log for errors. Log Files Logfile Description /var/log/ansible.log Ansible logfile /var/log/slurm/slurmdbd.log slurmctld logfile Compute Nodes If there are problems with the compute nodes, connect to them using SSM Manager. Check for cloud-init errors the same way as for the slurmctl instance. The compute nodes do not run ansible; their AMIs are configured using ansible. Also check the slurmd.log . Check that the slurm daemon is running. systemctl status slurmd Log Files Logfile Description /var/log/slurm/slurmd.log slurmctld logfile Job Stuck in Pending State You can use scontrol to get detailed information about a job. scontrol show job *jobid* Job Stuck in Completing State When a node starts it reports it's number of cores and free memory to the controller. If the memory is less than in slurm_node.conf then the controller will mark the node as invalid. You can confirm this by searching for the node in /var/log/slurm/slurmctld.log on the controller. If this happens, fix the memory in slurm_nodes.conf and restart slurmctld. systemctl restart slurmctld Then reboot the node. Another cause of this is a hung process on the compute node. To clear this out, connect to the slurm controller and mark the node down, resume, and then idle. scontrol update node NODENAME state=DOWN reason=hung scontrol update node NODENAME state=RESUME scontrol update node NODENAME state=IDLE","title":"Debug"},{"location":"debug/#debug","text":"","title":"Debug"},{"location":"debug/#log-files-on-file-system","text":"Most of the key log files are stored on the Slurm file system so that they can be accessed from any instance with the file system mounted. Logfile Description /opt/slurm/{{ClusterName}}/logs/nodes/{{node-name}}/slurmd.log Slurm daemon (slurmd) logfile /opt/slurm/{{ClusterName}}/logs/nodes/{{node-name}}/spot_monitor.log Spot monitor logfile /opt/slurm/{{ClusterName}}/logs/slurmctl[1-2]/cloudwatch.log Cloudwatch cron (slurm_ec2_publish_cw.py) logfile /opt/slurm/{{ClusterName}}/logs/slurmctl[1-2]/power_save.log Power saving API logfile /opt/slurm/{{ClusterName}}/logs/slurmctl[1-2]/slurmctld.log Slurm controller daemon (slurmctld) logfile /opt/slurm/{{ClusterName}}/logs/slurmctl[1-2]/terminate_old_instances.log Terminate old instances cron (terminate_old_instances.py) logfile /opt/slurm/{{ClusterName}}/logs/slurmdbd/slurmdbd.log Slurm database daemon (slurmdbd) logfile","title":"Log Files on File System"},{"location":"debug/#slurm-ami-nodes","text":"The Slurm AMI nodes build the Slurm binaries for all of the configured operating system (OS) variants. The Amazon Linux 2 build is a prerequisite for the Slurm controllers and slurmdbd instances. The other builds are prerequisites for compute nodes and submitters. First check for errors in the user data script. The following command will show the output: grep cloud-init /var/log/messages | less The most common problem is that the ansible playbook failed. Check the ansible log file to see what failed. less /var/log/ansible.log The following command will rerun the user data. It will download the playbooks from the S3 deployment bucket and then run it to configure the instance. /var/lib/cloud/instance/scripts/part-001 If the problem is with the ansible playbook, then you can edit it in /root/playbooks and then run your modified playbook by running the following command. /root/slurm_node_ami_config.sh","title":"Slurm AMI Nodes"},{"location":"debug/#slurm-controller","text":"If slurm commands hang, then it's likely a problem with the Slurm controller. The first thing to check is the controller's logfile which is stored on the Slurm file system. /opt/slurm/{{ClusterName}}/logs/nodes/slurmctl[1-2]/slurmctld.log If the logfile doesn't exist or is empty then you will need to connect to the slurmctl instance using SSM Manager or ssh and switch to the root user. sudo su The first thing to do is to ensure that the Slurm controller daemon is running: systemctl status slurmctld If it isn't then first check for errors in the user data script. The following command will show the output: grep cloud-init /var/log/messages | less The most common problem is that the ansible playbook failed. Check the ansible log file to see what failed. less /var/log/ansible.log The following command will rerun the user data. It will download the playbooks from the S3 deployment bucket and then run it to configure the instance. /var/lib/cloud/instance/scripts/part-001 If the problem is with the ansible playbook, then you can edit it in /root/playbooks and then run your modified playbook by running the following command. /root/slurmctl_config.sh The daemon may also be failing because of some other error. Check the slurmctld.log for errors. Another way to debug the slurmctld daemon is to launch it interactively with debug set high. The first thing to do is get the path to the slurmctld binary. slurmctld=$(cat /etc/systemd/system/slurmctld.service | awk -F '=' '/ExecStart/ {print $2}') Then you can run slurmctld: $slurmctld -D -vvvvv","title":"Slurm Controller"},{"location":"debug/#slurm-controller-log-files","text":"Logfile Description /var/log/ansible.log Ansible logfile /var/log/slurm/cloudwatch.log Logfile for the script that uploads CloudWatch events. /var/log/slurm/slurmctld.log slurmctld logfile /var/log/slurm/power_save.log Slurm plugin logfile with power saving scripts that start, stop, and terminated instances. /var/log/slurm/terminate_old_instances.log Logfile for the script that terminates stopped instances.","title":"Slurm Controller Log Files"},{"location":"debug/#slurm-accounting-database-slurmdbd","text":"If you are having problems with the slurm accounting database connect to the slurmdbd instance using SSM Manager. Check for cloud-init and ansible errors the same way as for the slurmctl instance. Also check the slurmdbd.log for errors.","title":"Slurm Accounting Database (slurmdbd)"},{"location":"debug/#log-files","text":"Logfile Description /var/log/ansible.log Ansible logfile /var/log/slurm/slurmdbd.log slurmctld logfile","title":"Log Files"},{"location":"debug/#compute-nodes","text":"If there are problems with the compute nodes, connect to them using SSM Manager. Check for cloud-init errors the same way as for the slurmctl instance. The compute nodes do not run ansible; their AMIs are configured using ansible. Also check the slurmd.log . Check that the slurm daemon is running. systemctl status slurmd","title":"Compute Nodes"},{"location":"debug/#log-files_1","text":"Logfile Description /var/log/slurm/slurmd.log slurmctld logfile","title":"Log Files"},{"location":"debug/#job-stuck-in-pending-state","text":"You can use scontrol to get detailed information about a job. scontrol show job *jobid*","title":"Job Stuck in Pending State"},{"location":"debug/#job-stuck-in-completing-state","text":"When a node starts it reports it's number of cores and free memory to the controller. If the memory is less than in slurm_node.conf then the controller will mark the node as invalid. You can confirm this by searching for the node in /var/log/slurm/slurmctld.log on the controller. If this happens, fix the memory in slurm_nodes.conf and restart slurmctld. systemctl restart slurmctld Then reboot the node. Another cause of this is a hung process on the compute node. To clear this out, connect to the slurm controller and mark the node down, resume, and then idle. scontrol update node NODENAME state=DOWN reason=hung scontrol update node NODENAME state=RESUME scontrol update node NODENAME state=IDLE","title":"Job Stuck in Completing State"},{"location":"delete-cluster/","text":"Delete Cluster (legacy) Most of the resources can be deleted by simply deleting the cluster's CloudFormation stack. However, there a couple of resources that must be manually deleted: The Slurm RDS database The Slurm file system The deletion of the CloudFormation stack will fail because of these 2 resources and some resources that are used by them will also fail to delete. Manually delete the resources and then retry deleting the CloudFormation stack. Manually Delete RDS Database If the database contains production data then it is highly recommended that you back up the data. You could also keep the database and use it for creating new clusters. Even after deleting the database CloudFormation may say that it failed to delete. Confirm in the RDS console that it deleted and then ignore the resource when retrying the stack deletion. Go the the RDS console Select Databases on the left Remove deletion protection Select the cluster's database Click Modify Expand Additional scaling configuration Uncheck Scale the capacity to 0 ACIs when cluster is idle Uncheck Enable deletion protection Click Continue Select Apply immediately Click Modify cluster Delete the database Select the cluster's database Click Actions -> Delete Click Delete DB cluster Manually delete the Slurm file system FSx for OpenZfs Go to the FSx console Select the cluster's file system Click Actions -> Delete file system Click Delete file system","title":"Delete Cluster (legacy)"},{"location":"delete-cluster/#delete-cluster-legacy","text":"Most of the resources can be deleted by simply deleting the cluster's CloudFormation stack. However, there a couple of resources that must be manually deleted: The Slurm RDS database The Slurm file system The deletion of the CloudFormation stack will fail because of these 2 resources and some resources that are used by them will also fail to delete. Manually delete the resources and then retry deleting the CloudFormation stack.","title":"Delete Cluster (legacy)"},{"location":"delete-cluster/#manually-delete-rds-database","text":"If the database contains production data then it is highly recommended that you back up the data. You could also keep the database and use it for creating new clusters. Even after deleting the database CloudFormation may say that it failed to delete. Confirm in the RDS console that it deleted and then ignore the resource when retrying the stack deletion. Go the the RDS console Select Databases on the left Remove deletion protection Select the cluster's database Click Modify Expand Additional scaling configuration Uncheck Scale the capacity to 0 ACIs when cluster is idle Uncheck Enable deletion protection Click Continue Select Apply immediately Click Modify cluster Delete the database Select the cluster's database Click Actions -> Delete Click Delete DB cluster","title":"Manually Delete RDS Database"},{"location":"delete-cluster/#manually-delete-the-slurm-file-system","text":"","title":"Manually delete the Slurm file system"},{"location":"delete-cluster/#fsx-for-openzfs","text":"Go to the FSx console Select the cluster's file system Click Actions -> Delete file system Click Delete file system","title":"FSx for OpenZfs"},{"location":"deploy-legacy-cluster/","text":"Deploy Legacy Cluster The original (legacy) version used a custom Slurm plugin for orchestrating the EC2 compute nodes. The latest version uses ParallelCluster to provision the core Slurm infrastructure. When using ParallelCluster, a ParallelCluster configuration will be generated and used to create a ParallelCluster slurm cluster. The first supported ParallelCluster version is 3.6.0. Version 3.7.0 is the recommended minimum version because it support compute node weighting that is proportional to instance type cost so that the least expensive instance types that meet job requirements are used. Prerequisites Configure AWS CLI Credentials You will needs AWS credentials that provide admin access to deploy the cluster. Clone or Download the Repository Clone or download the aws-eda-slurm-cluster repository to your system. git clone git@github.com:aws-samples/aws-eda-slurm-cluster.git Make sure required packages are installed cd aws-eda-slurm-cluster source setup.sh The setup script assumes that you have sudo access so that you can install or update packages. If you do not, then contact an administrator to help you do the updates. If necessary modify the setup script for your environment. Create SNS Topic for Error Notifications (Optional but recommended) The Slurm cluster allows you to specify an SNS notification that will be notified when an error is detected. You can provide the ARN for the topic in the config file or on the command line. You can use the SNS notification in various ways. The simplest is to subscribe your email address to the topic so that you get an email when there is an error. You could also use it to trigger a CloudWatch alarm that could be used to trigger a lambda to do automatic remediation or create a support ticket. Deploy Using ParallelCluster Create ParallelCluster UI (optional but recommended) It is highly recommended to create a ParallelCluster UI to manage your ParallelCluster clusters. A different UI is required for each version of ParallelCluster that you are using. The versions are list in the ParallelCluster Release Notes . The minimum required version is 3.6.0 which adds support for RHEL 8 and increases the number of allows queues and compute resources. The suggested version is at least 3.7.0 because it adds configurable compute node weights which we use to prioritize the selection of compute nodes by their cost. The instructions are in the ParallelCluster User Guide . Create ParallelCluster Slurm Database The Slurm Database is required for configuring Slurm accounts, users, groups, and fair share scheduling. It you need these and other features then you will need to create a ParallelCluster Slurm Database. You do not need to create a new database for each cluster; multiple clusters can share the same database. Follow the directions in this ParallelCluster tutorial to configure slurm accounting . Create Configuration File The first step in deploying your cluster is to create a configuration file. A default configuration file is found in source/resources/config/default_config.yml . You should create a new config file and update the parameters for your cluster. The schema for the config file along with its default values can be found in source/cdk/config_schema.py . The schema is defined in python, but the actual config file should be in yaml format. The following are key parameters that you will need to update. If you do not have the required parameters in your config file then the installer script will fail unless you specify the --prompt option. You should save your selections in the config file. Parameter Description Valid Values Default StackName The cloudformation stack that will deploy the cluster. None ClusterName Shouldn't be the same as StackName None Region Region where VPC is located $AWS_DEFAULT_REGION VpcId The vpc where the cluster will be deployed. vpc-* None SshKeyPair EC2 Keypair to use for instances None slurm/SubmitterSecurityGroupIds Existing security groups that can submit to the cluster. For SOCA this is the ComputeNodeSG* resource. sg-* None ErrorSnsTopicArn ARN of an SNS topic that will be notified of errors arn:aws:sns:{{region}}:{AccountId}:{TopicName} None The defaults for the following parameters are generally acceptable, but may be modified based on your requirements. Parameter Description Valid Values Default InstanceConfig Configures the instance families and types that the cluster can use. See default_config.yml Configure the Compute Instances The InstanceConfig configuration parameter configures the base operating systems, CPU architectures, instance families, and instance types that the Slurm cluster should support. ParallelCluster currently doesn't support heterogeneous cluster; all nodes must have the same architecture and Base OS. The supported OSes and CPU architectures are: Base OS CPU Architectures Amazon Linux 2 x86_64, arm64 CentOS 7 x86_64 RedHat 7 x86_64 RedHat 8 x86_64, arm64 You can exclude instances types by family or specific instance type. By default the InstanceConfig excludes older generation instance families. You can include instances by family or specific instance type. If no includes are specified then all non-excluded instance types will be used. You can also choose to only include the largest instance size within a family. The advantage of using the max instance size is that jobs running on the instance have the highest network bandwidth for that family and fewer instances are required to run the same number of jobs. This may help jobs run faster and allow jobs to wait less time for a new instance to start. The disadvantage is higher cost if the instance is lightly loaded. The default InstanceConfig includes all supported base OSes and architectures and burstable and general purpose instance types. Note that instance types and families are python regular expressions. slurm: InstanceConfig: BaseOsArchitecture: CentOS: 7: [x86_64] Include: InstanceFamilies: - t3.* - m6a.* InstanceTypes: - r6a.large The following InstanceConfig configures instance types recommended for EDA workloads running on CentOS. slurm: InstanceConfig: BaseOsArchitecture: CentOS: 7: [x86_64] Include: InstanceFamilies: - c5.* - c6g.* - f1 - m5.* - m6g.* - r5.* - r6g.* - x2gd - z1d If you have reserved instances (RIs) or savings plans then you can configure instances so that they are always on since you are paying for them whether they are running or not. To do this add a MinCount greater than 0 for the compute resources that contain the instance types. slurm: InstanceConfig: NodeCounts: DefaultMinCount: 1 Create the Cluster To install the cluster run the install script. You can override some parameters in the config file with command line arguments, however it is better to specify all of the parameters in the config file. ./install.sh --config-file <config-file> --cdk-cmd create This will create the ParallelCuster configuration file, store it in S3, and then use a lambda function to create the cluster. If you look in CloudFormation you will see 2 new stacks when deployment is finished. The first is the configuration stack and the second is the cluster. Create users_groups.json Before you can use the cluster you must configure the Linux users and groups for the head and compute nodes. One way to do that would be to join the cluster to your domain. But joining each compute node to a domain effectively creates a distributed denial of service (DDOS) attack on the demain controller when the cluster rapidly scales out or in and each node tries to join or leave the domain. This can lead to domain controller timeouts and widespread havoc in your environment. To solve this problem a script runs on a server that is joined to the domain which writes a JSON file with all of the non-privileged users and groups and their respective uids and gids. A script and cron job on the head and compute nodes reads this json file to create local users and groups that match the domain-joined servers. Select the server that you want to use to create and update the JSON file. The outputs of the configuration stack have the commands required. Config Stack Output Description Command01SubmitterMountHeadNode Mounts the Slurm cluster's shared file system, adds it to /etc/fstab. Command02CreateUsersGroupsJsonConfigure Create /opt/slurm/{{ClusterName}}/config/users_groups.json and create a cron job to refresh it hourly. Before deleting the cluster you can undo the configuration by running the commands in the following outputs. Config Stack Output Description command10CreateUsersGroupsJsonDeconfigure Removes the crontab that refreshes users_groups.json. Now the cluster is ready to be used by sshing into the head node or a login node, if you configured one. If you configured extra file systems for the cluster that contain the users' home directories, then they should be able to ssh in with their own ssh keys. Configure submission hosts to use the cluster ParallelCluster was built assuming that users would ssh into the head node or login nodes to execute Slurm commands. This can be undesirable for a number of reasons. First, users shouldn't be given ssh access to a critical infrastructure like the cluster head node. With ParallelCluster 3.7.0 you can configure login nodes, but if you have already provisioned desktop nodes then it's wasteful to have to provision login nodes. Second, it's just inconvenient to have to use ssh to access the cluster and use it. Fortunately, you can configure any server as a submission host so that users can run slurm commands. These commands must be run by an administrator that has root access to the submission host. The commands could also be run to create a custom AMI for user desktops so that they can access the clusters. The commands to configure submission hosts are in the outputs of the configuration CloudFormation stack. Run them in the following order: Config Stack Output Description Command01SubmitterMountHeadNode Mounts the Slurm cluster's shared file system, adds it to /etc/fstab. Command03SubmitterConfigure Configure the submission host so it can directly access the Slurm cluster. The first command simply mounts the head node's NFS file system so you have access to the Slurm commands and configuration. The second command runs an ansible playbook that configures the submission host so that it can run the Slurm commands for the cluster. It also configures the modulefile that sets up the environment to use the slurm cluster. The clusters have been configured so that a submission host can use more than one cluster by simply changing the modulefile that is loaded. On the submission host just open a new shell and load the modulefile for your cluster and you can access Slurm. Customize the compute node AMI The easiest way to create a custom AMI is to find the default ParallelCluster AMI in the UI. Create an instance using the AMI and make whatever customizations you require such as installing packages and configuring users and groups. Custom file system mounts can be configured in the aws-eda-slurm-cluster config file which will add it to the ParallelCluster config file so that ParallelCluster can manage them for you. When you are done create a new AMI and wait for the AMI to become available. After it is available you can add the custom ami to the aws-eda-slurm-cluster config file. slurm: ParallelClusterConfig: ComputeNodeAmi: ami-0fdb972bda05d2932 Then update your aws-eda-slurm-cluster stack by running the install script again. Deploy using legacy cluster Subscribe to AWS MarketPlace AMIs (Legacy) This is only required for the Legacy scheduler. It is not required for ParallelCluster which uses it's own public AMIs. Subscribe to the MarketPlace AMIs you will use in your cluster. Examples are: Alma Linux 8 - arm64 Alma Linux 8 - x86_64 AWS FPGA Developer AMI - CentOS 7 -x86_64 AWS FPGA Developer AMI - Amazon Linux 2 - x86_64 CentOS 7 - x86_64 Rocky Linux 8 - arm64 Rocky Linux 8 - x86_64 Quick Minimal Deployment The install script can create a minimal Slurm cluster using the default configuration file in the repository and it will prompt you for the required parameters. You will first need to configure your AWS credentials. The installer now defaults to using the ParallelCluster version. cd edaslurmcluster ./install.sh --prompt --cdk-cmd create Install Cloud Development Kit (CDK) (Optional) The install script will attempt to install all of the prerequisites for you. If the install script fails on your system then you can refer to this section for instructions on how to install or update CDK. This cluster uses Cloud Development Kit (CDK) and Python 3 to deploy the cluster. Install the packages used by the installer. sudo yum -y install curl gcc-c++ make nfs-utils python3 tcl unzip wget The following link documents how to setup for CDK. Follow the instructions for Python. https://docs.aws.amazon.com/cdk/v2/guide/getting_started.html#getting_started_prerequisites Note that CDK requires a pretty new version of nodejs which you may have to download from, for example, https://nodejs.org/dist/v16.13.1/node-v16.13.1-linux-x64.tar.xz sudo yum -y install wget wget https://nodejs.org/dist/v16.13.1/node-v16.13.1-linux-x64.tar.xz tar -xf node-v16.13.1-linux-x64.tar.xz ~ Add the nodjs bin directory to your path. https://docs.aws.amazon.com/cdk/v2/guide/getting_started.html#getting_started_install Note that the version of aws-cdk changes frequently. The version that has been tested is in the CDK_VERSION variable in the install script. The install script will try to install the prerequisites if they aren't already installed. Configuration File The first step in deploying your cluster is to create a configuration file. A default configuration file is found in source/resources/config/default_config.yml . You should create a new config file and update the parameters for your cluster. The schema for the config file along with its default values can be found in source/cdk/config_schema.py . The schema is defined in python, but the actual config file should be in yaml format. The following are key parameters that you will need to update. If you do not have the required parameters in your config file then the installer script will fail unless you specify the --prompt option. You should save your selections in the config file. Parameter Description Valid Values Default StackName The cloudformation stack that will deploy the cluster. None VpcId The vpc where the cluster will be deployed. vpc-* None Region Region where VPC is located $AWS_DEFAULT_REGION SshKeyPair EC2 Keypair to use for instances None slurm/SubmitterSecurityGroupIds Existing security groups that can submit to the cluster. For SOCA this is the ComputeNodeSG* resource. sg-* None ErrorSnsTopicArn ARN of an SNS topic that will be notified of errors arn:aws:sns:{{region}}:{AccountId}:{TopicName} None The defaults for the following parameters are generally acceptable, but may be modified based on your requirements. Parameter Description Valid Values Default slurm/SlurmDbd Create a slurmdbd instance connected to an RDS Serverless database. No database. InstanceConfig Configures the instance families and types that the cluster can use. See default_config.yml NumberOfControllers For high availability you can have 2 or 3 controllers. 1-3 1 SuspendAction What to do to an idle instance. Stopped instances will restart faster, but still incur EBS charges while stopped. stop or terminate stop MaxStoppedDuration You can configure how long instances can be stopped before they are automatically terminated. The default is set to 1 hour. This is checked at least hourly. The format uses the ISO 8601 duration format. PnYnMnDTnHnMnS P0Y0M0DT1H0M0S CloudWatchPeriod Default: 5. Set to 1 for finer metric resolution. Configure the Compute Instances The InstanceConfig configuration parameter configures the base operating systems, CPU architectures, instance families, and instance types that the Slurm cluster should support. The supported OSes and CPU architectures are: ParallelCluster Base OS CPU Architectures Amazon Linux 2 x86_64, arm64 CentOS 7 x86_64 RedHat 7 x86_64 RedHat 8 x86_64, arm64 Legacy Base OS CPU Architectures Alma Linux 8 x86_64, arm64 Amazon Linux 2 x86_64, arm64 CentOS 7 x86_64 RedHat 7 x86_64 RedHat 8 x86_64, arm64 Rocky Linux 8 x86_64, arm64 You can exclude instances types by family or specific instance type. By default the InstanceConfig excludes older generation instance families. You can include instances by family or specific instance type. If no includes are specified then all non-excluded instance types will be used. You can also choose to only include the largest instance size within a family. The advantage of using the max instance size is that jobs running on the instance have the highest network bandwidth for that family and fewer instances are required to run the same number of jobs. This may help jobs run faster and allow jobs to wait less time for a new instance to start. The disadvantage is higher cost if the instance is lightly loaded. The default InstanceConfig includes all supported base OSes and architectures and burstable and general purpose instance types. Note that instance types and families are python regular expressions. slurm: InstanceConfig: DefaultPartition: CentOS_7_x86_64 BaseOsArchitecture: AlmaLinux: 8: [x86_64, arm64] CentOS: 7: [x86_64] Amazon: {2: [x86_64, arm64]} RedHat: 7: [x86_64] 8: [x86_64, arm64] Include: MaxSizeOnly: false InstanceFamilies: - t3.* - t4g - m5.* InstanceTypes: [] Exclude: InstanceTypes: - '.+\\.(micro|nano)' # Not enough memory - '.*\\.metal' The following InstanceConfig configures instance types recommended for EDA workloads running on CentOS. slurm: InstanceConfig: DefaultPartition: CentOS_7_x86_64 BaseOsArchitecture: AlmaLinux: {8: [x86_64, arm64]} CentOS: 7: [x86_64] Include: MaxSizeOnly: false InstanceFamilies: - c5.* - c6g.* - f1 - m5.* - m6g.* - r5.* - r6g.* - x2gd - z1d InstanceTypes: [] Exclude: InstanceTypes: - '.*\\.metal' If you have reserved instances (RIs) or savings plans then you can configure instances so that they are always on since you are paying for them whether they are running or not. slurm: InstanceConfig: AlwaysOnNodes: - nodename-[0-4] Update to Latest Base Operating System AMIs (Optional) The default configuration includes the latest AMIs that have been tested. If you want to use the latest base OS AMIs, then configure your AWS cli credentials and run the following script. Note : Updating the AMIs to a newer version may break deployment if repositories and package versions have changed from the tested version. ./source/create-ami-map.py > source/resources/config/ami_map.yml Use Your Own AMIs (Optional) You may already have base AMIs that are configured for your environment. To use them update the SlurmNodeAmis configuration parameter. The parameter is a map with the keys being the region, base OS, and CPU architecture as the keys. So, for example, to use a custom CentOS 7 AMI in the us-east-1 region you would have: slurm: SlurmNodeAmis: us-east-1: CentOS: 7: x86_64: ami-xxxxxxxxxxxxxxxxx Another example is to use the AWS FPGA Developer AMI as the base AMI for your compute nodes so that you can use the Xilinx Vivado tools for FPGA development for AWS F1 instances. By default the EBS volumes are created with the same sizes as in the AMI. You can increase the size of the root EBS volume as shown in this example. This is useful if the root volume needs additional space to install additional packages or tools. SlurmNodeAmis: # Customized AMIs with file system mounts, packages, etc. configured. BaseAmis: us-east-1: Amazon: {2: {x86_64: {ImageId: ami-0efdec76678df9a64, RootDeviceSize: '+5'}}} CentOS: {7: {x86_64: {ImageId: ami-02155c6289e76719a, RootDeviceSize: '+5'}}} Configure Fair Share Scheduling (Optional) Slurm supports fair share scheduling , but it requires the fair share policy to be configured. By default, all users will be put into a default group that has a low fair share. The configuration file is at source/resources/playbooks/roles/SlurmCtl/templates/opt/slurm/cluster/etc/accounts.yml.example in the repository and is deployed to /opt/slurm/{{ClusterName}}/conf/accounts.yml . The file is a simple yaml file that allows you to configure groups, the users that belong to the group, and a fair share weight for the group. Refer to the Slurm documentation for details on how the fair share weight is calculated. The scheduler can be configured so that users who aren't getting their fair share of resources get higher priority. The following shows 3 top level groups. Note that the fairshare weights aren't a percentage. They are just a relative weight. In this example, the projects have 9 times higher weight than the jenkins group. jenkins: fairshare: 10 users: - jenkins project1: fairshare: 90 project2: fairshare: 90 The allocation of top level groups can be further subdivided to control the relative priority of jobs within that group. For example, a project may have design verification (dv), rtl design (rtl), physical design (dv), and formal verification (fv) teams. The following example shows how the project's allocation can be prioritized for the different teams. If a group is using more than it's fair share then its jobs will have lower priority than jobs whose users aren't getting their fair share. project1-dv: parent: project1 fairshare: 80 users: - dvuser1 project1-pd: parent: project1 fairshare: 10 users: - pduser1 project1-rtl: parent: project1 fairshare: 10 users: - rtluser1 project1-fv: parent: project1 fairshare: 10 users: - fvuser1 The scheduler uses the priority/multifactor plugin to calculate job priorities. Fair share is just one of the factors. Read the Multifactor Priority Plugin documentation for the details. This is the default configuration in slurm.conf. The partition weight is set the highest so that jobs in the interactive partition always have the highest priority. Fairshare and QOS are the next highest weighted factors. The next factor is the job age, which means all else being equal the jobs run in FIFO order with the jobs that have been waiting the longest getting higher priority. PriorityType=priority/multifactor PriorityWeightPartition=100000 PriorityWeightFairshare=10000 PriorityWeightQOS=10000 PriorityWeightAge=1000 PriorityWeightAssoc=0 PriorityWeightJobSize=0 These weights can be adjusted based on your needs to control job priorities. Configure Licenses Slurm supports configuring licenses as a consumable resource . It will keep track of how many running jobs are using a license and when no more licenses are available then jobs will stay pending in the queue until a job completes and frees up a license. Combined with the fairshare algorithm, this can prevent users from monopolizing licenses and preventing others from being able to run their jobs. The configuration file is at source/resources/playbooks/roles/SlurmCtl/templates/tools/slurm/etc/slurm_licenses.conf.example in the repository and is deployed to /opt/slurm/{{ClusterName}}/conf/accounts.yml . The example configuration shows how the number of licenses can be configured as just a comma separated list. In this example, the cluster will manage 800 vcs licenses and 1 ansys license. Users must request a license using the -L or --licenses options. Licenses=vcs:800,ansys:1 Create the Cluster To install the cluster run the install script. You can override some parameters in the config file with command line arguments, however it is better to specify all of the parameters in the config file. ./install.sh --config-file <config-file> --stack-name <stack-name> --cdk-cmd create Create Custom ParallelCluster AMI The ParallelCluster User Guide has instructions for customizing the AMI used by compute nodes. Use the Cluster Configuring your environment for users requires root privileges. The configuration commands are found in the outputs of the Slurm cloudformation stack. Configure Slurm Users and Groups The Slurm cluster needs to configure the users and groups of your environment. For efficiency, it does this by capturing the users and groups from your environment and saves them in a json file. When the compute nodes start they create local unix users and groups using this json file. Choose a single instance in your VPC that will always be running and that is joined to a domain so that it can list all users and groups. For SOCA this would be the Scheduler instance. Connect to that instance and run the commands in the MountCommand and ConfigureSyncSlurmUsersGroups outputs of the Slurm stack. These commands will mount the Slurm file system at /opt/slurm/{{ClusterName}} and then create a cron job that runs every 5 minutes and updates /opt/slurm/{{ClusterName}}/config/users_groups.json . Configure Slurm Submitter Instances Instances that need to submit to Slurm need to have their security group IDs in the SubmitterSecurityGroupIds configuration parameter so that the security groups allow communication between the submitter instances and the Slurm cluster. They also need to be configured by mounting the file system with the Slurm tools and configuring their environment. Connect to the submitter instance and run the commands in the MountCommand and ConfigureSubmitterCommand outputs of the Slurm stack. If all users need to use the cluster then it is probably best to create a custom AMI that is configured with the configuration commands. Run Your First Job Run the following command in a shell to configure your environment to use your slurm cluster. module load {{ClusterName}} To submit a job run the following command. sbatch /opt/slurm/$SLURM_CLUSTER_NAME/test/job_simple_array.sh To check the status run the following command. squeue To open an interactive shell on a slurm node. srun --pty /bin/bash Slurm Documentation https://slurm.schedmd.com","title":"Deploy Legacy Cluster"},{"location":"deploy-legacy-cluster/#deploy-legacy-cluster","text":"The original (legacy) version used a custom Slurm plugin for orchestrating the EC2 compute nodes. The latest version uses ParallelCluster to provision the core Slurm infrastructure. When using ParallelCluster, a ParallelCluster configuration will be generated and used to create a ParallelCluster slurm cluster. The first supported ParallelCluster version is 3.6.0. Version 3.7.0 is the recommended minimum version because it support compute node weighting that is proportional to instance type cost so that the least expensive instance types that meet job requirements are used.","title":"Deploy Legacy Cluster"},{"location":"deploy-legacy-cluster/#prerequisites","text":"","title":"Prerequisites"},{"location":"deploy-legacy-cluster/#configure-aws-cli-credentials","text":"You will needs AWS credentials that provide admin access to deploy the cluster.","title":"Configure AWS CLI Credentials"},{"location":"deploy-legacy-cluster/#clone-or-download-the-repository","text":"Clone or download the aws-eda-slurm-cluster repository to your system. git clone git@github.com:aws-samples/aws-eda-slurm-cluster.git","title":"Clone or Download the Repository"},{"location":"deploy-legacy-cluster/#make-sure-required-packages-are-installed","text":"cd aws-eda-slurm-cluster source setup.sh The setup script assumes that you have sudo access so that you can install or update packages. If you do not, then contact an administrator to help you do the updates. If necessary modify the setup script for your environment.","title":"Make sure required packages are installed"},{"location":"deploy-legacy-cluster/#create-sns-topic-for-error-notifications-optional-but-recommended","text":"The Slurm cluster allows you to specify an SNS notification that will be notified when an error is detected. You can provide the ARN for the topic in the config file or on the command line. You can use the SNS notification in various ways. The simplest is to subscribe your email address to the topic so that you get an email when there is an error. You could also use it to trigger a CloudWatch alarm that could be used to trigger a lambda to do automatic remediation or create a support ticket.","title":"Create SNS Topic for Error Notifications (Optional but recommended)"},{"location":"deploy-legacy-cluster/#deploy-using-parallelcluster","text":"","title":"Deploy Using ParallelCluster"},{"location":"deploy-legacy-cluster/#create-parallelcluster-ui-optional-but-recommended","text":"It is highly recommended to create a ParallelCluster UI to manage your ParallelCluster clusters. A different UI is required for each version of ParallelCluster that you are using. The versions are list in the ParallelCluster Release Notes . The minimum required version is 3.6.0 which adds support for RHEL 8 and increases the number of allows queues and compute resources. The suggested version is at least 3.7.0 because it adds configurable compute node weights which we use to prioritize the selection of compute nodes by their cost. The instructions are in the ParallelCluster User Guide .","title":"Create ParallelCluster UI (optional but recommended)"},{"location":"deploy-legacy-cluster/#create-parallelcluster-slurm-database","text":"The Slurm Database is required for configuring Slurm accounts, users, groups, and fair share scheduling. It you need these and other features then you will need to create a ParallelCluster Slurm Database. You do not need to create a new database for each cluster; multiple clusters can share the same database. Follow the directions in this ParallelCluster tutorial to configure slurm accounting .","title":"Create ParallelCluster Slurm Database"},{"location":"deploy-legacy-cluster/#create-configuration-file","text":"The first step in deploying your cluster is to create a configuration file. A default configuration file is found in source/resources/config/default_config.yml . You should create a new config file and update the parameters for your cluster. The schema for the config file along with its default values can be found in source/cdk/config_schema.py . The schema is defined in python, but the actual config file should be in yaml format. The following are key parameters that you will need to update. If you do not have the required parameters in your config file then the installer script will fail unless you specify the --prompt option. You should save your selections in the config file. Parameter Description Valid Values Default StackName The cloudformation stack that will deploy the cluster. None ClusterName Shouldn't be the same as StackName None Region Region where VPC is located $AWS_DEFAULT_REGION VpcId The vpc where the cluster will be deployed. vpc-* None SshKeyPair EC2 Keypair to use for instances None slurm/SubmitterSecurityGroupIds Existing security groups that can submit to the cluster. For SOCA this is the ComputeNodeSG* resource. sg-* None ErrorSnsTopicArn ARN of an SNS topic that will be notified of errors arn:aws:sns:{{region}}:{AccountId}:{TopicName} None The defaults for the following parameters are generally acceptable, but may be modified based on your requirements. Parameter Description Valid Values Default InstanceConfig Configures the instance families and types that the cluster can use. See default_config.yml","title":"Create Configuration File"},{"location":"deploy-legacy-cluster/#configure-the-compute-instances","text":"The InstanceConfig configuration parameter configures the base operating systems, CPU architectures, instance families, and instance types that the Slurm cluster should support. ParallelCluster currently doesn't support heterogeneous cluster; all nodes must have the same architecture and Base OS. The supported OSes and CPU architectures are: Base OS CPU Architectures Amazon Linux 2 x86_64, arm64 CentOS 7 x86_64 RedHat 7 x86_64 RedHat 8 x86_64, arm64 You can exclude instances types by family or specific instance type. By default the InstanceConfig excludes older generation instance families. You can include instances by family or specific instance type. If no includes are specified then all non-excluded instance types will be used. You can also choose to only include the largest instance size within a family. The advantage of using the max instance size is that jobs running on the instance have the highest network bandwidth for that family and fewer instances are required to run the same number of jobs. This may help jobs run faster and allow jobs to wait less time for a new instance to start. The disadvantage is higher cost if the instance is lightly loaded. The default InstanceConfig includes all supported base OSes and architectures and burstable and general purpose instance types. Note that instance types and families are python regular expressions. slurm: InstanceConfig: BaseOsArchitecture: CentOS: 7: [x86_64] Include: InstanceFamilies: - t3.* - m6a.* InstanceTypes: - r6a.large The following InstanceConfig configures instance types recommended for EDA workloads running on CentOS. slurm: InstanceConfig: BaseOsArchitecture: CentOS: 7: [x86_64] Include: InstanceFamilies: - c5.* - c6g.* - f1 - m5.* - m6g.* - r5.* - r6g.* - x2gd - z1d If you have reserved instances (RIs) or savings plans then you can configure instances so that they are always on since you are paying for them whether they are running or not. To do this add a MinCount greater than 0 for the compute resources that contain the instance types. slurm: InstanceConfig: NodeCounts: DefaultMinCount: 1","title":"Configure the Compute Instances"},{"location":"deploy-legacy-cluster/#create-the-cluster","text":"To install the cluster run the install script. You can override some parameters in the config file with command line arguments, however it is better to specify all of the parameters in the config file. ./install.sh --config-file <config-file> --cdk-cmd create This will create the ParallelCuster configuration file, store it in S3, and then use a lambda function to create the cluster. If you look in CloudFormation you will see 2 new stacks when deployment is finished. The first is the configuration stack and the second is the cluster.","title":"Create the Cluster"},{"location":"deploy-legacy-cluster/#create-users_groupsjson","text":"Before you can use the cluster you must configure the Linux users and groups for the head and compute nodes. One way to do that would be to join the cluster to your domain. But joining each compute node to a domain effectively creates a distributed denial of service (DDOS) attack on the demain controller when the cluster rapidly scales out or in and each node tries to join or leave the domain. This can lead to domain controller timeouts and widespread havoc in your environment. To solve this problem a script runs on a server that is joined to the domain which writes a JSON file with all of the non-privileged users and groups and their respective uids and gids. A script and cron job on the head and compute nodes reads this json file to create local users and groups that match the domain-joined servers. Select the server that you want to use to create and update the JSON file. The outputs of the configuration stack have the commands required. Config Stack Output Description Command01SubmitterMountHeadNode Mounts the Slurm cluster's shared file system, adds it to /etc/fstab. Command02CreateUsersGroupsJsonConfigure Create /opt/slurm/{{ClusterName}}/config/users_groups.json and create a cron job to refresh it hourly. Before deleting the cluster you can undo the configuration by running the commands in the following outputs. Config Stack Output Description command10CreateUsersGroupsJsonDeconfigure Removes the crontab that refreshes users_groups.json. Now the cluster is ready to be used by sshing into the head node or a login node, if you configured one. If you configured extra file systems for the cluster that contain the users' home directories, then they should be able to ssh in with their own ssh keys.","title":"Create users_groups.json"},{"location":"deploy-legacy-cluster/#configure-submission-hosts-to-use-the-cluster","text":"ParallelCluster was built assuming that users would ssh into the head node or login nodes to execute Slurm commands. This can be undesirable for a number of reasons. First, users shouldn't be given ssh access to a critical infrastructure like the cluster head node. With ParallelCluster 3.7.0 you can configure login nodes, but if you have already provisioned desktop nodes then it's wasteful to have to provision login nodes. Second, it's just inconvenient to have to use ssh to access the cluster and use it. Fortunately, you can configure any server as a submission host so that users can run slurm commands. These commands must be run by an administrator that has root access to the submission host. The commands could also be run to create a custom AMI for user desktops so that they can access the clusters. The commands to configure submission hosts are in the outputs of the configuration CloudFormation stack. Run them in the following order: Config Stack Output Description Command01SubmitterMountHeadNode Mounts the Slurm cluster's shared file system, adds it to /etc/fstab. Command03SubmitterConfigure Configure the submission host so it can directly access the Slurm cluster. The first command simply mounts the head node's NFS file system so you have access to the Slurm commands and configuration. The second command runs an ansible playbook that configures the submission host so that it can run the Slurm commands for the cluster. It also configures the modulefile that sets up the environment to use the slurm cluster. The clusters have been configured so that a submission host can use more than one cluster by simply changing the modulefile that is loaded. On the submission host just open a new shell and load the modulefile for your cluster and you can access Slurm.","title":"Configure submission hosts to use the cluster"},{"location":"deploy-legacy-cluster/#customize-the-compute-node-ami","text":"The easiest way to create a custom AMI is to find the default ParallelCluster AMI in the UI. Create an instance using the AMI and make whatever customizations you require such as installing packages and configuring users and groups. Custom file system mounts can be configured in the aws-eda-slurm-cluster config file which will add it to the ParallelCluster config file so that ParallelCluster can manage them for you. When you are done create a new AMI and wait for the AMI to become available. After it is available you can add the custom ami to the aws-eda-slurm-cluster config file. slurm: ParallelClusterConfig: ComputeNodeAmi: ami-0fdb972bda05d2932 Then update your aws-eda-slurm-cluster stack by running the install script again.","title":"Customize the compute node AMI"},{"location":"deploy-legacy-cluster/#deploy-using-legacy-cluster","text":"","title":"Deploy using legacy cluster"},{"location":"deploy-legacy-cluster/#subscribe-to-aws-marketplace-amis-legacy","text":"This is only required for the Legacy scheduler. It is not required for ParallelCluster which uses it's own public AMIs. Subscribe to the MarketPlace AMIs you will use in your cluster. Examples are: Alma Linux 8 - arm64 Alma Linux 8 - x86_64 AWS FPGA Developer AMI - CentOS 7 -x86_64 AWS FPGA Developer AMI - Amazon Linux 2 - x86_64 CentOS 7 - x86_64 Rocky Linux 8 - arm64 Rocky Linux 8 - x86_64","title":"Subscribe to AWS MarketPlace AMIs (Legacy)"},{"location":"deploy-legacy-cluster/#quick-minimal-deployment","text":"The install script can create a minimal Slurm cluster using the default configuration file in the repository and it will prompt you for the required parameters. You will first need to configure your AWS credentials. The installer now defaults to using the ParallelCluster version. cd edaslurmcluster ./install.sh --prompt --cdk-cmd create","title":"Quick Minimal Deployment"},{"location":"deploy-legacy-cluster/#install-cloud-development-kit-cdk-optional","text":"The install script will attempt to install all of the prerequisites for you. If the install script fails on your system then you can refer to this section for instructions on how to install or update CDK. This cluster uses Cloud Development Kit (CDK) and Python 3 to deploy the cluster. Install the packages used by the installer. sudo yum -y install curl gcc-c++ make nfs-utils python3 tcl unzip wget The following link documents how to setup for CDK. Follow the instructions for Python. https://docs.aws.amazon.com/cdk/v2/guide/getting_started.html#getting_started_prerequisites Note that CDK requires a pretty new version of nodejs which you may have to download from, for example, https://nodejs.org/dist/v16.13.1/node-v16.13.1-linux-x64.tar.xz sudo yum -y install wget wget https://nodejs.org/dist/v16.13.1/node-v16.13.1-linux-x64.tar.xz tar -xf node-v16.13.1-linux-x64.tar.xz ~ Add the nodjs bin directory to your path. https://docs.aws.amazon.com/cdk/v2/guide/getting_started.html#getting_started_install Note that the version of aws-cdk changes frequently. The version that has been tested is in the CDK_VERSION variable in the install script. The install script will try to install the prerequisites if they aren't already installed.","title":"Install Cloud Development Kit (CDK) (Optional)"},{"location":"deploy-legacy-cluster/#configuration-file","text":"The first step in deploying your cluster is to create a configuration file. A default configuration file is found in source/resources/config/default_config.yml . You should create a new config file and update the parameters for your cluster. The schema for the config file along with its default values can be found in source/cdk/config_schema.py . The schema is defined in python, but the actual config file should be in yaml format. The following are key parameters that you will need to update. If you do not have the required parameters in your config file then the installer script will fail unless you specify the --prompt option. You should save your selections in the config file. Parameter Description Valid Values Default StackName The cloudformation stack that will deploy the cluster. None VpcId The vpc where the cluster will be deployed. vpc-* None Region Region where VPC is located $AWS_DEFAULT_REGION SshKeyPair EC2 Keypair to use for instances None slurm/SubmitterSecurityGroupIds Existing security groups that can submit to the cluster. For SOCA this is the ComputeNodeSG* resource. sg-* None ErrorSnsTopicArn ARN of an SNS topic that will be notified of errors arn:aws:sns:{{region}}:{AccountId}:{TopicName} None The defaults for the following parameters are generally acceptable, but may be modified based on your requirements. Parameter Description Valid Values Default slurm/SlurmDbd Create a slurmdbd instance connected to an RDS Serverless database. No database. InstanceConfig Configures the instance families and types that the cluster can use. See default_config.yml NumberOfControllers For high availability you can have 2 or 3 controllers. 1-3 1 SuspendAction What to do to an idle instance. Stopped instances will restart faster, but still incur EBS charges while stopped. stop or terminate stop MaxStoppedDuration You can configure how long instances can be stopped before they are automatically terminated. The default is set to 1 hour. This is checked at least hourly. The format uses the ISO 8601 duration format. PnYnMnDTnHnMnS P0Y0M0DT1H0M0S CloudWatchPeriod Default: 5. Set to 1 for finer metric resolution.","title":"Configuration File"},{"location":"deploy-legacy-cluster/#configure-the-compute-instances_1","text":"The InstanceConfig configuration parameter configures the base operating systems, CPU architectures, instance families, and instance types that the Slurm cluster should support. The supported OSes and CPU architectures are:","title":"Configure the Compute Instances"},{"location":"deploy-legacy-cluster/#parallelcluster","text":"Base OS CPU Architectures Amazon Linux 2 x86_64, arm64 CentOS 7 x86_64 RedHat 7 x86_64 RedHat 8 x86_64, arm64","title":"ParallelCluster"},{"location":"deploy-legacy-cluster/#legacy","text":"Base OS CPU Architectures Alma Linux 8 x86_64, arm64 Amazon Linux 2 x86_64, arm64 CentOS 7 x86_64 RedHat 7 x86_64 RedHat 8 x86_64, arm64 Rocky Linux 8 x86_64, arm64 You can exclude instances types by family or specific instance type. By default the InstanceConfig excludes older generation instance families. You can include instances by family or specific instance type. If no includes are specified then all non-excluded instance types will be used. You can also choose to only include the largest instance size within a family. The advantage of using the max instance size is that jobs running on the instance have the highest network bandwidth for that family and fewer instances are required to run the same number of jobs. This may help jobs run faster and allow jobs to wait less time for a new instance to start. The disadvantage is higher cost if the instance is lightly loaded. The default InstanceConfig includes all supported base OSes and architectures and burstable and general purpose instance types. Note that instance types and families are python regular expressions. slurm: InstanceConfig: DefaultPartition: CentOS_7_x86_64 BaseOsArchitecture: AlmaLinux: 8: [x86_64, arm64] CentOS: 7: [x86_64] Amazon: {2: [x86_64, arm64]} RedHat: 7: [x86_64] 8: [x86_64, arm64] Include: MaxSizeOnly: false InstanceFamilies: - t3.* - t4g - m5.* InstanceTypes: [] Exclude: InstanceTypes: - '.+\\.(micro|nano)' # Not enough memory - '.*\\.metal' The following InstanceConfig configures instance types recommended for EDA workloads running on CentOS. slurm: InstanceConfig: DefaultPartition: CentOS_7_x86_64 BaseOsArchitecture: AlmaLinux: {8: [x86_64, arm64]} CentOS: 7: [x86_64] Include: MaxSizeOnly: false InstanceFamilies: - c5.* - c6g.* - f1 - m5.* - m6g.* - r5.* - r6g.* - x2gd - z1d InstanceTypes: [] Exclude: InstanceTypes: - '.*\\.metal' If you have reserved instances (RIs) or savings plans then you can configure instances so that they are always on since you are paying for them whether they are running or not. slurm: InstanceConfig: AlwaysOnNodes: - nodename-[0-4]","title":"Legacy"},{"location":"deploy-legacy-cluster/#update-to-latest-base-operating-system-amis-optional","text":"The default configuration includes the latest AMIs that have been tested. If you want to use the latest base OS AMIs, then configure your AWS cli credentials and run the following script. Note : Updating the AMIs to a newer version may break deployment if repositories and package versions have changed from the tested version. ./source/create-ami-map.py > source/resources/config/ami_map.yml","title":"Update to Latest Base Operating System AMIs (Optional)"},{"location":"deploy-legacy-cluster/#use-your-own-amis-optional","text":"You may already have base AMIs that are configured for your environment. To use them update the SlurmNodeAmis configuration parameter. The parameter is a map with the keys being the region, base OS, and CPU architecture as the keys. So, for example, to use a custom CentOS 7 AMI in the us-east-1 region you would have: slurm: SlurmNodeAmis: us-east-1: CentOS: 7: x86_64: ami-xxxxxxxxxxxxxxxxx Another example is to use the AWS FPGA Developer AMI as the base AMI for your compute nodes so that you can use the Xilinx Vivado tools for FPGA development for AWS F1 instances. By default the EBS volumes are created with the same sizes as in the AMI. You can increase the size of the root EBS volume as shown in this example. This is useful if the root volume needs additional space to install additional packages or tools. SlurmNodeAmis: # Customized AMIs with file system mounts, packages, etc. configured. BaseAmis: us-east-1: Amazon: {2: {x86_64: {ImageId: ami-0efdec76678df9a64, RootDeviceSize: '+5'}}} CentOS: {7: {x86_64: {ImageId: ami-02155c6289e76719a, RootDeviceSize: '+5'}}}","title":"Use Your Own AMIs (Optional)"},{"location":"deploy-legacy-cluster/#configure-fair-share-scheduling-optional","text":"Slurm supports fair share scheduling , but it requires the fair share policy to be configured. By default, all users will be put into a default group that has a low fair share. The configuration file is at source/resources/playbooks/roles/SlurmCtl/templates/opt/slurm/cluster/etc/accounts.yml.example in the repository and is deployed to /opt/slurm/{{ClusterName}}/conf/accounts.yml . The file is a simple yaml file that allows you to configure groups, the users that belong to the group, and a fair share weight for the group. Refer to the Slurm documentation for details on how the fair share weight is calculated. The scheduler can be configured so that users who aren't getting their fair share of resources get higher priority. The following shows 3 top level groups. Note that the fairshare weights aren't a percentage. They are just a relative weight. In this example, the projects have 9 times higher weight than the jenkins group. jenkins: fairshare: 10 users: - jenkins project1: fairshare: 90 project2: fairshare: 90 The allocation of top level groups can be further subdivided to control the relative priority of jobs within that group. For example, a project may have design verification (dv), rtl design (rtl), physical design (dv), and formal verification (fv) teams. The following example shows how the project's allocation can be prioritized for the different teams. If a group is using more than it's fair share then its jobs will have lower priority than jobs whose users aren't getting their fair share. project1-dv: parent: project1 fairshare: 80 users: - dvuser1 project1-pd: parent: project1 fairshare: 10 users: - pduser1 project1-rtl: parent: project1 fairshare: 10 users: - rtluser1 project1-fv: parent: project1 fairshare: 10 users: - fvuser1 The scheduler uses the priority/multifactor plugin to calculate job priorities. Fair share is just one of the factors. Read the Multifactor Priority Plugin documentation for the details. This is the default configuration in slurm.conf. The partition weight is set the highest so that jobs in the interactive partition always have the highest priority. Fairshare and QOS are the next highest weighted factors. The next factor is the job age, which means all else being equal the jobs run in FIFO order with the jobs that have been waiting the longest getting higher priority. PriorityType=priority/multifactor PriorityWeightPartition=100000 PriorityWeightFairshare=10000 PriorityWeightQOS=10000 PriorityWeightAge=1000 PriorityWeightAssoc=0 PriorityWeightJobSize=0 These weights can be adjusted based on your needs to control job priorities.","title":"Configure Fair Share Scheduling (Optional)"},{"location":"deploy-legacy-cluster/#configure-licenses","text":"Slurm supports configuring licenses as a consumable resource . It will keep track of how many running jobs are using a license and when no more licenses are available then jobs will stay pending in the queue until a job completes and frees up a license. Combined with the fairshare algorithm, this can prevent users from monopolizing licenses and preventing others from being able to run their jobs. The configuration file is at source/resources/playbooks/roles/SlurmCtl/templates/tools/slurm/etc/slurm_licenses.conf.example in the repository and is deployed to /opt/slurm/{{ClusterName}}/conf/accounts.yml . The example configuration shows how the number of licenses can be configured as just a comma separated list. In this example, the cluster will manage 800 vcs licenses and 1 ansys license. Users must request a license using the -L or --licenses options. Licenses=vcs:800,ansys:1","title":"Configure Licenses"},{"location":"deploy-legacy-cluster/#create-the-cluster_1","text":"To install the cluster run the install script. You can override some parameters in the config file with command line arguments, however it is better to specify all of the parameters in the config file. ./install.sh --config-file <config-file> --stack-name <stack-name> --cdk-cmd create","title":"Create the Cluster"},{"location":"deploy-legacy-cluster/#create-custom-parallelcluster-ami","text":"The ParallelCluster User Guide has instructions for customizing the AMI used by compute nodes.","title":"Create Custom ParallelCluster AMI"},{"location":"deploy-legacy-cluster/#use-the-cluster","text":"Configuring your environment for users requires root privileges. The configuration commands are found in the outputs of the Slurm cloudformation stack.","title":"Use the Cluster"},{"location":"deploy-legacy-cluster/#configure-slurm-users-and-groups","text":"The Slurm cluster needs to configure the users and groups of your environment. For efficiency, it does this by capturing the users and groups from your environment and saves them in a json file. When the compute nodes start they create local unix users and groups using this json file. Choose a single instance in your VPC that will always be running and that is joined to a domain so that it can list all users and groups. For SOCA this would be the Scheduler instance. Connect to that instance and run the commands in the MountCommand and ConfigureSyncSlurmUsersGroups outputs of the Slurm stack. These commands will mount the Slurm file system at /opt/slurm/{{ClusterName}} and then create a cron job that runs every 5 minutes and updates /opt/slurm/{{ClusterName}}/config/users_groups.json .","title":"Configure Slurm Users and Groups"},{"location":"deploy-legacy-cluster/#configure-slurm-submitter-instances","text":"Instances that need to submit to Slurm need to have their security group IDs in the SubmitterSecurityGroupIds configuration parameter so that the security groups allow communication between the submitter instances and the Slurm cluster. They also need to be configured by mounting the file system with the Slurm tools and configuring their environment. Connect to the submitter instance and run the commands in the MountCommand and ConfigureSubmitterCommand outputs of the Slurm stack. If all users need to use the cluster then it is probably best to create a custom AMI that is configured with the configuration commands.","title":"Configure Slurm Submitter Instances"},{"location":"deploy-legacy-cluster/#run-your-first-job","text":"Run the following command in a shell to configure your environment to use your slurm cluster. module load {{ClusterName}} To submit a job run the following command. sbatch /opt/slurm/$SLURM_CLUSTER_NAME/test/job_simple_array.sh To check the status run the following command. squeue To open an interactive shell on a slurm node. srun --pty /bin/bash","title":"Run Your First Job"},{"location":"deploy-legacy-cluster/#slurm-documentation","text":"https://slurm.schedmd.com","title":"Slurm Documentation"},{"location":"deploy-parallel-cluster/","text":"Deploy ParallelCluster The original (legacy) version used a custom Slurm plugin for orchestrating the EC2 compute nodes. The latest version uses ParallelCluster to provision the core Slurm infrastructure. When using ParallelCluster, a ParallelCluster configuration will be generated and used to create a ParallelCluster slurm cluster. The first supported ParallelCluster version is 3.6.0. Version 3.7.0 is the recommended minimum version because it support compute node weighting that is proportional to instance type cost so that the least expensive instance types that meet job requirements are used. Prerequisites See Deployment Prerequisites page. The following are prerequisites that are specific to ParallelCluster. Create ParallelCluster UI (optional but recommended) It is highly recommended to create a ParallelCluster UI to manage your ParallelCluster clusters. A different UI is required for each version of ParallelCluster that you are using. The versions are list in the ParallelCluster Release Notes . The minimum required version is 3.6.0 which adds support for RHEL 8 and increases the number of allows queues and compute resources. The suggested version is at least 3.7.0 because it adds configurable compute node weights which we use to prioritize the selection of compute nodes by their cost. The instructions are in the ParallelCluster User Guide . Create ParallelCluster Slurm Database The Slurm Database is required for configuring Slurm accounts, users, groups, and fair share scheduling. It you need these and other features then you will need to create a ParallelCluster Slurm Database. You do not need to create a new database for each cluster; multiple clusters can share the same database. Follow the directions in this ParallelCluster tutorial to configure slurm accounting . Create the Cluster To install the cluster run the install script. You can override some parameters in the config file with command line arguments, however it is better to specify all of the parameters in the config file. ./install.sh --config-file <config-file> --cdk-cmd create This will create the ParallelCuster configuration file, store it in S3, and then use a lambda function to create the cluster. If you look in CloudFormation you will see 2 new stacks when deployment is finished. The first is the configuration stack and the second is the cluster. Create users_groups.json Before you can use the cluster you must configure the Linux users and groups for the head and compute nodes. One way to do that would be to join the cluster to your domain. But joining each compute node to a domain effectively creates a distributed denial of service (DDOS) attack on the demain controller when the cluster rapidly scales out or in and each node tries to join or leave the domain. This can lead to domain controller timeouts and widespread havoc in your environment. To solve this problem a script runs on a server that is joined to the domain which writes a JSON file with all of the non-privileged users and groups and their respective uids and gids. A script and cron job on the head and compute nodes reads this json file to create local users and groups that match the domain-joined servers. Select the server that you want to use to create and update the JSON file. The outputs of the configuration stack have the commands required. Config Stack Output Description Command01SubmitterMountHeadNode Mounts the Slurm cluster's shared file system, adds it to /etc/fstab. Command02CreateUsersGroupsJsonConfigure Create /opt/slurm/{{ClusterName}}/config/users_groups.json and create a cron job to refresh it hourly. Before deleting the cluster you can undo the configuration by running the commands in the following outputs. Config Stack Output Description command10CreateUsersGroupsJsonDeconfigure Removes the crontab that refreshes users_groups.json. Now the cluster is ready to be used by sshing into the head node or a login node, if you configured one. If you configured extra file systems for the cluster that contain the users' home directories, then they should be able to ssh in with their own ssh keys. Configure submission hosts to use the cluster ParallelCluster was built assuming that users would ssh into the head node or login nodes to execute Slurm commands. This can be undesirable for a number of reasons. First, users shouldn't be given ssh access to a critical infrastructure like the cluster head node. With ParallelCluster 3.7.0 you can configure login nodes, but if you have already provisioned desktop nodes then it's wasteful to have to provision login nodes. Second, it's just inconvenient to have to use ssh to access the cluster and use it. Fortunately, you can configure any server as a submission host so that users can run slurm commands. These commands must be run by an administrator that has root access to the submission host. The commands could also be run to create a custom AMI for user desktops so that they can access the clusters. The commands to configure submission hosts are in the outputs of the configuration CloudFormation stack. Run them in the following order: Config Stack Output Description Command01SubmitterMountHeadNode Mounts the Slurm cluster's shared file system, adds it to /etc/fstab. Command03SubmitterConfigure Configure the submission host so it can directly access the Slurm cluster. The first command simply mounts the head node's NFS file system so you have access to the Slurm commands and configuration. The second command runs an ansible playbook that configures the submission host so that it can run the Slurm commands for the cluster. It also configures the modulefile that sets up the environment to use the slurm cluster. The clusters have been configured so that a submission host can use more than one cluster by simply changing the modulefile that is loaded. On the submission host just open a new shell and load the modulefile for your cluster and you can access Slurm. Customize the compute node AMI The easiest way to create a custom AMI is to find the default ParallelCluster AMI in the UI. Create an instance using the AMI and make whatever customizations you require such as installing packages and configuring users and groups. Custom file system mounts can be configured in the aws-eda-slurm-cluster config file which will add it to the ParallelCluster config file so that ParallelCluster can manage them for you. When you are done create a new AMI and wait for the AMI to become available. After it is available you can add the custom ami to the aws-eda-slurm-cluster config file. slurm: ParallelClusterConfig: ComputeNodeAmi: ami-0fdb972bda05d2932 Then update your aws-eda-slurm-cluster stack by running the install script again. Run Your First Job Run the following command in a shell to configure your environment to use your slurm cluster. module load {{ClusterName}} To submit a job run the following command. sbatch /opt/slurm/$SLURM_CLUSTER_NAME/test/job_simple_array.sh To check the status run the following command. squeue To open an interactive shell on a slurm node. srun --pty /bin/bash Slurm Documentation https://slurm.schedmd.com","title":"Deploy ParallelCluster"},{"location":"deploy-parallel-cluster/#deploy-parallelcluster","text":"The original (legacy) version used a custom Slurm plugin for orchestrating the EC2 compute nodes. The latest version uses ParallelCluster to provision the core Slurm infrastructure. When using ParallelCluster, a ParallelCluster configuration will be generated and used to create a ParallelCluster slurm cluster. The first supported ParallelCluster version is 3.6.0. Version 3.7.0 is the recommended minimum version because it support compute node weighting that is proportional to instance type cost so that the least expensive instance types that meet job requirements are used.","title":"Deploy ParallelCluster"},{"location":"deploy-parallel-cluster/#prerequisites","text":"See Deployment Prerequisites page. The following are prerequisites that are specific to ParallelCluster.","title":"Prerequisites"},{"location":"deploy-parallel-cluster/#create-parallelcluster-ui-optional-but-recommended","text":"It is highly recommended to create a ParallelCluster UI to manage your ParallelCluster clusters. A different UI is required for each version of ParallelCluster that you are using. The versions are list in the ParallelCluster Release Notes . The minimum required version is 3.6.0 which adds support for RHEL 8 and increases the number of allows queues and compute resources. The suggested version is at least 3.7.0 because it adds configurable compute node weights which we use to prioritize the selection of compute nodes by their cost. The instructions are in the ParallelCluster User Guide .","title":"Create ParallelCluster UI (optional but recommended)"},{"location":"deploy-parallel-cluster/#create-parallelcluster-slurm-database","text":"The Slurm Database is required for configuring Slurm accounts, users, groups, and fair share scheduling. It you need these and other features then you will need to create a ParallelCluster Slurm Database. You do not need to create a new database for each cluster; multiple clusters can share the same database. Follow the directions in this ParallelCluster tutorial to configure slurm accounting .","title":"Create ParallelCluster Slurm Database"},{"location":"deploy-parallel-cluster/#create-the-cluster","text":"To install the cluster run the install script. You can override some parameters in the config file with command line arguments, however it is better to specify all of the parameters in the config file. ./install.sh --config-file <config-file> --cdk-cmd create This will create the ParallelCuster configuration file, store it in S3, and then use a lambda function to create the cluster. If you look in CloudFormation you will see 2 new stacks when deployment is finished. The first is the configuration stack and the second is the cluster.","title":"Create the Cluster"},{"location":"deploy-parallel-cluster/#create-users_groupsjson","text":"Before you can use the cluster you must configure the Linux users and groups for the head and compute nodes. One way to do that would be to join the cluster to your domain. But joining each compute node to a domain effectively creates a distributed denial of service (DDOS) attack on the demain controller when the cluster rapidly scales out or in and each node tries to join or leave the domain. This can lead to domain controller timeouts and widespread havoc in your environment. To solve this problem a script runs on a server that is joined to the domain which writes a JSON file with all of the non-privileged users and groups and their respective uids and gids. A script and cron job on the head and compute nodes reads this json file to create local users and groups that match the domain-joined servers. Select the server that you want to use to create and update the JSON file. The outputs of the configuration stack have the commands required. Config Stack Output Description Command01SubmitterMountHeadNode Mounts the Slurm cluster's shared file system, adds it to /etc/fstab. Command02CreateUsersGroupsJsonConfigure Create /opt/slurm/{{ClusterName}}/config/users_groups.json and create a cron job to refresh it hourly. Before deleting the cluster you can undo the configuration by running the commands in the following outputs. Config Stack Output Description command10CreateUsersGroupsJsonDeconfigure Removes the crontab that refreshes users_groups.json. Now the cluster is ready to be used by sshing into the head node or a login node, if you configured one. If you configured extra file systems for the cluster that contain the users' home directories, then they should be able to ssh in with their own ssh keys.","title":"Create users_groups.json"},{"location":"deploy-parallel-cluster/#configure-submission-hosts-to-use-the-cluster","text":"ParallelCluster was built assuming that users would ssh into the head node or login nodes to execute Slurm commands. This can be undesirable for a number of reasons. First, users shouldn't be given ssh access to a critical infrastructure like the cluster head node. With ParallelCluster 3.7.0 you can configure login nodes, but if you have already provisioned desktop nodes then it's wasteful to have to provision login nodes. Second, it's just inconvenient to have to use ssh to access the cluster and use it. Fortunately, you can configure any server as a submission host so that users can run slurm commands. These commands must be run by an administrator that has root access to the submission host. The commands could also be run to create a custom AMI for user desktops so that they can access the clusters. The commands to configure submission hosts are in the outputs of the configuration CloudFormation stack. Run them in the following order: Config Stack Output Description Command01SubmitterMountHeadNode Mounts the Slurm cluster's shared file system, adds it to /etc/fstab. Command03SubmitterConfigure Configure the submission host so it can directly access the Slurm cluster. The first command simply mounts the head node's NFS file system so you have access to the Slurm commands and configuration. The second command runs an ansible playbook that configures the submission host so that it can run the Slurm commands for the cluster. It also configures the modulefile that sets up the environment to use the slurm cluster. The clusters have been configured so that a submission host can use more than one cluster by simply changing the modulefile that is loaded. On the submission host just open a new shell and load the modulefile for your cluster and you can access Slurm.","title":"Configure submission hosts to use the cluster"},{"location":"deploy-parallel-cluster/#customize-the-compute-node-ami","text":"The easiest way to create a custom AMI is to find the default ParallelCluster AMI in the UI. Create an instance using the AMI and make whatever customizations you require such as installing packages and configuring users and groups. Custom file system mounts can be configured in the aws-eda-slurm-cluster config file which will add it to the ParallelCluster config file so that ParallelCluster can manage them for you. When you are done create a new AMI and wait for the AMI to become available. After it is available you can add the custom ami to the aws-eda-slurm-cluster config file. slurm: ParallelClusterConfig: ComputeNodeAmi: ami-0fdb972bda05d2932 Then update your aws-eda-slurm-cluster stack by running the install script again.","title":"Customize the compute node AMI"},{"location":"deploy-parallel-cluster/#run-your-first-job","text":"Run the following command in a shell to configure your environment to use your slurm cluster. module load {{ClusterName}} To submit a job run the following command. sbatch /opt/slurm/$SLURM_CLUSTER_NAME/test/job_simple_array.sh To check the status run the following command. squeue To open an interactive shell on a slurm node. srun --pty /bin/bash","title":"Run Your First Job"},{"location":"deploy-parallel-cluster/#slurm-documentation","text":"https://slurm.schedmd.com","title":"Slurm Documentation"},{"location":"deployment-prerequisites/","text":"Deployment Prerequisites The original (legacy) version used a custom Slurm plugin for orchestrating the EC2 compute nodes. The latest version uses ParallelCluster to provision the core Slurm infrastructure. This page shows common prerequisites that need to be done for deploying either version. Configure AWS CLI Credentials You will needs AWS credentials that provide admin access to deploy the cluster. Clone or Download the Repository Clone or download the aws-eda-slurm-cluster repository to your system. git clone git@github.com:aws-samples/aws-eda-slurm-cluster.git Create SNS Topic for Error Notifications (Optional but recommended) The Slurm cluster allows you to specify an SNS notification that will be notified when an error is detected. You can provide the ARN for the topic in the config file or on the command line. You can use the SNS notification in various ways. The simplest is to subscribe your email address to the topic so that you get an email when there is an error. You could also use it to trigger a CloudWatch alarm that could be used to trigger a lambda to do automatic remediation or create a support ticket. Make sure required packages are installed cd aws-eda-slurm-cluster source setup.sh The setup script assumes that you have sudo access so that you can install or update packages. If you do not, then contact an administrator to help you do the updates. If necessary modify the setup script for your environment. Install Cloud Development Kit (CDK) (Optional) The setup script will attempt to install all of the prerequisites for you. If the install script fails on your system then you can refer to this section for instructions on how to install or update CDK. This cluster uses Cloud Development Kit (CDK) and Python 3 to deploy the cluster. Install the packages used by the installer. sudo yum -y install curl gcc-c++ make nfs-utils python3 tcl unzip wget The following link documents how to setup for CDK. Follow the instructions for Python. https://docs.aws.amazon.com/cdk/v2/guide/getting_started.html#getting_started_prerequisites Note that CDK requires a pretty new version of nodejs which you may have to download from, for example, https://nodejs.org/dist/v16.13.1/node-v16.13.1-linux-x64.tar.xz sudo yum -y install wget wget https://nodejs.org/dist/v16.13.1/node-v16.13.1-linux-x64.tar.xz tar -xf node-v16.13.1-linux-x64.tar.xz ~ Add the nodjs bin directory to your path. https://docs.aws.amazon.com/cdk/v2/guide/getting_started.html#getting_started_install Note that the version of aws-cdk changes frequently. The version that has been tested is in the CDK_VERSION variable in the install script. The install script will try to install the prerequisites if they aren't already installed. Create Configuration File Before you deploy a cluster you need to create a configuration file. A default configuration file is found in source/resources/config/default_config.yml . You should create a new config file and update the parameters for your cluster. Ideally you should version control this file so you can keep track of changes. The schema for the config file along with its default values can be found in source/cdk/config_schema.py . The schema is defined in python, but the actual config file should be in yaml format. The following are key parameters that you will need to update. If you do not have the required parameters in your config file then the installer script will fail unless you specify the --prompt option. You should save your selections in the config file. Parameter Description Valid Values Default StackName ] The cloudformation stack that will deploy the cluster. None slurm/ClusterName Name of the Slurm cluster For ParallelCluster shouldn't be the same as StackName Region Region where VPC is located $AWS_DEFAULT_REGION VpcId The vpc where the cluster will be deployed. vpc-* None SshKeyPair EC2 Keypair to use for instances None slurm/SubmitterSecurityGroupIds Existing security groups that can submit to the cluster. For SOCA this is the ComputeNodeSG* resource. sg-* None ErrorSnsTopicArn ARN of an SNS topic that will be notified of errors arn:aws:sns:{{region}}:{AccountId}:{TopicName} None slurm/ParallelClusterConfig/enable Use ParallelCluster true, false false slurm/InstanceConfig Configure instance types that the cluster can use and number of nodes. See default_config.yml Configure the Compute Instances The slurm/InstanceConfig configuration parameter configures the base operating systems, CPU architectures, instance families, and instance types that the Slurm cluster should support. ParallelCluster currently doesn't support heterogeneous clusters; all nodes must have the same architecture and Base OS. ParallelCluster Base OS CPU Architectures Amazon Linux 2 x86_64, arm64 CentOS 7 x86_64 RedHat 7 x86_64 RedHat 8 x86_64, arm64 Legacy Base OS CPU Architectures Alma Linux 8 x86_64, arm64 Amazon Linux 2 x86_64, arm64 CentOS 7 x86_64 RedHat 7 x86_64 RedHat 8 x86_64, arm64 Rocky Linux 8 x86_64, arm64 You can exclude instances types by family or specific instance type. By default the InstanceConfig excludes older generation instance families. You can include instances by family or specific instance type. If no includes are specified then all non-excluded instance types will be used. You can also choose to only include the largest instance size within a family. The advantage of using the max instance size is that jobs running on the instance have the highest network bandwidth for that family and fewer instances are required to run the same number of jobs. This may help jobs run faster and allow jobs to wait less time for a new instance to start. The disadvantage is higher cost if the instance is lightly loaded. The default InstanceConfig includes all supported base OSes and architectures and burstable and general purpose instance types. default instance families default instance types default excluded instance families default excluded instance types Note that instance types and families are python regular expressions. slurm: InstanceConfig: BaseOsArchitecture: CentOS: 7: [x86_64] Include: InstanceFamilies: - t3.* - m6a.* InstanceTypes: - r6a.large The following InstanceConfig configures instance types recommended for EDA workloads running on CentOS. slurm: InstanceConfig: BaseOsArchitecture: CentOS: 7: [x86_64] Include: InstanceFamilies: - c5.* - c6g.* - f1 - m5.* - m6g.* - r5.* - r6g.* - x2gd - z1d If you have reserved instances (RIs) or savings plans then you can configure instances so that they are always on since you are paying for them whether they are running or not. To do this add a MinCount greater than 0 for the compute resources that contain the instance types. slurm: InstanceConfig: NodeCounts: DefaultMinCount: 1 The Legacy cluster also allows you to specify the names of specific nodes. slurm: InstanceConfig: AlwaysOnNodes: - nodename-[0-4] Configure Fair Share Scheduling (Optional) Slurm supports fair share scheduling , but it requires the fair share policy to be configured. By default, all users will be put into a default group that has a low fair share. The configuration file is at source/resources/playbooks/roles/SlurmCtl/templates/opt/slurm/cluster/etc/accounts.yml.example in the repository and is deployed to /opt/slurm/{{ClusterName}}/conf/accounts.yml . The file is a simple yaml file that allows you to configure groups, the users that belong to the group, and a fair share weight for the group. Refer to the Slurm documentation for details on how the fair share weight is calculated. The scheduler can be configured so that users who aren't getting their fair share of resources get higher priority. The following shows 3 top level groups. Note that the fairshare weights aren't a percentage. They are just a relative weight. In this example, the projects have 9 times higher weight than the jenkins group. jenkins: fairshare: 10 users: - jenkins project1: fairshare: 90 project2: fairshare: 90 The allocation of top level groups can be further subdivided to control the relative priority of jobs within that group. For example, a project may have design verification (dv), rtl design (rtl), physical design (dv), and formal verification (fv) teams. The following example shows how the project's allocation can be prioritized for the different teams. If a group is using more than it's fair share then its jobs will have lower priority than jobs whose users aren't getting their fair share. project1-dv: parent: project1 fairshare: 80 users: - dvuser1 project1-pd: parent: project1 fairshare: 10 users: - pduser1 project1-rtl: parent: project1 fairshare: 10 users: - rtluser1 project1-fv: parent: project1 fairshare: 10 users: - fvuser1 The scheduler uses the priority/multifactor plugin to calculate job priorities. Fair share is just one of the factors. Read the Multifactor Priority Plugin documentation for the details. This is the default configuration in slurm.conf. The partition weight is set the highest so that jobs in the interactive partition always have the highest priority. Fairshare and QOS are the next highest weighted factors. The next factor is the job age, which means all else being equal the jobs run in FIFO order with the jobs that have been waiting the longest getting higher priority. PriorityType=priority/multifactor PriorityWeightPartition=100000 PriorityWeightFairshare=10000 PriorityWeightQOS=10000 PriorityWeightAge=1000 PriorityWeightAssoc=0 PriorityWeightJobSize=0 These weights can be adjusted based on your needs to control job priorities. Configure Licenses Slurm supports configuring licenses as a consumable resource . It will keep track of how many running jobs are using a license and when no more licenses are available then jobs will stay pending in the queue until a job completes and frees up a license. Combined with the fairshare algorithm, this can prevent users from monopolizing licenses and preventing others from being able to run their jobs. Licenses are configured using the slurm/Licenses configuration variable. If you are using the Slurm database then these will be configured in the database. Otherwises they will be configured in /opt/slurm/{{ClusterName}}/etc/slurm_licenses.conf . The example configuration shows how the number of licenses can be configured. In this example, the cluster will manage 800 vcs licenses and 1 ansys license. Users must request a license using the -L or --licenses options. slurm: Licenses: vcs: Count: 800 ansys: Count: 1","title":"Deployment Prerequisites"},{"location":"deployment-prerequisites/#deployment-prerequisites","text":"The original (legacy) version used a custom Slurm plugin for orchestrating the EC2 compute nodes. The latest version uses ParallelCluster to provision the core Slurm infrastructure. This page shows common prerequisites that need to be done for deploying either version.","title":"Deployment Prerequisites"},{"location":"deployment-prerequisites/#configure-aws-cli-credentials","text":"You will needs AWS credentials that provide admin access to deploy the cluster.","title":"Configure AWS CLI Credentials"},{"location":"deployment-prerequisites/#clone-or-download-the-repository","text":"Clone or download the aws-eda-slurm-cluster repository to your system. git clone git@github.com:aws-samples/aws-eda-slurm-cluster.git","title":"Clone or Download the Repository"},{"location":"deployment-prerequisites/#create-sns-topic-for-error-notifications-optional-but-recommended","text":"The Slurm cluster allows you to specify an SNS notification that will be notified when an error is detected. You can provide the ARN for the topic in the config file or on the command line. You can use the SNS notification in various ways. The simplest is to subscribe your email address to the topic so that you get an email when there is an error. You could also use it to trigger a CloudWatch alarm that could be used to trigger a lambda to do automatic remediation or create a support ticket.","title":"Create SNS Topic for Error Notifications (Optional but recommended)"},{"location":"deployment-prerequisites/#make-sure-required-packages-are-installed","text":"cd aws-eda-slurm-cluster source setup.sh The setup script assumes that you have sudo access so that you can install or update packages. If you do not, then contact an administrator to help you do the updates. If necessary modify the setup script for your environment.","title":"Make sure required packages are installed"},{"location":"deployment-prerequisites/#install-cloud-development-kit-cdk-optional","text":"The setup script will attempt to install all of the prerequisites for you. If the install script fails on your system then you can refer to this section for instructions on how to install or update CDK. This cluster uses Cloud Development Kit (CDK) and Python 3 to deploy the cluster. Install the packages used by the installer. sudo yum -y install curl gcc-c++ make nfs-utils python3 tcl unzip wget The following link documents how to setup for CDK. Follow the instructions for Python. https://docs.aws.amazon.com/cdk/v2/guide/getting_started.html#getting_started_prerequisites Note that CDK requires a pretty new version of nodejs which you may have to download from, for example, https://nodejs.org/dist/v16.13.1/node-v16.13.1-linux-x64.tar.xz sudo yum -y install wget wget https://nodejs.org/dist/v16.13.1/node-v16.13.1-linux-x64.tar.xz tar -xf node-v16.13.1-linux-x64.tar.xz ~ Add the nodjs bin directory to your path. https://docs.aws.amazon.com/cdk/v2/guide/getting_started.html#getting_started_install Note that the version of aws-cdk changes frequently. The version that has been tested is in the CDK_VERSION variable in the install script. The install script will try to install the prerequisites if they aren't already installed.","title":"Install Cloud Development Kit (CDK) (Optional)"},{"location":"deployment-prerequisites/#create-configuration-file","text":"Before you deploy a cluster you need to create a configuration file. A default configuration file is found in source/resources/config/default_config.yml . You should create a new config file and update the parameters for your cluster. Ideally you should version control this file so you can keep track of changes. The schema for the config file along with its default values can be found in source/cdk/config_schema.py . The schema is defined in python, but the actual config file should be in yaml format. The following are key parameters that you will need to update. If you do not have the required parameters in your config file then the installer script will fail unless you specify the --prompt option. You should save your selections in the config file. Parameter Description Valid Values Default StackName ] The cloudformation stack that will deploy the cluster. None slurm/ClusterName Name of the Slurm cluster For ParallelCluster shouldn't be the same as StackName Region Region where VPC is located $AWS_DEFAULT_REGION VpcId The vpc where the cluster will be deployed. vpc-* None SshKeyPair EC2 Keypair to use for instances None slurm/SubmitterSecurityGroupIds Existing security groups that can submit to the cluster. For SOCA this is the ComputeNodeSG* resource. sg-* None ErrorSnsTopicArn ARN of an SNS topic that will be notified of errors arn:aws:sns:{{region}}:{AccountId}:{TopicName} None slurm/ParallelClusterConfig/enable Use ParallelCluster true, false false slurm/InstanceConfig Configure instance types that the cluster can use and number of nodes. See default_config.yml","title":"Create Configuration File"},{"location":"deployment-prerequisites/#configure-the-compute-instances","text":"The slurm/InstanceConfig configuration parameter configures the base operating systems, CPU architectures, instance families, and instance types that the Slurm cluster should support. ParallelCluster currently doesn't support heterogeneous clusters; all nodes must have the same architecture and Base OS.","title":"Configure the Compute Instances"},{"location":"deployment-prerequisites/#parallelcluster","text":"Base OS CPU Architectures Amazon Linux 2 x86_64, arm64 CentOS 7 x86_64 RedHat 7 x86_64 RedHat 8 x86_64, arm64","title":"ParallelCluster"},{"location":"deployment-prerequisites/#legacy","text":"Base OS CPU Architectures Alma Linux 8 x86_64, arm64 Amazon Linux 2 x86_64, arm64 CentOS 7 x86_64 RedHat 7 x86_64 RedHat 8 x86_64, arm64 Rocky Linux 8 x86_64, arm64 You can exclude instances types by family or specific instance type. By default the InstanceConfig excludes older generation instance families. You can include instances by family or specific instance type. If no includes are specified then all non-excluded instance types will be used. You can also choose to only include the largest instance size within a family. The advantage of using the max instance size is that jobs running on the instance have the highest network bandwidth for that family and fewer instances are required to run the same number of jobs. This may help jobs run faster and allow jobs to wait less time for a new instance to start. The disadvantage is higher cost if the instance is lightly loaded. The default InstanceConfig includes all supported base OSes and architectures and burstable and general purpose instance types. default instance families default instance types default excluded instance families default excluded instance types Note that instance types and families are python regular expressions. slurm: InstanceConfig: BaseOsArchitecture: CentOS: 7: [x86_64] Include: InstanceFamilies: - t3.* - m6a.* InstanceTypes: - r6a.large The following InstanceConfig configures instance types recommended for EDA workloads running on CentOS. slurm: InstanceConfig: BaseOsArchitecture: CentOS: 7: [x86_64] Include: InstanceFamilies: - c5.* - c6g.* - f1 - m5.* - m6g.* - r5.* - r6g.* - x2gd - z1d If you have reserved instances (RIs) or savings plans then you can configure instances so that they are always on since you are paying for them whether they are running or not. To do this add a MinCount greater than 0 for the compute resources that contain the instance types. slurm: InstanceConfig: NodeCounts: DefaultMinCount: 1 The Legacy cluster also allows you to specify the names of specific nodes. slurm: InstanceConfig: AlwaysOnNodes: - nodename-[0-4]","title":"Legacy"},{"location":"deployment-prerequisites/#configure-fair-share-scheduling-optional","text":"Slurm supports fair share scheduling , but it requires the fair share policy to be configured. By default, all users will be put into a default group that has a low fair share. The configuration file is at source/resources/playbooks/roles/SlurmCtl/templates/opt/slurm/cluster/etc/accounts.yml.example in the repository and is deployed to /opt/slurm/{{ClusterName}}/conf/accounts.yml . The file is a simple yaml file that allows you to configure groups, the users that belong to the group, and a fair share weight for the group. Refer to the Slurm documentation for details on how the fair share weight is calculated. The scheduler can be configured so that users who aren't getting their fair share of resources get higher priority. The following shows 3 top level groups. Note that the fairshare weights aren't a percentage. They are just a relative weight. In this example, the projects have 9 times higher weight than the jenkins group. jenkins: fairshare: 10 users: - jenkins project1: fairshare: 90 project2: fairshare: 90 The allocation of top level groups can be further subdivided to control the relative priority of jobs within that group. For example, a project may have design verification (dv), rtl design (rtl), physical design (dv), and formal verification (fv) teams. The following example shows how the project's allocation can be prioritized for the different teams. If a group is using more than it's fair share then its jobs will have lower priority than jobs whose users aren't getting their fair share. project1-dv: parent: project1 fairshare: 80 users: - dvuser1 project1-pd: parent: project1 fairshare: 10 users: - pduser1 project1-rtl: parent: project1 fairshare: 10 users: - rtluser1 project1-fv: parent: project1 fairshare: 10 users: - fvuser1 The scheduler uses the priority/multifactor plugin to calculate job priorities. Fair share is just one of the factors. Read the Multifactor Priority Plugin documentation for the details. This is the default configuration in slurm.conf. The partition weight is set the highest so that jobs in the interactive partition always have the highest priority. Fairshare and QOS are the next highest weighted factors. The next factor is the job age, which means all else being equal the jobs run in FIFO order with the jobs that have been waiting the longest getting higher priority. PriorityType=priority/multifactor PriorityWeightPartition=100000 PriorityWeightFairshare=10000 PriorityWeightQOS=10000 PriorityWeightAge=1000 PriorityWeightAssoc=0 PriorityWeightJobSize=0 These weights can be adjusted based on your needs to control job priorities.","title":"Configure Fair Share Scheduling (Optional)"},{"location":"deployment-prerequisites/#configure-licenses","text":"Slurm supports configuring licenses as a consumable resource . It will keep track of how many running jobs are using a license and when no more licenses are available then jobs will stay pending in the queue until a job completes and frees up a license. Combined with the fairshare algorithm, this can prevent users from monopolizing licenses and preventing others from being able to run their jobs. Licenses are configured using the slurm/Licenses configuration variable. If you are using the Slurm database then these will be configured in the database. Otherwises they will be configured in /opt/slurm/{{ClusterName}}/etc/slurm_licenses.conf . The example configuration shows how the number of licenses can be configured. In this example, the cluster will manage 800 vcs licenses and 1 ansys license. Users must request a license using the -L or --licenses options. slurm: Licenses: vcs: Count: 800 ansys: Count: 1","title":"Configure Licenses"},{"location":"f1-ami/","text":"SLURM AMI Based On FPGA Developer AMI (legacy) This tutorial shows how to create an AMI based on the AWS FPGA Developer AMI. The FPGA Developer AMI has the Xilinx Vivado tools that can be used free of additional charges when run on AWS EC2 instances to develop FPGA images that can be run on AWS F1 instances. Subscribe To the AMI First subscribe to the FPGA developer AMI in the AWS Marketplace. There are 2 versions, one for CentOS 7 and the other for Amazon Linux 2 . Add the AMI to Your Config File slurm: SlurmNodeAmis: # Customized AMIs with file system mounts, packages, etc. configured. # If these aren't defined then the generic base AMIs are used. # Example in the comment below is the AWS FPGA Developer AMI BaseAmis: us-east-1: Amazon: {2: {x86_64: {ImageId: ami-0efdec76678df9a64, RootDeviceSize: '+5'}}} CentOS: {7: {x86_64: {ImageId: ami-02155c6289e76719a, RootDeviceSize: '+5'}}} Deploy the Cluster With the config updated the AMIs for the compute nodes will be built using the specified base AMIs.","title":"SLURM AMI Based On FPGA Developer AMI (legacy)"},{"location":"f1-ami/#slurm-ami-based-on-fpga-developer-ami-legacy","text":"This tutorial shows how to create an AMI based on the AWS FPGA Developer AMI. The FPGA Developer AMI has the Xilinx Vivado tools that can be used free of additional charges when run on AWS EC2 instances to develop FPGA images that can be run on AWS F1 instances.","title":"SLURM AMI Based On FPGA Developer AMI (legacy)"},{"location":"f1-ami/#subscribe-to-the-ami","text":"First subscribe to the FPGA developer AMI in the AWS Marketplace. There are 2 versions, one for CentOS 7 and the other for Amazon Linux 2 .","title":"Subscribe To the AMI"},{"location":"f1-ami/#add-the-ami-to-your-config-file","text":"slurm: SlurmNodeAmis: # Customized AMIs with file system mounts, packages, etc. configured. # If these aren't defined then the generic base AMIs are used. # Example in the comment below is the AWS FPGA Developer AMI BaseAmis: us-east-1: Amazon: {2: {x86_64: {ImageId: ami-0efdec76678df9a64, RootDeviceSize: '+5'}}} CentOS: {7: {x86_64: {ImageId: ami-02155c6289e76719a, RootDeviceSize: '+5'}}}","title":"Add the AMI to Your Config File"},{"location":"f1-ami/#deploy-the-cluster","text":"With the config updated the AMIs for the compute nodes will be built using the specified base AMIs.","title":"Deploy the Cluster"},{"location":"federation/","text":"Federation (legacy) To maximize performance, EDA workloads should run in a single AZ. If you need to run jobs in more than one AZ then you can use the federation feature of Slurm so that you can run jobs on multiple clusters. The config directory has example configuration files that demonstrate how deploy federated cluster into 3 AZs. source/config/slurm_eda_az1.yml source/config/slurm_eda_az2.yml source/config/slurm_eda_az3.yml These clusters should be deployed sequentially. The first cluster creates a cluster and a slurmdbd instance. The other 2 clusters are deployed into their own AZ by configuring the SubnetId of the cluster. They reuse the same slurmdbd instance so that they can reuse a common pool of licenses that is managed by the slurmdbd instance. The config files for the 2nd and 3rd clusters provide the stack names from the others so that the security groups can be updated to allow the required network traffic between the clusters. The following shows an example of the configuration. slurm_eda_az1: Federation: Name: slurmeda FederatedClusterStackNames: [] slurm_eda_az2: Federation: Name: slurmeda FederatedClusterStackNames: - slurmedaaz1 slurm_eda_az3: Federation: Name: slurmeda FederatedClusterStackNames: - slurmedaaz1 - slurmedaaz2","title":"Federation (legacy)"},{"location":"federation/#federation-legacy","text":"To maximize performance, EDA workloads should run in a single AZ. If you need to run jobs in more than one AZ then you can use the federation feature of Slurm so that you can run jobs on multiple clusters. The config directory has example configuration files that demonstrate how deploy federated cluster into 3 AZs. source/config/slurm_eda_az1.yml source/config/slurm_eda_az2.yml source/config/slurm_eda_az3.yml These clusters should be deployed sequentially. The first cluster creates a cluster and a slurmdbd instance. The other 2 clusters are deployed into their own AZ by configuring the SubnetId of the cluster. They reuse the same slurmdbd instance so that they can reuse a common pool of licenses that is managed by the slurmdbd instance. The config files for the 2nd and 3rd clusters provide the stack names from the others so that the security groups can be updated to allow the required network traffic between the clusters. The following shows an example of the configuration. slurm_eda_az1: Federation: Name: slurmeda FederatedClusterStackNames: [] slurm_eda_az2: Federation: Name: slurmeda FederatedClusterStackNames: - slurmedaaz1 slurm_eda_az3: Federation: Name: slurmeda FederatedClusterStackNames: - slurmedaaz1 - slurmedaaz2","title":"Federation (legacy)"},{"location":"implementation/","text":"Implementation Details (legacy) Slurm Infrastructure All hosts in the cluster must share a uniform user and group namespace. The munged service must be running before starting any slurm daemons. Directory Structure All of the configuration files, scripts, and logs can be found under the following directory. /opt/slurm/{{ClusterName}} CloudWatch Metrics CloudWatch metrics are published by the following sources, but the code is all in SlurmPlugin.py . Slurm power saving scripts /opt/slurm/{{ClusterName}}/bin/slurm_ec2_resume.py /opt/slurm/{{ClusterName}}/bin/slurm_ec2_resume_fail.py /opt/slurm/{{ClusterName}}/bin/slurm_ec2_stop.py /opt/slurm/{{ClusterName}}/bin/slurm_ec2_terminate.py Spot monitor running on compute nodes /opt/slurm/{{ClusterName}}/bin/spot_monitor.py Cron jobs running on the Slurm controller /opt/slurm/{{ClusterName}}/bin/slurm_ec2_publish_cw.py /opt/slurm/{{ClusterName}}/bin/terminate_old_instances.py Down Node Handling If a node has a problem running jobs then Slurm can mark it DOWN. This includes if the resume script cannot start an instance for any reason include insufficient EC2 capacity. This can create 2 issues. First, if the compute node is running then it is wasting EC2 costs. Second, the node will be unavailable for scheduling which reduces the configured capacity of the cluster. The cluster is configured to periodically check for DOWN nodes so that they aren't left running and wasting compute costs. This is done by /opt/slurm/{{ClusterName}}/bin/slurm_down_nodes_clean.sh . The script is called every day by a systemd service: /etc/systemd/system/slurm_down_nodes_clean.service This service is run at boot and once a day as defined in /etc/systemd/system/slurm_down_nodes_clean.timer Insufficient Capacity Exception (ICE) Handling When Slurm schedules a powered down node it calls the ResumeScript defined in slurm.conf . This is in /opt/slurm/{{ClusterName}}/bin/slurm_ec2_resume.py . The script will attempt to start an EC2 instance and if it receives and InsufficientCapacityException (ICE) then the node will be marked down and Slurm will requeue the job. However, this is inadequate because if there are a large number of instances of that instance type configured then Slurm will schedule them and try to start them with the same result. Eventually all of the powered down nodes will be marked DOWN and depending on the job requirements the job will be allocated to a node with a different instance type or it will fail. This can take a substantial amount of time so SlurmPlugin.py does the following when it receives an ICE. Mark the node as DRAIN so no new jobs are scheduled on it. Find all other powered down nodes of the same type and mark them DOWN so that they won't be scheduled after this node is marked DOWN. Nodes that are running will be left alone. Requeue jobs on the node that failed to resume because of ICE. Mark the node DOWN. Power down the node. This is so that Slurm knows that the node is powered down so that when it is marked IDLE it will be powered up when a job is scheduled on it. The slurm_down_nodes_clean.service periodically finds all DOWN Slurm nodes, powers them down, and then marks them IDLE so that they can have jobs scheduled on them. This will allow Slurm to attempt to use more nodes of the instance type in the hopes that there is more capacity. If not, then the cycle repeats.","title":"Implementation Details (legacy)"},{"location":"implementation/#implementation-details-legacy","text":"","title":"Implementation Details (legacy)"},{"location":"implementation/#slurm-infrastructure","text":"All hosts in the cluster must share a uniform user and group namespace. The munged service must be running before starting any slurm daemons.","title":"Slurm Infrastructure"},{"location":"implementation/#directory-structure","text":"All of the configuration files, scripts, and logs can be found under the following directory. /opt/slurm/{{ClusterName}}","title":"Directory Structure"},{"location":"implementation/#cloudwatch-metrics","text":"CloudWatch metrics are published by the following sources, but the code is all in SlurmPlugin.py . Slurm power saving scripts /opt/slurm/{{ClusterName}}/bin/slurm_ec2_resume.py /opt/slurm/{{ClusterName}}/bin/slurm_ec2_resume_fail.py /opt/slurm/{{ClusterName}}/bin/slurm_ec2_stop.py /opt/slurm/{{ClusterName}}/bin/slurm_ec2_terminate.py Spot monitor running on compute nodes /opt/slurm/{{ClusterName}}/bin/spot_monitor.py Cron jobs running on the Slurm controller /opt/slurm/{{ClusterName}}/bin/slurm_ec2_publish_cw.py /opt/slurm/{{ClusterName}}/bin/terminate_old_instances.py","title":"CloudWatch Metrics"},{"location":"implementation/#down-node-handling","text":"If a node has a problem running jobs then Slurm can mark it DOWN. This includes if the resume script cannot start an instance for any reason include insufficient EC2 capacity. This can create 2 issues. First, if the compute node is running then it is wasting EC2 costs. Second, the node will be unavailable for scheduling which reduces the configured capacity of the cluster. The cluster is configured to periodically check for DOWN nodes so that they aren't left running and wasting compute costs. This is done by /opt/slurm/{{ClusterName}}/bin/slurm_down_nodes_clean.sh . The script is called every day by a systemd service: /etc/systemd/system/slurm_down_nodes_clean.service This service is run at boot and once a day as defined in /etc/systemd/system/slurm_down_nodes_clean.timer","title":"Down Node Handling"},{"location":"implementation/#insufficient-capacity-exception-ice-handling","text":"When Slurm schedules a powered down node it calls the ResumeScript defined in slurm.conf . This is in /opt/slurm/{{ClusterName}}/bin/slurm_ec2_resume.py . The script will attempt to start an EC2 instance and if it receives and InsufficientCapacityException (ICE) then the node will be marked down and Slurm will requeue the job. However, this is inadequate because if there are a large number of instances of that instance type configured then Slurm will schedule them and try to start them with the same result. Eventually all of the powered down nodes will be marked DOWN and depending on the job requirements the job will be allocated to a node with a different instance type or it will fail. This can take a substantial amount of time so SlurmPlugin.py does the following when it receives an ICE. Mark the node as DRAIN so no new jobs are scheduled on it. Find all other powered down nodes of the same type and mark them DOWN so that they won't be scheduled after this node is marked DOWN. Nodes that are running will be left alone. Requeue jobs on the node that failed to resume because of ICE. Mark the node DOWN. Power down the node. This is so that Slurm knows that the node is powered down so that when it is marked IDLE it will be powered up when a job is scheduled on it. The slurm_down_nodes_clean.service periodically finds all DOWN Slurm nodes, powers them down, and then marks them IDLE so that they can have jobs scheduled on them. This will allow Slurm to attempt to use more nodes of the instance type in the hopes that there is more capacity. If not, then the cycle repeats.","title":"Insufficient Capacity Exception (ICE) Handling"},{"location":"job_preemption/","text":"Job Preemption The cluster is set up with an interactive partition that has a higher priority than all other partitions. All other partitions are configured to allow jobs to be preempted by the interactive queue. When an interactive job is pending because of compute resources then it can preempt another job and use the resources. The preempted job will be requeued so that it will rerun when resources become available. Jobs should rarely pend because of lack of compute resources if you've defined enough compute nodes in your configuration. The more likely reason for a job to pend is if it requires a license and all available licenses are already being used. However, it appears that Slurm doesn't support preemption based on licenses availability so if the reason a job is pending is because of licenses then it will not preempt jobs in a lower priority queue even if doing so would free up a license. Documentation https://slurm.schedmd.com/preempt.html","title":"Job Preemption"},{"location":"job_preemption/#job-preemption","text":"The cluster is set up with an interactive partition that has a higher priority than all other partitions. All other partitions are configured to allow jobs to be preempted by the interactive queue. When an interactive job is pending because of compute resources then it can preempt another job and use the resources. The preempted job will be requeued so that it will rerun when resources become available. Jobs should rarely pend because of lack of compute resources if you've defined enough compute nodes in your configuration. The more likely reason for a job to pend is if it requires a license and all available licenses are already being used. However, it appears that Slurm doesn't support preemption based on licenses availability so if the reason a job is pending is because of licenses then it will not preempt jobs in a lower priority queue even if doing so would free up a license.","title":"Job Preemption"},{"location":"job_preemption/#documentation","text":"https://slurm.schedmd.com/preempt.html","title":"Documentation"},{"location":"multi-region/","text":"Multi-AZ and Multi-Region Support (legacy) By default, the EDA Slurm Cluster deploys all resources in a single availability zone (AZ). This is done for performance and cost reasons to minimize network latency and network cross AZ charges. Very large clusters may hit capacity constraints in a single AZ and can benefit from being able to launch compute nodes in multiple AZs and even multiple regions to get the required capacity. For this reason, the cluster can support compute nodes in multiple AZ and regions. All compute nodes are managed by one Slurm controller and the compute nodes encode the region and AZ in their node names. When a job gets scheduled on a compute node, the Slurm plugin runs an instance in the region and AZ encoded in the node name. Compute nodes in each AZ are grouped in partitions that can be given priorities. This allows a job submission to specify multiple partitions and the scheduler will choose available compute nodes from the highest priority partition. NOTE : This is an advanced topic with significant configuration complexity so it is recommended that you get guidance from and AWS specialist to help you set up your configuration. This page documents a simple setup which is unlikely to meet your file system performance goals without modification. Requirements There are three primary requirements for multi-AZ/region support: 1) networking, 2) file systems and 3) DNS. Networking The main networking requirement is that each region must have a VPC with a subnet in each AZ. The CIDR ranges of the VPC must be non-overlapping. The VPCs must connected using VPC Peering Connections or Transit Gateways and the routes and ACLs must be configured to allow communication between all of the VPCs. The compute nodes use all the ephemeral ports so those ports must be routed between the VPCs. File Systems The compute nodes must have the same logical view of the file systems. All paths used by the compute nodes must be available in each AZ and region. One way to accomplish this is to simply mount the exact same file systems on all compute nodes. This has the advantage of simplicity, however, it will incur performance penalties because of increased network latency and network charges because of cross-AZ and cross-Region network charges. The slurm file system is not performance critical and can be cross mounted. Performance critical file systems can be replicated across AZs and regions and and automatically synchronized using FSx for NetApp Ontap (FSxN) and FlexCache or SnapMirror. FlexCache is particularly efficient because it is a sparse cache that only synchronizes data when it it accessed. This means that not all of the data has to be replicated anywhere. If you replicate file systems then it means that your machine images will need to be configured to mount the closest file system. This could be done at boot time by using SSM parameters or by using location specific automount maps. Currently Route53 doesn't support a policy that allows you to choose an AZ dependent domain resolution. This is an advanced topic and we recommend that you consult with an AWS storage specialist to help you architect a storage solution that will meet your performance needs. DNS The cluster creates a Route53 private hosted zone or can use an existing one to get the IP addresses for the Slurm controllers and slurmdbd instances. It uses the AWS provided DNS in the VPC to get the IP addresses of AWS managed file systems. All of the VPCs need access to all of the DNS entries used by the Slurm instances. Configuration This example is going to demonstrate how to configure a cluster that spans 3 AZs in 3 regions for a total of 9 AZs. It is going to use a very simple file system topology with all of the file systems located in the primary AZ. Create VPCs In this example I deployed 3 Scale Out Computing on AWS (SOCA) clusters in eu-west-1, us-east-1, and us-west-2 with non-overlapping CIDRs. This created 3 VPCs each with 3 private subnets. Region SOCA Cluster CIDR eu-west-1 oktank-dub 10.1.0.0/16 us-east-1 oktank-iad 10.2.0.0/16 us-west-2 oktank-pdx 10.3.0.0/16 I am going to create a multi-region Slurm cluster in eu-west-1 that can run compute nodes in all 3 regions with the priority of the regions being eu-west-1 (dub) us-east-1 (iad) us-west-2 (pdx) If you have a globally distributed team you could modify the instructions to use regions close to your global team and deploy a cluster in the local region of each team that they can use that can run jobs in all of the regions. Connect the VPCs using VPC Peering Connections Go to the VPC console in eu-west-1 and select Peering connections on the left. Click on Create peering connection on the upper right Name the connection dub-to-iad For the local VPC select the SOCA VPC For the other VPC's region select Another Region Select us-east-1 Open the us-east-1 VPC console in another tab and copy the SOCA vpc id Go back to the eu-west-1 VPC console and paste the vpc id into the **VPC ID (Accepter) field Click Create peering connection to create the connection. Go back to the us-east-1 console and select Peering connections on the left. Select the connection you just created. It should be in Pending acceptance state. Select Actions and Accept request Repeat the same steps to create VPC peering connections between eu-west-1 (dub) and us-west-2 (pdx) and between us-east-1 (iad) and us-west-2 (pdx). When you are complete all of the VPCs will have a connection to the others. The next step is to set up routing table entries to route the traffic between the VPCs over the peering connections. Do the following steps in each region. Open the VPC console Select Route tables on the left Select the route table for each of the 3 private subnets Click Edit routes Click Add route Enter the CIDR range for another VPC in the destination and the peering connection to that VPC (start typing pcx- and then select from the list) Click Save changes When this is done packets from each VPC to any other will be routed across the appropriate VPC peering connection. DNS: Route53 Private Hosted Zone The oktank-dub SOCA cluster has two EFS file systems mounted at /apps and /data that contain the home and tools directories for the user's desktops. We are enabling the SOCA users to submit jobs to Slurm so those volumes will need to be available on all compute nodes. However, the EFS DNS name will only able to be resolved by the AWS provided DNS server in the oktank-dub VPC. We could just use the IP address, but it is more maintainable to create a Route53 private hosted zone that is shared by all of the clusters so that we can refer the the EFS file systems with a friendly DNS name. Note that Route53 is a global, not regional, service. Open the Route53 console Select Hosted zones on the left Click Create hosted zone in the upper right Enter a domain name like slurmdub.local For Type select Private hosted zone Associate the oktank-dub VPC with the hosted zone. For Region select eu-west-1 Click the VPC ID and select the SOCA VPC Associate the oktank-iad VPC with the hosted zone. Click Add VPC For Region select eu-west-1 Click the VPC ID and select the SOCA VPC Associate the oktank-pdx VPC with the hosted zone. Click Add VPC For Region select eu-west-1 Click the VPC ID and select the SOCA VPC Click Create hosted zone Expand Hosted zone details and save the Hosted zone ID which will be used in the config file. Create DNS entries for the SOCA EFS file systems. Open the EFS console in eu-west-1 Select the Apps file system Select the Network tab and note the IP addresses for all 3 availability zones. Repeat to get the IP addresses for the Data file system. Open the Route53 console and select the slurmdub.local hosted zone. Click Create record Name it fs-apps For record type select A For Value put the 3 IP addresses for the EFS file system on separate lines For Routing policy select latency For Region select eu-west-1 For Record ID enter fs-apps-dub Create idential A records for fs-apps in the us-east-1 and us-west-2 regions with Record ID of fs-apps-iad and fs-apps-pdx. Repeat for the Data file system and create fs-data A records in all 3 regions. File System Access Make sure that file system security groups allow access from all slurm VPCs. You may need to allow inbound access from 10.2.0.0/16 and 10.3.0.0/16. Slurm Configuration The following configuration file configures all three regions. Note that there are values for key pairs, VPC IDs, subnet IDs, etc. that you will have to update with the information from your SOCA clusters. Regional resources that must be provided: * VPC IDs * VPC CIDRs * Subnet IDs * EC2 Keypairs * Security Group IDs Regional resources that will be created for you: * ComputeNodeSecurityGroup Global resources that will be created for you: * IAM instance roles slurm_dub.yml: --- # Multi-region Slurm cluster with Netapp Ontap # # Origin of the cluster is in eu-west-1 and extends into us-east-1 and us-west-2 StackName: slurmdub Region: eu-west-1 SshKeyPair: admin-eu-west-1 # Or whatever Key Pair you've created VpcId: vpc-xxxxxxxxxxxxxxxxx # oktank-dub SubnetId: subnet-xxxxxxxxxxxxxxxxx # oktank-dub, PrivateSubnet1 HostedZoneId: XXXXXXXXXXXXXXXXXXX # The hosted zone ID for the hosted zone you created above. ErrorSnsTopicArn: arn:aws:sns:eu-west-1:${AccountId}:SlurmError # ARN of your SNS topic. TimeZone: 'US/Central' # Or whatever you prefer slurm: MungeKeySsmParameter: \"/slurm/munge_key\" SlurmCtl: NumberOfControllers: 2 SlurmDbd: {} # External security groups that should be able to use the cluster SubmitterSecurityGroupIds: soca-oktank-dub-ComputeNodeSG: sg-xxxxxxxxxxxxxxxxx SubmitterInstanceTags: 'soca:ClusterId': ['soca-oktank-dub'] # InstanceConfig: # Configure the instances used by the cluster # A partition will be created for each combination of Base OS, Architecture, and Spot InstanceConfig: UseSpot: true NodesPerInstanceType: 10 BaseOsArchitecture: AlmaLinux: {8: [x86_64, arm64]} CentOS: 7: [x86_64] Include: MaxSizeOnly: false InstanceFamilies: - t3 - t4g InstanceTypes: [] Exclude: InstanceFamilies: [] InstanceTypes: - '.+\\.(micro|nano)' # Not enough memory - '.*\\.metal' Regions: eu-west-1: VpcId: vpc-xxxxxxxxxxxxxxxxx # oktank-dub CIDR: 10.1.0.0/16 SshKeyPair: admin-eu-west-1 AZs: - Priority: 10 Subnet: subnet-xxxxxxxxxxxxxxxxx # oktank-dub - PrivateSubnet1 - Priority: 9 Subnet: subnet-xxxxxxxxxxxxxxxxx # oktank-dub - PrivateSubnet2 - Priority: 8 Subnet: subnet-xxxxxxxxxxxxxxxxx # oktank-dub - PrivateSubnet3 us-east-1: VpcId: vpc-xxxxxxxxxxxxxxxxx CIDR: 10.2.0.0/16 SshKeyPair: admin-us-east-1 AZs: - Priority: 7 Subnet: subnet-xxxxxxxxxxxxxxxxx # oktank-iad - PrivateSubnet1 - Priority: 6 Subnet: subnet-xxxxxxxxxxxxxxxxx # oktank-iad - PrivateSubnet2 - Priority: 5 Subnet: subnet-xxxxxxxxxxxxxxxxx # oktank-iad - PrivateSubnet3 us-west-2: VpcId: vpc-xxxxxxxxxxxxxxxxx CIDR: 10.3.0.0/16 SshKeyPair: admin-us-west-2 AZs: - Priority: 4 Subnet: subnet-xxxxxxxxxxxxxxxxx # oktank-pdx - PrivateSubnet1 - Priority: 3 Subnet: subnet-xxxxxxxxxxxxxxxxx # oktank-pdx - PrivateSubnet2 - Priority: 2 Subnet: subnet-xxxxxxxxxxxxxxxxx # oktank-pdx - PrivateSubnet3 storage: provider: ontap removal_policy: DESTROY ontap: {} ExtraMounts: - dest: /apps src: fs-apps.slurmdub.local:/ type: nfs4 options: nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport - dest: /data src: fs-data.slurmdub.local:/ type: nfs4 options: nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport Deployment After the configuration is complete then deployment is the same as document on the Deploy the Legacy Cluster page.","title":"Multi-AZ and Multi-Region Support (legacy)"},{"location":"multi-region/#multi-az-and-multi-region-support-legacy","text":"By default, the EDA Slurm Cluster deploys all resources in a single availability zone (AZ). This is done for performance and cost reasons to minimize network latency and network cross AZ charges. Very large clusters may hit capacity constraints in a single AZ and can benefit from being able to launch compute nodes in multiple AZs and even multiple regions to get the required capacity. For this reason, the cluster can support compute nodes in multiple AZ and regions. All compute nodes are managed by one Slurm controller and the compute nodes encode the region and AZ in their node names. When a job gets scheduled on a compute node, the Slurm plugin runs an instance in the region and AZ encoded in the node name. Compute nodes in each AZ are grouped in partitions that can be given priorities. This allows a job submission to specify multiple partitions and the scheduler will choose available compute nodes from the highest priority partition. NOTE : This is an advanced topic with significant configuration complexity so it is recommended that you get guidance from and AWS specialist to help you set up your configuration. This page documents a simple setup which is unlikely to meet your file system performance goals without modification.","title":"Multi-AZ and Multi-Region Support (legacy)"},{"location":"multi-region/#requirements","text":"There are three primary requirements for multi-AZ/region support: 1) networking, 2) file systems and 3) DNS.","title":"Requirements"},{"location":"multi-region/#networking","text":"The main networking requirement is that each region must have a VPC with a subnet in each AZ. The CIDR ranges of the VPC must be non-overlapping. The VPCs must connected using VPC Peering Connections or Transit Gateways and the routes and ACLs must be configured to allow communication between all of the VPCs. The compute nodes use all the ephemeral ports so those ports must be routed between the VPCs.","title":"Networking"},{"location":"multi-region/#file-systems","text":"The compute nodes must have the same logical view of the file systems. All paths used by the compute nodes must be available in each AZ and region. One way to accomplish this is to simply mount the exact same file systems on all compute nodes. This has the advantage of simplicity, however, it will incur performance penalties because of increased network latency and network charges because of cross-AZ and cross-Region network charges. The slurm file system is not performance critical and can be cross mounted. Performance critical file systems can be replicated across AZs and regions and and automatically synchronized using FSx for NetApp Ontap (FSxN) and FlexCache or SnapMirror. FlexCache is particularly efficient because it is a sparse cache that only synchronizes data when it it accessed. This means that not all of the data has to be replicated anywhere. If you replicate file systems then it means that your machine images will need to be configured to mount the closest file system. This could be done at boot time by using SSM parameters or by using location specific automount maps. Currently Route53 doesn't support a policy that allows you to choose an AZ dependent domain resolution. This is an advanced topic and we recommend that you consult with an AWS storage specialist to help you architect a storage solution that will meet your performance needs.","title":"File Systems"},{"location":"multi-region/#dns","text":"The cluster creates a Route53 private hosted zone or can use an existing one to get the IP addresses for the Slurm controllers and slurmdbd instances. It uses the AWS provided DNS in the VPC to get the IP addresses of AWS managed file systems. All of the VPCs need access to all of the DNS entries used by the Slurm instances.","title":"DNS"},{"location":"multi-region/#configuration","text":"This example is going to demonstrate how to configure a cluster that spans 3 AZs in 3 regions for a total of 9 AZs. It is going to use a very simple file system topology with all of the file systems located in the primary AZ.","title":"Configuration"},{"location":"multi-region/#create-vpcs","text":"In this example I deployed 3 Scale Out Computing on AWS (SOCA) clusters in eu-west-1, us-east-1, and us-west-2 with non-overlapping CIDRs. This created 3 VPCs each with 3 private subnets. Region SOCA Cluster CIDR eu-west-1 oktank-dub 10.1.0.0/16 us-east-1 oktank-iad 10.2.0.0/16 us-west-2 oktank-pdx 10.3.0.0/16 I am going to create a multi-region Slurm cluster in eu-west-1 that can run compute nodes in all 3 regions with the priority of the regions being eu-west-1 (dub) us-east-1 (iad) us-west-2 (pdx) If you have a globally distributed team you could modify the instructions to use regions close to your global team and deploy a cluster in the local region of each team that they can use that can run jobs in all of the regions.","title":"Create VPCs"},{"location":"multi-region/#connect-the-vpcs-using-vpc-peering-connections","text":"Go to the VPC console in eu-west-1 and select Peering connections on the left. Click on Create peering connection on the upper right Name the connection dub-to-iad For the local VPC select the SOCA VPC For the other VPC's region select Another Region Select us-east-1 Open the us-east-1 VPC console in another tab and copy the SOCA vpc id Go back to the eu-west-1 VPC console and paste the vpc id into the **VPC ID (Accepter) field Click Create peering connection to create the connection. Go back to the us-east-1 console and select Peering connections on the left. Select the connection you just created. It should be in Pending acceptance state. Select Actions and Accept request Repeat the same steps to create VPC peering connections between eu-west-1 (dub) and us-west-2 (pdx) and between us-east-1 (iad) and us-west-2 (pdx). When you are complete all of the VPCs will have a connection to the others. The next step is to set up routing table entries to route the traffic between the VPCs over the peering connections. Do the following steps in each region. Open the VPC console Select Route tables on the left Select the route table for each of the 3 private subnets Click Edit routes Click Add route Enter the CIDR range for another VPC in the destination and the peering connection to that VPC (start typing pcx- and then select from the list) Click Save changes When this is done packets from each VPC to any other will be routed across the appropriate VPC peering connection.","title":"Connect the VPCs using VPC Peering Connections"},{"location":"multi-region/#dns-route53-private-hosted-zone","text":"The oktank-dub SOCA cluster has two EFS file systems mounted at /apps and /data that contain the home and tools directories for the user's desktops. We are enabling the SOCA users to submit jobs to Slurm so those volumes will need to be available on all compute nodes. However, the EFS DNS name will only able to be resolved by the AWS provided DNS server in the oktank-dub VPC. We could just use the IP address, but it is more maintainable to create a Route53 private hosted zone that is shared by all of the clusters so that we can refer the the EFS file systems with a friendly DNS name. Note that Route53 is a global, not regional, service. Open the Route53 console Select Hosted zones on the left Click Create hosted zone in the upper right Enter a domain name like slurmdub.local For Type select Private hosted zone Associate the oktank-dub VPC with the hosted zone. For Region select eu-west-1 Click the VPC ID and select the SOCA VPC Associate the oktank-iad VPC with the hosted zone. Click Add VPC For Region select eu-west-1 Click the VPC ID and select the SOCA VPC Associate the oktank-pdx VPC with the hosted zone. Click Add VPC For Region select eu-west-1 Click the VPC ID and select the SOCA VPC Click Create hosted zone Expand Hosted zone details and save the Hosted zone ID which will be used in the config file. Create DNS entries for the SOCA EFS file systems. Open the EFS console in eu-west-1 Select the Apps file system Select the Network tab and note the IP addresses for all 3 availability zones. Repeat to get the IP addresses for the Data file system. Open the Route53 console and select the slurmdub.local hosted zone. Click Create record Name it fs-apps For record type select A For Value put the 3 IP addresses for the EFS file system on separate lines For Routing policy select latency For Region select eu-west-1 For Record ID enter fs-apps-dub Create idential A records for fs-apps in the us-east-1 and us-west-2 regions with Record ID of fs-apps-iad and fs-apps-pdx. Repeat for the Data file system and create fs-data A records in all 3 regions.","title":"DNS: Route53 Private Hosted Zone"},{"location":"multi-region/#file-system-access","text":"Make sure that file system security groups allow access from all slurm VPCs. You may need to allow inbound access from 10.2.0.0/16 and 10.3.0.0/16.","title":"File System Access"},{"location":"multi-region/#slurm-configuration","text":"The following configuration file configures all three regions. Note that there are values for key pairs, VPC IDs, subnet IDs, etc. that you will have to update with the information from your SOCA clusters. Regional resources that must be provided: * VPC IDs * VPC CIDRs * Subnet IDs * EC2 Keypairs * Security Group IDs Regional resources that will be created for you: * ComputeNodeSecurityGroup Global resources that will be created for you: * IAM instance roles slurm_dub.yml: --- # Multi-region Slurm cluster with Netapp Ontap # # Origin of the cluster is in eu-west-1 and extends into us-east-1 and us-west-2 StackName: slurmdub Region: eu-west-1 SshKeyPair: admin-eu-west-1 # Or whatever Key Pair you've created VpcId: vpc-xxxxxxxxxxxxxxxxx # oktank-dub SubnetId: subnet-xxxxxxxxxxxxxxxxx # oktank-dub, PrivateSubnet1 HostedZoneId: XXXXXXXXXXXXXXXXXXX # The hosted zone ID for the hosted zone you created above. ErrorSnsTopicArn: arn:aws:sns:eu-west-1:${AccountId}:SlurmError # ARN of your SNS topic. TimeZone: 'US/Central' # Or whatever you prefer slurm: MungeKeySsmParameter: \"/slurm/munge_key\" SlurmCtl: NumberOfControllers: 2 SlurmDbd: {} # External security groups that should be able to use the cluster SubmitterSecurityGroupIds: soca-oktank-dub-ComputeNodeSG: sg-xxxxxxxxxxxxxxxxx SubmitterInstanceTags: 'soca:ClusterId': ['soca-oktank-dub'] # InstanceConfig: # Configure the instances used by the cluster # A partition will be created for each combination of Base OS, Architecture, and Spot InstanceConfig: UseSpot: true NodesPerInstanceType: 10 BaseOsArchitecture: AlmaLinux: {8: [x86_64, arm64]} CentOS: 7: [x86_64] Include: MaxSizeOnly: false InstanceFamilies: - t3 - t4g InstanceTypes: [] Exclude: InstanceFamilies: [] InstanceTypes: - '.+\\.(micro|nano)' # Not enough memory - '.*\\.metal' Regions: eu-west-1: VpcId: vpc-xxxxxxxxxxxxxxxxx # oktank-dub CIDR: 10.1.0.0/16 SshKeyPair: admin-eu-west-1 AZs: - Priority: 10 Subnet: subnet-xxxxxxxxxxxxxxxxx # oktank-dub - PrivateSubnet1 - Priority: 9 Subnet: subnet-xxxxxxxxxxxxxxxxx # oktank-dub - PrivateSubnet2 - Priority: 8 Subnet: subnet-xxxxxxxxxxxxxxxxx # oktank-dub - PrivateSubnet3 us-east-1: VpcId: vpc-xxxxxxxxxxxxxxxxx CIDR: 10.2.0.0/16 SshKeyPair: admin-us-east-1 AZs: - Priority: 7 Subnet: subnet-xxxxxxxxxxxxxxxxx # oktank-iad - PrivateSubnet1 - Priority: 6 Subnet: subnet-xxxxxxxxxxxxxxxxx # oktank-iad - PrivateSubnet2 - Priority: 5 Subnet: subnet-xxxxxxxxxxxxxxxxx # oktank-iad - PrivateSubnet3 us-west-2: VpcId: vpc-xxxxxxxxxxxxxxxxx CIDR: 10.3.0.0/16 SshKeyPair: admin-us-west-2 AZs: - Priority: 4 Subnet: subnet-xxxxxxxxxxxxxxxxx # oktank-pdx - PrivateSubnet1 - Priority: 3 Subnet: subnet-xxxxxxxxxxxxxxxxx # oktank-pdx - PrivateSubnet2 - Priority: 2 Subnet: subnet-xxxxxxxxxxxxxxxxx # oktank-pdx - PrivateSubnet3 storage: provider: ontap removal_policy: DESTROY ontap: {} ExtraMounts: - dest: /apps src: fs-apps.slurmdub.local:/ type: nfs4 options: nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport - dest: /data src: fs-data.slurmdub.local:/ type: nfs4 options: nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport","title":"Slurm Configuration"},{"location":"multi-region/#deployment","text":"After the configuration is complete then deployment is the same as document on the Deploy the Legacy Cluster page.","title":"Deployment"},{"location":"onprem/","text":"On-Premises Integration (legacy) The slurm cluster can also be configured to manage on-premises compute nodes. The user must configure the on-premises compute nodes and then give the configuration information. Network Requirements The on-prem network must have a CIDR range that doesn't overlap the Slurm cluster's VPC and the two networks need to be connected using VPN or AWS Direct Connect. The on-prem firewall must allow ingress and egress from the VPC. The ports are used to connect to the file systems, slurm controllers, and allow traffic between virtual desktops and compute nodes. DNS Requirements Local network DNS must have an entry for the slurm controller or have a forwarding rule to the AWS provided DNS in the Slurm VPC. File System Requirements All of the compute nodes in the cluster, including the on-prem nodes, must have file system mounts that replicate the same directory structure. This can involve mounting filesystems across VPN or Direct Connect or synchronizing file systems using tools like rsync or NetApp FlexCache or SnapMirror. Performance will dictate the architecture of the file system. Slurm Configuration of On-Premises Compute Nodes The slurm cluster's configuration file allows the configuration of on-premises compute nodes. The Slurm cluster will not provision any of the on-prem nodes, network, or firewall, but it will configure the cluster's resources to be used by the on-prem nodes. All that needs to be configured are the configuration file for the on-prem nodes and the CIDR block. InstanceConfig: UseSpot: true DefaultPartition: CentOS_7_x86_64_spot NodesPerInstanceType: 10 BaseOsArchitecture: CentOS: {7: [x86_64]} Include: MaxSizeOnly: false InstanceFamilies: - t3 InstanceTypes: [] Exclude: InstanceFamilies: [] InstanceTypes: - '.+\\.(micro|nano)' # Not enough memory - '.*\\.metal' OnPremComputeNodes: ConfigFile: 'slurm_nodes_on_prem.conf' CIDR: '10.1.0.0/16' slurm_nodes_on_prem.conf # # ON PREMISES COMPUTE NODES # # Config file with list of statically provisioned on-premises compute nodes that # are managed by this cluster. # # These nodes must be addressable on the network and firewalls must allow access on all ports # required by slurm. # # The compute nodes must have mounts that mirror the compute cluster including mounting the slurm file system # or a mirror of it. NodeName=Default State=DOWN NodeName=onprem-c7-x86-t3-2xl-0 NodeAddr=onprem-c7-x86-t3-2xl-0.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-1 NodeAddr=onprem-c7-x86-t3-2xl-1.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-2 NodeAddr=onprem-c7-x86-t3-2xl-2.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-3 NodeAddr=onprem-c7-x86-t3-2xl-3.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-4 NodeAddr=onprem-c7-x86-t3-2xl-4.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-5 NodeAddr=onprem-c7-x86-t3-2xl-5.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-6 NodeAddr=onprem-c7-x86-t3-2xl-6.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-7 NodeAddr=onprem-c7-x86-t3-2xl-7.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-8 NodeAddr=onprem-c7-x86-t3-2xl-8.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-9 NodeAddr=onprem-c7-x86-t3-2xl-9.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 # # # OnPrem Partition # # The is the default partition and includes all nodes from the 1st OS. # PartitionName=onprem Default=YES PriorityTier=20000 Nodes=\\ onprem-c7-x86-t3-2xl-[0-9] # # Always on partitions # SuspendExcParts=onprem Simulating an On-Premises Network Using AWS Create a new VPC with public and private subnets and NAT gateways. To simulate the latency between an AWS region and on-prem you can create the VPC in a different region in your account. The CIDR must not overlap with the Slurm VPC. Create a VPC peering connection to your Slurm VPC and accept the connection in the Slurm VPC. Create routes in the private subnets for the CIDR of the peered VPC and route it to the vpc peering connection. Add the on-prem VPC to the Slurm VPC's Route53 private local zone. Create a Route53 private hosted zone for the on-prem compute nodes and add it to the onprem VPC and the slurm VPC so that onprem compute nodes can be resolved. Copy the Slurm AMIs to the region of the on-prem VPC. Create an instance using the copied AMI. Connect to the instance and confirm that the mount points mounted correctly. You will probably have to change the DNS names for the file systems to IP addresses. I created A records in the Route53 zone for the file systems so that if the IP addresses ever change in the future I can easily update them in one place without having to create a new AMI or updated any instances. Create a new AMI from the instance. Create compute node instances from the new AMI and run the following commands on them get the slurmd daemon running so they can join the slurm cluster. # Instance specific variables hostname=onprem-c7-x86-t3-2xl-0 # Domain specific variables onprem_domain=onprem.com source /etc/profile.d/instance_vars.sh # munge needs to be running before calling scontrol /usr/bin/cp /opt/slurm/$ClusterName/config/munge.key /etc/munge/munge.key systemctl enable munged systemctl start munged ipaddress=$(hostname -I) $SLURM_ROOT/bin/scontrol update nodename=${hostname} nodeaddr=$ipaddress # Set hostname hostname_fqdn=${hostname}.${onprem_domain} if [ $(hostname) != $hostname_fqdn ]; then hostnamectl --static set-hostname $hostname_fqdn hostnamectl --pretty set-hostname $hostname fi if [ -e /opt/slurm/${ClusterName}/config/users_groups.json ] && [ -e /opt/slurm/${ClusterName}/bin/create_users_groups.py ]; then /opt/slurm/${ClusterName}/bin/create_users_groups.py -i /opt/slurm/${ClusterName}/config/users_groups.json fi # Create directory for slurmd.log logs_dir=/opt/slurm/${ClusterName}/logs/nodes/${hostname} if [[ ! -d $logs_dir ]]; then mkdir -p $logs_dir fi if [[ -e /var/log/slurm ]]; then rm -rf /var/log/slurm fi ln -s $logs_dir /var/log/slurm systemctl enable slurmd systemctl start slurmd # Restart so that log file goes to file system systemctl restart spot_monitor","title":"On-Premises Integration (legacy)"},{"location":"onprem/#on-premises-integration-legacy","text":"The slurm cluster can also be configured to manage on-premises compute nodes. The user must configure the on-premises compute nodes and then give the configuration information.","title":"On-Premises Integration (legacy)"},{"location":"onprem/#network-requirements","text":"The on-prem network must have a CIDR range that doesn't overlap the Slurm cluster's VPC and the two networks need to be connected using VPN or AWS Direct Connect. The on-prem firewall must allow ingress and egress from the VPC. The ports are used to connect to the file systems, slurm controllers, and allow traffic between virtual desktops and compute nodes.","title":"Network Requirements"},{"location":"onprem/#dns-requirements","text":"Local network DNS must have an entry for the slurm controller or have a forwarding rule to the AWS provided DNS in the Slurm VPC.","title":"DNS Requirements"},{"location":"onprem/#file-system-requirements","text":"All of the compute nodes in the cluster, including the on-prem nodes, must have file system mounts that replicate the same directory structure. This can involve mounting filesystems across VPN or Direct Connect or synchronizing file systems using tools like rsync or NetApp FlexCache or SnapMirror. Performance will dictate the architecture of the file system.","title":"File System Requirements"},{"location":"onprem/#slurm-configuration-of-on-premises-compute-nodes","text":"The slurm cluster's configuration file allows the configuration of on-premises compute nodes. The Slurm cluster will not provision any of the on-prem nodes, network, or firewall, but it will configure the cluster's resources to be used by the on-prem nodes. All that needs to be configured are the configuration file for the on-prem nodes and the CIDR block. InstanceConfig: UseSpot: true DefaultPartition: CentOS_7_x86_64_spot NodesPerInstanceType: 10 BaseOsArchitecture: CentOS: {7: [x86_64]} Include: MaxSizeOnly: false InstanceFamilies: - t3 InstanceTypes: [] Exclude: InstanceFamilies: [] InstanceTypes: - '.+\\.(micro|nano)' # Not enough memory - '.*\\.metal' OnPremComputeNodes: ConfigFile: 'slurm_nodes_on_prem.conf' CIDR: '10.1.0.0/16' slurm_nodes_on_prem.conf # # ON PREMISES COMPUTE NODES # # Config file with list of statically provisioned on-premises compute nodes that # are managed by this cluster. # # These nodes must be addressable on the network and firewalls must allow access on all ports # required by slurm. # # The compute nodes must have mounts that mirror the compute cluster including mounting the slurm file system # or a mirror of it. NodeName=Default State=DOWN NodeName=onprem-c7-x86-t3-2xl-0 NodeAddr=onprem-c7-x86-t3-2xl-0.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-1 NodeAddr=onprem-c7-x86-t3-2xl-1.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-2 NodeAddr=onprem-c7-x86-t3-2xl-2.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-3 NodeAddr=onprem-c7-x86-t3-2xl-3.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-4 NodeAddr=onprem-c7-x86-t3-2xl-4.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-5 NodeAddr=onprem-c7-x86-t3-2xl-5.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-6 NodeAddr=onprem-c7-x86-t3-2xl-6.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-7 NodeAddr=onprem-c7-x86-t3-2xl-7.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-8 NodeAddr=onprem-c7-x86-t3-2xl-8.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-9 NodeAddr=onprem-c7-x86-t3-2xl-9.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 # # # OnPrem Partition # # The is the default partition and includes all nodes from the 1st OS. # PartitionName=onprem Default=YES PriorityTier=20000 Nodes=\\ onprem-c7-x86-t3-2xl-[0-9] # # Always on partitions # SuspendExcParts=onprem","title":"Slurm Configuration of On-Premises Compute Nodes"},{"location":"onprem/#simulating-an-on-premises-network-using-aws","text":"Create a new VPC with public and private subnets and NAT gateways. To simulate the latency between an AWS region and on-prem you can create the VPC in a different region in your account. The CIDR must not overlap with the Slurm VPC. Create a VPC peering connection to your Slurm VPC and accept the connection in the Slurm VPC. Create routes in the private subnets for the CIDR of the peered VPC and route it to the vpc peering connection. Add the on-prem VPC to the Slurm VPC's Route53 private local zone. Create a Route53 private hosted zone for the on-prem compute nodes and add it to the onprem VPC and the slurm VPC so that onprem compute nodes can be resolved. Copy the Slurm AMIs to the region of the on-prem VPC. Create an instance using the copied AMI. Connect to the instance and confirm that the mount points mounted correctly. You will probably have to change the DNS names for the file systems to IP addresses. I created A records in the Route53 zone for the file systems so that if the IP addresses ever change in the future I can easily update them in one place without having to create a new AMI or updated any instances. Create a new AMI from the instance. Create compute node instances from the new AMI and run the following commands on them get the slurmd daemon running so they can join the slurm cluster. # Instance specific variables hostname=onprem-c7-x86-t3-2xl-0 # Domain specific variables onprem_domain=onprem.com source /etc/profile.d/instance_vars.sh # munge needs to be running before calling scontrol /usr/bin/cp /opt/slurm/$ClusterName/config/munge.key /etc/munge/munge.key systemctl enable munged systemctl start munged ipaddress=$(hostname -I) $SLURM_ROOT/bin/scontrol update nodename=${hostname} nodeaddr=$ipaddress # Set hostname hostname_fqdn=${hostname}.${onprem_domain} if [ $(hostname) != $hostname_fqdn ]; then hostnamectl --static set-hostname $hostname_fqdn hostnamectl --pretty set-hostname $hostname fi if [ -e /opt/slurm/${ClusterName}/config/users_groups.json ] && [ -e /opt/slurm/${ClusterName}/bin/create_users_groups.py ]; then /opt/slurm/${ClusterName}/bin/create_users_groups.py -i /opt/slurm/${ClusterName}/config/users_groups.json fi # Create directory for slurmd.log logs_dir=/opt/slurm/${ClusterName}/logs/nodes/${hostname} if [[ ! -d $logs_dir ]]; then mkdir -p $logs_dir fi if [[ -e /var/log/slurm ]]; then rm -rf /var/log/slurm fi ln -s $logs_dir /var/log/slurm systemctl enable slurmd systemctl start slurmd # Restart so that log file goes to file system systemctl restart spot_monitor","title":"Simulating an On-Premises Network Using AWS"},{"location":"plugin-integration/","text":"Plugin Integration Into Existing Slurm Cluster (legacy) If you have an existing Slurm cluster and want to enable it to scale into AWS then you can integrate the SlurmPlugin.py into your existing cluster. The plugin implements the Slurm power saving API scripts so the integration is as simple as copying the plugin and it's scripts into your file system and then modifying your slurm configuration to use them. Network Requirements The instructions for integrating the AWS cluster with on-premises compute nodes contains the network requirements . The VPC's DNS must be configured so that it can resolve the on-prem hostnames such as the slurm controller and file systems. File System Requirements The compute nodes in the AWS VPC will need access to the same file systems paths as your existing compute nodes. This can easily be accomplished by mounting your on-prem file systems across the VPN of Direct Connect connection, but that will likely have significant latency and performance impacts. If your on-prem file systems are NetApp Ontap then you can create Amazon FSx for NetApp Ontap file systems configured with FlexCache volumes to provide low latency, high performance caches of your on-prem file systems on AWS. If you do not use NetApp then you can use Amazon File Cache to create high performance file caches of your on-prem file systems. Create AWS Slurm Cluster The plugin requires a number of configuration files as well as AWS resources for the compute nodes such as Amazon Machine Images (AMIs), IAM instance roles, and security groups. The easiest way to create all of the required resources is to follow the instructions to deploy a cluster with on-premises compute nodes into your AWS VPC and then use those resources to configure your existing cluster. Note that slurm_nodes_on_prem.conf can be empty and doesn't have to include any on-prem compute nodes. The configuration file should include the file systems that should be mounted on the AWS compute nodes. After you've deployed the AWS cluster, Mount the slurm file system on your slurm controller at /opt/slurm/ cluster-name AWS Credentials The power saving scripts run as the slurm user on your slurm controller and require AWS IAM permissions to orchestrate AWS instances for your Slurm cluster. Specifically, the slurm controller needs access to the AMIs and permission to start, stop, and terminate EC2 instances. The permissions are defined in the SlurmCtlPolicy resource of the cluster's Cloudformation stack. You can create a slurm IAM user and attach the SlurmCtlPolicy. Then you can create IAM credentials for the slurm user and save them on the slurm controller using aws config . Amazon Machine Images for Compute Nodes You can modify the AMIs created by the AWS slurm cluster for use by your on-prem cluster. SSM Parameter Store Variables The plugin reads serveral SSM parameters to get information that it needs to orchestrate instances. SSM Parameter Description / cluster-name /SlurmNodeAmi/ os-distribution / os-major-version / architecture / region AMIs for AWS EC2 compute nodes / cluster-name /SlurmNodeEc2KeyPairs/ region EC2 Keypairs for each region / cluster-name /SlurmNodeSecurityGroups/ region Security groups for AWS EC2 compute nodes Field Valid Values os-distribution AlmaLinux | Amazon | CentOS | RedHat | Rocky architecture arm64 | x86_64 Slurm Configuration The existing slurm cluster's configuration file will need to be updated to to configure the AWS compute nodes and to use the SlurmPlugin.py . Update your slurm.conf based on /opt/slurm/slurmaws/etc/slurm.conf . At a minimum you will need to add the following lines. # CommunicationParameters: # NoAddrCache: Do not assume that nodes will retain their IP addresses. Do not cache node->ip mapping CommunicationParameters = NoAddrCache # ReturnToService # 0: Make node available if it registers with a valid configuration regardless of why it is down. # 1: Only return to service if DOWN due to being non-responsive. # 2: A DOWN node will become available for use upon registration with a valid configuration. ReturnToService=2 # TreeWidth is set to the maximum for cloud clusters so messages go directly between controller and nodes. # https://slurm.schedmd.com/elastic_computing.html TreeWidth = 65533 # # SlurmctldParameters # cloud_dns: Do not set cloud_dns unless joining the domain and adding nodes to DNS # Joining the domain overloads the DCs when lots of nodes are started so # not joining the domain or using cloud_dns. # idle_on_node_suspend: Mark nodes as idle, regardless of current state, when suspending # nodes with SuspendProgram so that nodes will be eligible to be resumed at a later time. # node_reg_mem_percent: Percentage of memory a node is allowed to register with without being marked as invalid with low memory. # Use this so can configure nodes with 100% of their memory. # Without this option the node will fail because the system uses some of the memory. SlurmctldParameters=\\ idle_on_node_suspend,\\ ,node_reg_mem_percent=90 # # Allow users to see state of nodes that are powered down PrivateData = cloud # # POWER SAVE SUPPORT FOR IDLE NODES # SuspendProgram = /opt/slurm/slurmaws/bin/slurm_ec2_stop.py ResumeProgram = /opt/slurm/slurmaws/bin/slurm_ec2_resume.py ResumeFailProgram = /opt/slurm/slurmaws/bin/slurm_ec2_resume_fail.py # Maximum time between when a node is suspended and when suspend is complete. # At that time it should be ready to be resumed. SuspendTimeout = 60 ResumeTimeout = 600 # Number of nodes per minute that can be resumed or suspended ResumeRate = 300 SuspendRate = 60 # Time that a node has to be idle or down before being suspended # Should be >= (SuspendTimeout + ResumeTimeout) SuspendTime = 660 include /opt/slurm/slurmaws/etc/slurm_nodes.conf","title":"Plugin Integration Into Existing Slurm Cluster (legacy)"},{"location":"plugin-integration/#plugin-integration-into-existing-slurm-cluster-legacy","text":"If you have an existing Slurm cluster and want to enable it to scale into AWS then you can integrate the SlurmPlugin.py into your existing cluster. The plugin implements the Slurm power saving API scripts so the integration is as simple as copying the plugin and it's scripts into your file system and then modifying your slurm configuration to use them.","title":"Plugin Integration Into Existing Slurm Cluster (legacy)"},{"location":"plugin-integration/#network-requirements","text":"The instructions for integrating the AWS cluster with on-premises compute nodes contains the network requirements . The VPC's DNS must be configured so that it can resolve the on-prem hostnames such as the slurm controller and file systems.","title":"Network Requirements"},{"location":"plugin-integration/#file-system-requirements","text":"The compute nodes in the AWS VPC will need access to the same file systems paths as your existing compute nodes. This can easily be accomplished by mounting your on-prem file systems across the VPN of Direct Connect connection, but that will likely have significant latency and performance impacts. If your on-prem file systems are NetApp Ontap then you can create Amazon FSx for NetApp Ontap file systems configured with FlexCache volumes to provide low latency, high performance caches of your on-prem file systems on AWS. If you do not use NetApp then you can use Amazon File Cache to create high performance file caches of your on-prem file systems.","title":"File System Requirements"},{"location":"plugin-integration/#create-aws-slurm-cluster","text":"The plugin requires a number of configuration files as well as AWS resources for the compute nodes such as Amazon Machine Images (AMIs), IAM instance roles, and security groups. The easiest way to create all of the required resources is to follow the instructions to deploy a cluster with on-premises compute nodes into your AWS VPC and then use those resources to configure your existing cluster. Note that slurm_nodes_on_prem.conf can be empty and doesn't have to include any on-prem compute nodes. The configuration file should include the file systems that should be mounted on the AWS compute nodes. After you've deployed the AWS cluster, Mount the slurm file system on your slurm controller at /opt/slurm/ cluster-name","title":"Create AWS Slurm Cluster"},{"location":"plugin-integration/#aws-credentials","text":"The power saving scripts run as the slurm user on your slurm controller and require AWS IAM permissions to orchestrate AWS instances for your Slurm cluster. Specifically, the slurm controller needs access to the AMIs and permission to start, stop, and terminate EC2 instances. The permissions are defined in the SlurmCtlPolicy resource of the cluster's Cloudformation stack. You can create a slurm IAM user and attach the SlurmCtlPolicy. Then you can create IAM credentials for the slurm user and save them on the slurm controller using aws config .","title":"AWS Credentials"},{"location":"plugin-integration/#amazon-machine-images-for-compute-nodes","text":"You can modify the AMIs created by the AWS slurm cluster for use by your on-prem cluster.","title":"Amazon Machine Images for Compute Nodes"},{"location":"plugin-integration/#ssm-parameter-store-variables","text":"The plugin reads serveral SSM parameters to get information that it needs to orchestrate instances. SSM Parameter Description / cluster-name /SlurmNodeAmi/ os-distribution / os-major-version / architecture / region AMIs for AWS EC2 compute nodes / cluster-name /SlurmNodeEc2KeyPairs/ region EC2 Keypairs for each region / cluster-name /SlurmNodeSecurityGroups/ region Security groups for AWS EC2 compute nodes Field Valid Values os-distribution AlmaLinux | Amazon | CentOS | RedHat | Rocky architecture arm64 | x86_64","title":"SSM Parameter Store Variables"},{"location":"plugin-integration/#slurm-configuration","text":"The existing slurm cluster's configuration file will need to be updated to to configure the AWS compute nodes and to use the SlurmPlugin.py . Update your slurm.conf based on /opt/slurm/slurmaws/etc/slurm.conf . At a minimum you will need to add the following lines. # CommunicationParameters: # NoAddrCache: Do not assume that nodes will retain their IP addresses. Do not cache node->ip mapping CommunicationParameters = NoAddrCache # ReturnToService # 0: Make node available if it registers with a valid configuration regardless of why it is down. # 1: Only return to service if DOWN due to being non-responsive. # 2: A DOWN node will become available for use upon registration with a valid configuration. ReturnToService=2 # TreeWidth is set to the maximum for cloud clusters so messages go directly between controller and nodes. # https://slurm.schedmd.com/elastic_computing.html TreeWidth = 65533 # # SlurmctldParameters # cloud_dns: Do not set cloud_dns unless joining the domain and adding nodes to DNS # Joining the domain overloads the DCs when lots of nodes are started so # not joining the domain or using cloud_dns. # idle_on_node_suspend: Mark nodes as idle, regardless of current state, when suspending # nodes with SuspendProgram so that nodes will be eligible to be resumed at a later time. # node_reg_mem_percent: Percentage of memory a node is allowed to register with without being marked as invalid with low memory. # Use this so can configure nodes with 100% of their memory. # Without this option the node will fail because the system uses some of the memory. SlurmctldParameters=\\ idle_on_node_suspend,\\ ,node_reg_mem_percent=90 # # Allow users to see state of nodes that are powered down PrivateData = cloud # # POWER SAVE SUPPORT FOR IDLE NODES # SuspendProgram = /opt/slurm/slurmaws/bin/slurm_ec2_stop.py ResumeProgram = /opt/slurm/slurmaws/bin/slurm_ec2_resume.py ResumeFailProgram = /opt/slurm/slurmaws/bin/slurm_ec2_resume_fail.py # Maximum time between when a node is suspended and when suspend is complete. # At that time it should be ready to be resumed. SuspendTimeout = 60 ResumeTimeout = 600 # Number of nodes per minute that can be resumed or suspended ResumeRate = 300 SuspendRate = 60 # Time that a node has to be idle or down before being suspended # Should be >= (SuspendTimeout + ResumeTimeout) SuspendTime = 660 include /opt/slurm/slurmaws/etc/slurm_nodes.conf","title":"Slurm Configuration"},{"location":"rest_api/","text":"Slurm REST API The Slurm REST API give a programmatic way to access the features of Slurm. The REST API can be used, for example, to use a Lambda function to submit jobs to the Slurm cluster. How to use the REST API The following shows how to run a simple REST call. source /opt/slurm/{{ClusterName}}/config/slurm_config.sh unset SLURM_JWT . <(scontrol token) wget --header \"X-SLURM-USER-TOKEN: $SLURM_JWT\" --header \"X-SLURM-USER-NAME: $USER\" -q $SLURMRESTD_URL/slurm/v0.0.38/diag/ -O - The REST API is documented at https://slurm.schedmd.com/rest_api.html . The token returned by scontrol token has a default lifetime of 3600 seconds (1 hour). For automation, a cron job on the Slurm controller creates a new token for the root and slurmrestd users every 30 minutes and stores them in SSM Parameter Store at /{{ClusterName}}/slurmrestd/jwt/{{user_name}} . These tokens can be used by automations such as a Lambda function to access the REST API. An example Lambda function called {{ClusterName}}-CallSlurmRestApiLambda shows how to call various API functions. You can use this as a template to write functions that use your Slurm cluster for automations.","title":"Slurm REST API"},{"location":"rest_api/#slurm-rest-api","text":"The Slurm REST API give a programmatic way to access the features of Slurm. The REST API can be used, for example, to use a Lambda function to submit jobs to the Slurm cluster.","title":"Slurm REST API"},{"location":"rest_api/#how-to-use-the-rest-api","text":"The following shows how to run a simple REST call. source /opt/slurm/{{ClusterName}}/config/slurm_config.sh unset SLURM_JWT . <(scontrol token) wget --header \"X-SLURM-USER-TOKEN: $SLURM_JWT\" --header \"X-SLURM-USER-NAME: $USER\" -q $SLURMRESTD_URL/slurm/v0.0.38/diag/ -O - The REST API is documented at https://slurm.schedmd.com/rest_api.html . The token returned by scontrol token has a default lifetime of 3600 seconds (1 hour). For automation, a cron job on the Slurm controller creates a new token for the root and slurmrestd users every 30 minutes and stores them in SSM Parameter Store at /{{ClusterName}}/slurmrestd/jwt/{{user_name}} . These tokens can be used by automations such as a Lambda function to access the REST API. An example Lambda function called {{ClusterName}}-CallSlurmRestApiLambda shows how to call various API functions. You can use this as a template to write functions that use your Slurm cluster for automations.","title":"How to use the REST API"},{"location":"run_jobs/","text":"Run Jobs This page is to give some basic instructions on how to run and monitor jobs on Slurm. Slurm provides excellent man pages for all of its commands, so if you have questions refer to the man pages. Set Up Load the environment module for Slurm to configure your PATH and Slurm related environment variables. module load {{ClusterName}} The modulefile sets environment variables that control the defaults for Slurm commands. These are documented in the man pages for each command. If you don't like the defaults then you can set them in your environment (for example, your .bashrc) and the modulefile won't change any variables that are already set. The environment variables can always be overridden by the command line options. For example, the SQUEUE_FORMAT2 and SQUEUE_SORT environment variables are set so that the default output format is easier to read and contains useful information that isn't in the default format. Key Slurm Commands The key Slurm commands are Command Description Example salloc Create a compute allocation. salloc -c 1 --mem 1G -C 'spot&GHz:3.1' srun Run a job within an allocation. srun --pty bin/bash sbatch Submit a batch script sbatch -c 1 --mem 1G -C 'spot&GHz:3.1' script squeue Get job status scancel Cancel a job scancel jobid sinfo Get info about Slurm node status sinfo -p all scontrol view or modify Slurm configuration and state scontrol show node nodename sstat Display various status information about a running job/step sshare Tool for listing fair share information sprio View the factors that comprise a job's scheduling priority sacct Display accounting data for jobs sreport Generate reports from the Slurm accounting data. sview Graphical tool for viewing cluster state sbatch The most common options for sbatch are listed here. For more details run man sbatch . Options Description Default -p, --partition= partition-names Select the partition/partitions to run job on. Set by slurm.InstanceConfig.DefaultPartition in config file. -t, --time= time Set a limit on total run time of the job. SBATCH_TIMELIMIT=\"1:0:0\" (1 hour) -c, --cpus-per-task= ncpus Number of cores. Default is 1. --mem= size[units] Amount of memory. Default unit is M. Valid units are [K|M|G|T]. SBATCH_MEM_PER_NODE=100M -L, --licenses= license Licenses used by the job. -a, --array= indexes Submit job array -C, --constraint= list Features required by the job. Multiple constraints can be specified with AND(&) and OR( ). -d, --dependency= dependency-list Don't start the job until the dependencies have been completed. -D, --chdir= directory Set the working directory of the job --wait Do not exit until the job finishes, Exit code of sbatch will be the same as the exit code of the job. --wrap Wrap shell commands in a batch script. Run a simulation build followed by a regression build_jobid=$(sbatch -c 4 --mem 4G -L vcs_build -C 'GHz:4|GHz:4.5' -t 30:0 sim-build.sh) if sbatch -d \"afterok:$build_jobid\" -c 1 --mem 100M --wait submit-regression.sh; then echo \"Regression Passed\" else echo \"Regression Failed\" fi srun The srun is usually used to open a pseudo terminal on a compute node for you to run interactive jobs. It accepts most of the same options as sbatch to request cpus, memory, and node features. To open up a pseudo terminal in your shell on a compute node with 4 cores and 16G of memory, execute the following command. srun -c 4 --mem 8G --pty /bin/bash This will queue a job and when it is allocated to a node and the node runs, the job control will be returned to your shell, but stdin and stdout will be on the compute node. If you set your DISPLAY environment variable and allow external X11 connections you can use this to run interactive GUI jobs on the compute node and have the windows on your instance. xhost + export DISPLAY=$(hostname):$(echo $DISPLAY | cut -d ':' -f 2) srun -c 4 --mem 8G --pty /bin/bash emacs . # Or whatever gui application you want to run. Should open a window. Another way to run interactive GUI jobs is to use srun 's --x11 flag to enable X11 forwarding. srun -c 1 --mem 8G --pty --x11 emacs squeue The squeue command shows the status of jobs. The output format can be customized using the --format or --Format options and you can configure the default output format using the corresponding SQUEUE_FORMAT or SQUEUE_FORMAT2 environment variables. squeue sprio Use sprio to get information about a job's priority. This can be useful to figure out why a job is scheduled before or after another job. sprio -j10,11 sacct Display accounting information about jobs. For example, it can be used to get the requested CPU and memory and see the CPU time and memory actually used. sacct -o JobID,User,JobName,AllocCPUS,State,ExitCode,Elapsed,CPUTime,MaxRSS,MaxVMSize,ReqCPUS,ReqMem,SystemCPU,TotalCPU,UserCPU -j 44 This shows more details. sacct --allclusters --allusers --federation --starttime 1970-01-01 --format 'Submit,Start,End,jobid%15,State%15,user,account,cluster%15,AllocCPUS,AllocNodes,ExitCode,ReqMem,MaxRSS,MaxVMSize,MaxPages,Elapsed,CPUTime,UserCPU,SystemCPU,TotalCPU' | less For more information: man sacct sreport The sreport command can be used to generate report from the Slurm database. Other Slurm Commands Use man command to get information about these less commonly used Slurm commands. Command Description sacctmgr View/modify Slurm account information sattach Attach to a job step sbcast Transmit a file to the nodes allocated to a Slurm job. scrontab Manage slurm crontab files sdiag Diagnostic tool for Slurm. Shows information related to slurmctld execution. seff sgather Transmit a file from the nodes allocated to a Slurm job. sh5util Tool for merging HDF5 files from the acct_gather_profile plugin that gathers detailed data for jobs. sjobexitmod Modify derived exit code of a job strigger Set, get, or clear Slurm trigger information","title":"Run Jobs"},{"location":"run_jobs/#run-jobs","text":"This page is to give some basic instructions on how to run and monitor jobs on Slurm. Slurm provides excellent man pages for all of its commands, so if you have questions refer to the man pages.","title":"Run Jobs"},{"location":"run_jobs/#set-up","text":"Load the environment module for Slurm to configure your PATH and Slurm related environment variables. module load {{ClusterName}} The modulefile sets environment variables that control the defaults for Slurm commands. These are documented in the man pages for each command. If you don't like the defaults then you can set them in your environment (for example, your .bashrc) and the modulefile won't change any variables that are already set. The environment variables can always be overridden by the command line options. For example, the SQUEUE_FORMAT2 and SQUEUE_SORT environment variables are set so that the default output format is easier to read and contains useful information that isn't in the default format.","title":"Set Up"},{"location":"run_jobs/#key-slurm-commands","text":"The key Slurm commands are Command Description Example salloc Create a compute allocation. salloc -c 1 --mem 1G -C 'spot&GHz:3.1' srun Run a job within an allocation. srun --pty bin/bash sbatch Submit a batch script sbatch -c 1 --mem 1G -C 'spot&GHz:3.1' script squeue Get job status scancel Cancel a job scancel jobid sinfo Get info about Slurm node status sinfo -p all scontrol view or modify Slurm configuration and state scontrol show node nodename sstat Display various status information about a running job/step sshare Tool for listing fair share information sprio View the factors that comprise a job's scheduling priority sacct Display accounting data for jobs sreport Generate reports from the Slurm accounting data. sview Graphical tool for viewing cluster state","title":"Key Slurm Commands"},{"location":"run_jobs/#sbatch","text":"The most common options for sbatch are listed here. For more details run man sbatch . Options Description Default -p, --partition= partition-names Select the partition/partitions to run job on. Set by slurm.InstanceConfig.DefaultPartition in config file. -t, --time= time Set a limit on total run time of the job. SBATCH_TIMELIMIT=\"1:0:0\" (1 hour) -c, --cpus-per-task= ncpus Number of cores. Default is 1. --mem= size[units] Amount of memory. Default unit is M. Valid units are [K|M|G|T]. SBATCH_MEM_PER_NODE=100M -L, --licenses= license Licenses used by the job. -a, --array= indexes Submit job array -C, --constraint= list Features required by the job. Multiple constraints can be specified with AND(&) and OR( ). -d, --dependency= dependency-list Don't start the job until the dependencies have been completed. -D, --chdir= directory Set the working directory of the job --wait Do not exit until the job finishes, Exit code of sbatch will be the same as the exit code of the job. --wrap Wrap shell commands in a batch script.","title":"sbatch"},{"location":"run_jobs/#run-a-simulation-build-followed-by-a-regression","text":"build_jobid=$(sbatch -c 4 --mem 4G -L vcs_build -C 'GHz:4|GHz:4.5' -t 30:0 sim-build.sh) if sbatch -d \"afterok:$build_jobid\" -c 1 --mem 100M --wait submit-regression.sh; then echo \"Regression Passed\" else echo \"Regression Failed\" fi","title":"Run a simulation build followed by a regression"},{"location":"run_jobs/#srun","text":"The srun is usually used to open a pseudo terminal on a compute node for you to run interactive jobs. It accepts most of the same options as sbatch to request cpus, memory, and node features. To open up a pseudo terminal in your shell on a compute node with 4 cores and 16G of memory, execute the following command. srun -c 4 --mem 8G --pty /bin/bash This will queue a job and when it is allocated to a node and the node runs, the job control will be returned to your shell, but stdin and stdout will be on the compute node. If you set your DISPLAY environment variable and allow external X11 connections you can use this to run interactive GUI jobs on the compute node and have the windows on your instance. xhost + export DISPLAY=$(hostname):$(echo $DISPLAY | cut -d ':' -f 2) srun -c 4 --mem 8G --pty /bin/bash emacs . # Or whatever gui application you want to run. Should open a window. Another way to run interactive GUI jobs is to use srun 's --x11 flag to enable X11 forwarding. srun -c 1 --mem 8G --pty --x11 emacs","title":"srun"},{"location":"run_jobs/#squeue","text":"The squeue command shows the status of jobs. The output format can be customized using the --format or --Format options and you can configure the default output format using the corresponding SQUEUE_FORMAT or SQUEUE_FORMAT2 environment variables. squeue","title":"squeue"},{"location":"run_jobs/#sprio","text":"Use sprio to get information about a job's priority. This can be useful to figure out why a job is scheduled before or after another job. sprio -j10,11","title":"sprio"},{"location":"run_jobs/#sacct","text":"Display accounting information about jobs. For example, it can be used to get the requested CPU and memory and see the CPU time and memory actually used. sacct -o JobID,User,JobName,AllocCPUS,State,ExitCode,Elapsed,CPUTime,MaxRSS,MaxVMSize,ReqCPUS,ReqMem,SystemCPU,TotalCPU,UserCPU -j 44 This shows more details. sacct --allclusters --allusers --federation --starttime 1970-01-01 --format 'Submit,Start,End,jobid%15,State%15,user,account,cluster%15,AllocCPUS,AllocNodes,ExitCode,ReqMem,MaxRSS,MaxVMSize,MaxPages,Elapsed,CPUTime,UserCPU,SystemCPU,TotalCPU' | less For more information: man sacct","title":"sacct"},{"location":"run_jobs/#sreport","text":"The sreport command can be used to generate report from the Slurm database.","title":"sreport"},{"location":"run_jobs/#other-slurm-commands","text":"Use man command to get information about these less commonly used Slurm commands. Command Description sacctmgr View/modify Slurm account information sattach Attach to a job step sbcast Transmit a file to the nodes allocated to a Slurm job. scrontab Manage slurm crontab files sdiag Diagnostic tool for Slurm. Shows information related to slurmctld execution. seff sgather Transmit a file from the nodes allocated to a Slurm job. sh5util Tool for merging HDF5 files from the acct_gather_profile plugin that gathers detailed data for jobs. sjobexitmod Modify derived exit code of a job strigger Set, get, or clear Slurm trigger information","title":"Other Slurm Commands"},{"location":"soca_integration/","text":"SOCA Integration Integration with SOCA is straightforward. Set the following parameters in your config file. Parameter Description Value VpcId VPC id for the SOCA cluster vpc-xxxxxx SubmitterSecurityGroupIds The ComputeNode security group name and id cluster-id - ComputeNodeSG : sg-xxxxxxxx ExtraMounts Add the mount parameters for the /apps and /data directories. This is required for access to the home directory. Deploy your slurm cluster. Connect to the SOCA Scheduler instance and follow the instructions to Create users_groups.json . Connect to a remote desktop instance and follow the instructions in Configure submission hosts to use the cluster . If all users need to use the cluster then it is probably best to create a custom AMI that is configured with the configuration commands. You are now ready to run jobs from your SOCA desktop.","title":"SOCA Integration"},{"location":"soca_integration/#soca-integration","text":"Integration with SOCA is straightforward. Set the following parameters in your config file. Parameter Description Value VpcId VPC id for the SOCA cluster vpc-xxxxxx SubmitterSecurityGroupIds The ComputeNode security group name and id cluster-id - ComputeNodeSG : sg-xxxxxxxx ExtraMounts Add the mount parameters for the /apps and /data directories. This is required for access to the home directory. Deploy your slurm cluster. Connect to the SOCA Scheduler instance and follow the instructions to Create users_groups.json . Connect to a remote desktop instance and follow the instructions in Configure submission hosts to use the cluster . If all users need to use the cluster then it is probably best to create a custom AMI that is configured with the configuration commands. You are now ready to run jobs from your SOCA desktop.","title":"SOCA Integration"}]}