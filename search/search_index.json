{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AWS EDA Slurm Cluster This repository contains an AWS Cloud Development Kit (CDK) application that creates a Slurm cluster that is suitable for running production EDA workloads on AWS. The original (legacy) version of this repo that used a custom Python plugin to integrate Slurm with AWS has been deprecated and is no longer supported. It can be found on the v1 branch. The latest version of the repo uses AWS ParallelCluster for the core Slurm infrastructure and AWS integration. The big advantage of moving to AWS ParallelCluster is that it is a supported AWS service. Currently, some of the features of the legacy version are not supported in the ParallelCluster version, but work continues to add features to ParallelCluster so that those features may be supported in the future. Key features are: Automatic scaling of AWS EC2 instances based on demand Use any AWS EC2 instance type including Graviton2 Use of spot instances Memory-aware scheduling License-aware scheduling (Manages tool licenses as a consumable resource) User and group fair share scheduling Handling of spot terminations Handling of insufficient capacity exceptions Batch and interactive partitions (queues) Slurm accounting database CloudWatch dashboard Job preemption Manage on-premises compute nodes Configure partitions (queues) and nodes that are always on to support reserved instances (RIs) and savings plans (SPs). Integration with Research and Engineering Studio on AWS (RES) Features in the legacy version and not in the ParallelCluster version: Heterogenous clusters with mixed OSes and CPU architectures on compute nodes. Multi-AZ support. Supported by ParallelCluster, but not currently implemented. Multi-region support AWS Fault Injection Simulator (FIS) templates to test spot terminations Multi-cluster federation ParallelCluster Limitations Number of \"Compute Resources\" (CRs) is limited to 50 which limits the number of instance types allowed in a cluster. ParallelCluster can have multiple instance types in a compute resource (CR), but with memory based scheduling enabled, they must all have the same number of cores and amount of memory. All Slurm instances must have the same OS and CPU architecture. Stand-alone Slurm database daemon instance. Prevents federation. Multi-region support. This is unlikely to change because multi-region services run against our archiectural philosophy. Federation may be an option but its current implementation limits scheduler performance and doesn't allow cluster prioritization so jobs land on random clusters. Slurm Limitations Job preemption based on licenses Federation doesn't support prioritizing federated clusters for job scheduling. Result is jobs scattered across the federated clusters. Operating System and Processor Architecture Support This Slurm cluster supports the following OSes: ParallelCluster: Amazon Linux 2 CentOS 7 RedHat 7, 8 and 9 Rocky Linux 8 and 9 This Slurm cluster supports both Intel/AMD (x86_64) based instances and Graviton (arm64/aarch64) based instances. Graviton instances require Amazon Linux 2 or RedHat/Rocky >=8 operating systems. RedHat 7 and CentOS 7 do not support Graviton 2. This provides the following different combinations of OS and processor architecture. ParallelCluster: Amazon Linux 2 and arm64 Amazon Linux 2 and x86_64 CentOS 7 and x86_64 RedHat 7 and x86_64 RedHat 8/9 and arm64 RedHat 8/9 and x86_64 Rocky 8/9 and arm64 Rocky 8/9 and x86_64 Note that in ParallelCluster, all compute nodes must have the same OS and architecture. However, you can create as many clusters as you require. Documentation View on GitHub Pages You can also view the docs locally, The docs are in the docs directory. You can view them in an editor or using the mkdocs tool. I recommend installing mkdocs in a python virtual environment. python3 -m venv ~/.mkdocs_venv source ~/.mkdocs_venv/bin/activate pip install mkdocs Then run mkdocs. source ~/.mkdocs_venv/bin/activate mkdocs serve & firefox http://127.0.0.1:8000/ & Open a browser to: http://127.0.0.1:8000/ Or you can simply let make do this for you. make local-docs Security See CONTRIBUTING for more information. License This library is licensed under the MIT-0 License. See the LICENSE file.","title":"AWS EDA Slurm Cluster"},{"location":"#aws-eda-slurm-cluster","text":"This repository contains an AWS Cloud Development Kit (CDK) application that creates a Slurm cluster that is suitable for running production EDA workloads on AWS. The original (legacy) version of this repo that used a custom Python plugin to integrate Slurm with AWS has been deprecated and is no longer supported. It can be found on the v1 branch. The latest version of the repo uses AWS ParallelCluster for the core Slurm infrastructure and AWS integration. The big advantage of moving to AWS ParallelCluster is that it is a supported AWS service. Currently, some of the features of the legacy version are not supported in the ParallelCluster version, but work continues to add features to ParallelCluster so that those features may be supported in the future. Key features are: Automatic scaling of AWS EC2 instances based on demand Use any AWS EC2 instance type including Graviton2 Use of spot instances Memory-aware scheduling License-aware scheduling (Manages tool licenses as a consumable resource) User and group fair share scheduling Handling of spot terminations Handling of insufficient capacity exceptions Batch and interactive partitions (queues) Slurm accounting database CloudWatch dashboard Job preemption Manage on-premises compute nodes Configure partitions (queues) and nodes that are always on to support reserved instances (RIs) and savings plans (SPs). Integration with Research and Engineering Studio on AWS (RES) Features in the legacy version and not in the ParallelCluster version: Heterogenous clusters with mixed OSes and CPU architectures on compute nodes. Multi-AZ support. Supported by ParallelCluster, but not currently implemented. Multi-region support AWS Fault Injection Simulator (FIS) templates to test spot terminations Multi-cluster federation ParallelCluster Limitations Number of \"Compute Resources\" (CRs) is limited to 50 which limits the number of instance types allowed in a cluster. ParallelCluster can have multiple instance types in a compute resource (CR), but with memory based scheduling enabled, they must all have the same number of cores and amount of memory. All Slurm instances must have the same OS and CPU architecture. Stand-alone Slurm database daemon instance. Prevents federation. Multi-region support. This is unlikely to change because multi-region services run against our archiectural philosophy. Federation may be an option but its current implementation limits scheduler performance and doesn't allow cluster prioritization so jobs land on random clusters. Slurm Limitations Job preemption based on licenses Federation doesn't support prioritizing federated clusters for job scheduling. Result is jobs scattered across the federated clusters.","title":"AWS EDA Slurm Cluster"},{"location":"#operating-system-and-processor-architecture-support","text":"This Slurm cluster supports the following OSes: ParallelCluster: Amazon Linux 2 CentOS 7 RedHat 7, 8 and 9 Rocky Linux 8 and 9 This Slurm cluster supports both Intel/AMD (x86_64) based instances and Graviton (arm64/aarch64) based instances. Graviton instances require Amazon Linux 2 or RedHat/Rocky >=8 operating systems. RedHat 7 and CentOS 7 do not support Graviton 2. This provides the following different combinations of OS and processor architecture. ParallelCluster: Amazon Linux 2 and arm64 Amazon Linux 2 and x86_64 CentOS 7 and x86_64 RedHat 7 and x86_64 RedHat 8/9 and arm64 RedHat 8/9 and x86_64 Rocky 8/9 and arm64 Rocky 8/9 and x86_64 Note that in ParallelCluster, all compute nodes must have the same OS and architecture. However, you can create as many clusters as you require.","title":"Operating System and Processor Architecture Support"},{"location":"#documentation","text":"View on GitHub Pages You can also view the docs locally, The docs are in the docs directory. You can view them in an editor or using the mkdocs tool. I recommend installing mkdocs in a python virtual environment. python3 -m venv ~/.mkdocs_venv source ~/.mkdocs_venv/bin/activate pip install mkdocs Then run mkdocs. source ~/.mkdocs_venv/bin/activate mkdocs serve & firefox http://127.0.0.1:8000/ & Open a browser to: http://127.0.0.1:8000/ Or you can simply let make do this for you. make local-docs","title":"Documentation"},{"location":"#security","text":"See CONTRIBUTING for more information.","title":"Security"},{"location":"#license","text":"This library is licensed under the MIT-0 License. See the LICENSE file.","title":"License"},{"location":"CONTRIBUTING/","text":"Contributing Guidelines Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community. Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution. Reporting Bugs/Feature Requests We welcome you to use the GitHub issue tracker to report bugs or suggest features. When filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful: A reproducible test case or series of steps The version of our code being used Any modifications you've made relevant to the bug Anything unusual about your environment or deployment Contributing via Pull Requests Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that: You are working against the latest source on the main branch. You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already. You open an issue to discuss any significant work - we would hate for your time to be wasted. To send us a pull request, please: Fork the repository. Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change. Ensure local tests pass. Commit to your fork using clear commit messages. Send us a pull request, answering any default questions in the pull request interface. Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation. GitHub provides additional document on forking a repository and creating a pull request . Finding contributions to work on Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start. Code of Conduct This project has adopted the Amazon Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments. Security issue notifications If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page . Please do not create a public github issue. Licensing See the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution.","title":"Contributing Guidelines"},{"location":"CONTRIBUTING/#contributing-guidelines","text":"Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community. Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.","title":"Contributing Guidelines"},{"location":"CONTRIBUTING/#reporting-bugsfeature-requests","text":"We welcome you to use the GitHub issue tracker to report bugs or suggest features. When filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful: A reproducible test case or series of steps The version of our code being used Any modifications you've made relevant to the bug Anything unusual about your environment or deployment","title":"Reporting Bugs/Feature Requests"},{"location":"CONTRIBUTING/#contributing-via-pull-requests","text":"Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that: You are working against the latest source on the main branch. You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already. You open an issue to discuss any significant work - we would hate for your time to be wasted. To send us a pull request, please: Fork the repository. Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change. Ensure local tests pass. Commit to your fork using clear commit messages. Send us a pull request, answering any default questions in the pull request interface. Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation. GitHub provides additional document on forking a repository and creating a pull request .","title":"Contributing via Pull Requests"},{"location":"CONTRIBUTING/#finding-contributions-to-work-on","text":"Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start.","title":"Finding contributions to work on"},{"location":"CONTRIBUTING/#code-of-conduct","text":"This project has adopted the Amazon Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments.","title":"Code of Conduct"},{"location":"CONTRIBUTING/#security-issue-notifications","text":"If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page . Please do not create a public github issue.","title":"Security issue notifications"},{"location":"CONTRIBUTING/#licensing","text":"See the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution.","title":"Licensing"},{"location":"config/","text":"Configuraton File Format This project creates a ParallelCluster configuration file that is documented in the ParallelCluster User Guide . termination_protection : bool StackName : str Region : str SshKeyPair : str VpcId : str CIDR : str SubnetId : str ErrorSnsTopicArn : str TimeZone : str AdditionalSecurityGroupsStackName : str RESStackName : str ExternalLoginNodes : - Tags : - Key: str Values: [ str ] SecurityGroupId: str DomainJoinedInstance : - Tags : - Key: str Values: [ str ] SecurityGroupId: str slurm : ParallelClusterConfig : Version : str ClusterConfig : dict Image : Os : str CustomAmi : str Architecture : str ComputeNodeAmi : str EnableEfa : bool Database : DatabaseStackName : str FQDN : str Port : str AdminUserName : str AdminPasswordSecretArn : str ClientSecurityGroup : SecurityGroupName: SecurityGroupId Slurmdbd : SlurmdbdStackName : str Host : str Port : str ClientSecurityGroup : str Dcv: Enabled : bool Port : int AllowedIps : str LoginNodes : Pools : - Name : str Count : int InstanceType : str GracetimePeriod : int Image : CustomAmi : str Ssh : KeyName : str Networking : SubnetIds : - str SecurityGroups : - str AdditionalSecurityGroups : - str Iam : InstanceRole : str InstanceProfile : str AdditionalIamPolicies : - Policy : str ClusterName : str MungeKeySecret : str SlurmCtl : SlurmdPort : int instance_type : str volume_size : str CloudWatchPeriod : int PreemptMode : str PreemptType : str PreemptExemptTime : str SlurmConfOverrides : str SlurmrestdUid : str AdditionalSecurityGroups : - str AdditionalIamPolicies : - str Imds : Secured : bool InstanceConfig : UseOnDemand : str UseSpot : str DisableSimultaneousMultithreading : str Exclude : InstanceFamilies : - str InstanceTypes : - str Include : MaxSizeOnly : bool InstanceFamilies : - str - str: useOnDemand: bool UseSpot: bool DisableSimultaneousMultithreading: bool InstanceTypes : - str - str: UseOnDemand: bool UseSpot: bool DisableSimultaneousMultithreading: bool NodeCounts : DefaultMinCount : str DefaultMaxCount : str ComputeResourceCounts : str: # ComputeResourceName MinCount : int MaxCount : int AdditionalSecurityGroups : - str AdditionalIamPolicies : - str OnPremComputeNodes : ConfigFile : str CIDR : str Partition : str SlurmUid : int storage : ExtraMounts : - dest : str src : str type : str options : str StorageType : str FileSystemId : str VolumeId : str Licenses : LicenseName : Count : int Server : str Port : str ServerType : StatusScript : Top Level Config termination_protection Enable Cloudformation Stack termination protection default=True StackName The name of the configuration stack that will configure ParallelCluster and deploy it. If you do not specify the ClusterName then it will default to a value based on the StackName. If StackName ends in -config then ClusterName will be the StackName with -config stripped off. Otherwise it will be the StackName with -cl (for cluster) appended. Optional so can be specified on the command-line default='slurm-config' Region AWS region where the cluster will be deployed. Optional so can be specified on the command-line SshKeyPair Default EC2 key pair that will be used for all cluster instances. Optional so can be specified on the command-line VpcId The ID of the VPC where the cluster will be deployed. Optional so can be specified on the command-line CIDR The CIDR of the VPC. This is used in security group rules. SubnetId The ID of the VPC subnet where the cluster will be deployed. Optional. If not specified then the first private subnet is chosen. If no private subnets exist, then the first isolated subnet is chosen. If no isolated subnets exist, the the first public subnet is chosen. We recommend using a private or isolated subnet. ErrorSnsTopicArn The ARN of an existing SNS topic. Errors will be published to the SNS topic. You can subscribe to the topic so that you are notified for things like script or lambda errors. Optional, but highly recommended TimeZone The time zone to use for all EC2 instances in the cluster. default='US/Central' AdditionalSecurityGroupsStackName If you followed the automated process to create security groups for external login nodes and file systems , then specify the stack name that you deployed and the additional security groups will be configured for the head and compute nodes. RESStackName If you are deploying the cluster to use from Research and Engineering Studio (RES) virtual desktops, then you can specify the stack name for the RES environment to automate the integration. The virtual desktops automatically get configured to use the cluster. This requires you to configure security groups for external login nodes . The Slurm binaries will be compiled for the OS of the desktops and and environment modulefile will be created so that the users just need to load the cluster modulefile to use the cluster. ExternalLoginNodes An array of specifications for instances that should automatically be configured as Slurm login nodes. Each array element contains one or more tags that will be used to select login node instances. It also includes the security group id that must be attached to the login node to give it access to the slurm cluster. The tags for a group of instances is an array with the tag name and an array of values. A lambda function processes each login node specification. It uses the tags to select running instances. If the instances do not have the security group attached, then it will attach the security group. It will then run a script each instance to configure it as a login node for the slurm cluster. To use the cluster, users simply load the environment modulefile that is created by the script. For example, to configure RES virtual desktops as Slurm login nodes the following configuration is added. --- ExternalLoginNodes: - Tags: - Key: 'res:EnvironmentName' Values: [ 'res-eda' ] - Key: 'res:NodeType' Values: ['virtual-desktop-dcv-host'] SecurityGroupId: <SlurmLoginNodeSGId> DomainJoinedInstance A specifications for a domain joined instance that will be used to create and update users_groups.json. It also includes the security group id that must be attached to the login node to give it access to the slurm head node so it can mount the slurm configuration file system. The tags for the instance is an array with the tag name and an array of values. A lambda function the specification. It uses the tags to select a running instance. If the instance does not have the security group attached, then it will attach the security group. It will then run a script each instance to configure it to save all of the users and groups into a json file that is used to create local users and groups on compute nodes when they boot. For example, to configure the RES cluster manager, the following configuration is added. --- DomainJoinedInstance: - Tags: - Key: 'Name' Values: [ 'res-eda-cluster-manager' ] - Key: 'res:EnvironmentName' Values: [ 'res-eda' ] - Key: 'res:ModuleName' Values: [ 'cluster-manager' ] - Key: 'res:ModuleId' Values: [ 'cluster-manager' ] - Key: 'app' Values: ['virtual-desktop-dcv-host'] SecurityGroupId: <SlurmLoginNodeSGId> slurm Slurm configuration parameters. ParallelClusterConfig ParallelCluster specific configuration parameters. Version The ParallelCluster version. This is required and cannot be changed after the cluster is created. Updating to a new version of ParallelCluster requires either deleting the current cluster or creating a new cluster. ClusterConfig type: dict Additional ParallelCluster configuration settings that will be directly added to the configuration without checking. This will will be used to create the initial ParallelCluster configuration and other settings in this configuration file will override values in the dict. This exists to enable further customization of ParallelCluster beyond what this configuration supports. The cluster configuration format is documented in the ParallelCluster User Guide. For example, if you want to change the ScaledownIdletime , you would add the following to your config file. slurm: ParallelClusterConfig: ClusterConfig: Scheduling: SlurmSettings: ScaledownIdletime: 20 Image The OS and AMI to use for the head node and compute nodes. OS See the ParallelCluster docs for the supported OS distributions and versions. CustomAmi See the ParallelCluster docs for the custom AMI documentation. NOTE : A CustomAmi must be provided for Rocky8 or Rocky9. All other distributions have a default AMI that is provided by ParallelCluster. Architecture The CPU architecture to use for the cluster. ParallelCluster doesn't support heterogeneous clusters. All of the instances must have the same CPU architecture and the same OS. The cluster, however, can be accessed from login nodes of any architecture and OS. Valid Values: arm64 x86_64 default: x86_64 ComputeNodeAmi AMI to use for compute nodes. All compute nodes will use the same AMI. The default AMI is selected by the Image parameters. EnableEfa type: bool default: False Recommend to not use EFA unless necessary to avoid insufficient capacity errors when starting new instances in group or when multiple instance types in the group. See https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-cluster Database Optional Note : Starting with ParallelCluster 3.10.0, you should use slurm/ParallelClusterConfig/ Slurmdbd instead of slurm/ParallelClusterConfig/Database. You cannot have both parameters. Configure the Slurm database to use with the cluster. This is created independently of the cluster so that the same database can be used with multiple clusters. See Create ParallelCluster Slurm Database on the deployment prerequisites page. If you used the CloudFormation template provided by ParallelCluster , then the easiest way to configure it is to pass the name of the stack in slurm/ParallelClusterConfig/Database/ DatabaseStackName . All of the other parameters will be pulled from the outputs of the stack. See the ParallelCluster documentation . DatabaseStackName Name of the ParallelCluster CloudFormation stack that created the database. The following parameters will be set using the outputs of the stack: FQDN Port AdminUserName AdminPasswordSecretArn ClientSecurityGroup FQDN Used with the Port to set the Uri of the database. Database: Port type: int Database's port. AdminUserName type: str The identity that Slurm uses to connect to the database, write accounting logs, and perform queries. The user must have both read and write permissions on the database. Sets the UserName parameter in ParallelCluster. AdminPasswordSecretArn type: str The Amazon Resource Name (ARN) of the AWS Secrets Manager secret that contains the AdminUserName plaintext password. This password is used together with AdminUserName and Slurm accounting to authenticate on the database server. Sets the PasswordSecretArn parameter in ParallelCluster. Database: ClientSecurityGroup Security group that has permissions to connect to the database. Required to be attached to the head node that is running slurmdbd so that the port connection to the database is allowed. Slurmdbd Note : This is not supported before ParallelCluster 3.10.0. If you specify this parameter then you cannot specify slurm/ParallelClusterConfig/ Database . Optional Configure an external Slurmdbd instance to use with the cluster. The Slurmdbd instance provides access to the shared Slurm database. This is created independently of the cluster so that the same database can be used with multiple clusters. This is created independently of the cluster so that the same slurmdbd instance can be used with multiple clusters. See Create Slurmdbd instance on the deployment prerequisites page. If you used the CloudFormation template provided by ParallelCluster , then the easiest way to configure it is to pass the name of the stack in slurm/ParallelClusterConfig/Database/ SlurmdbdStackName . All of the other parameters will be pulled from the parameters and outputs of the stack. See the ParallelCluster documentation for ExternalSlurmdbd . SlurmdbdStackName Name of the ParallelCluster CloudFormation stack that created the Slurmdbd instance. The following parameters will be set using the outputs of the stack: Host Port ClientSecurityGroup Slurmdbd: Host IP address or DNS name of the Slurmdbd instance. Slurmdbd: Port Default: 6819 Port used by the slurmdbd daemon on the Slurmdbd instance. Slurmdbd: ClientSecurityGroup Security group that has access to use the Slurmdbd instance. This will be added as an extra security group to the head node. ClusterName Name of the ParallelCluster cluster. Default: If StackName ends with \"-config\" then ClusterName is StackName with \"-config\" stripped off. Otherwise add \"-cl\" to end of StackName. MungeKeySecret AWS secret with a base64 encoded munge key to use for the cluster. For an existing secret can be the secret name or the ARN. If the secret doesn't exist one will be created, but won't be part of the cloudformation stack so that it won't be deleted when the stack is deleted. Required if your login nodes need to use more than 1 cluster. See Create Munge Key for more details. SlurmCtl Configure the Slurm head node or controller. Required, but can be an empty dict to accept all of the defaults. SlurmdPort Port used for the slurmd daemon on the compute nodes. default=6818 type: int instance_type Instance type of the head node. Must match the architecture of the cluster. volume_size The size of the EBS root volume on the head node in GB. default=200 type: int CloudWatchPeriod The frequency of CloudWatch metrics in seconds. default=5 type: int PreemptMode Set job preemption policy for the cluster. Jobs can be set to be preemptible when they are submitted. This allows higher priority jobs to preempt a running job when resources are constrained. This policy sets what happens to the preempted jobs. Slurm documentation Valid values: 'OFF' 'CANCEL' 'GANG' 'REQUEUE' 'SUSPEND' default='REQUEUE' PreemptType Slurm documentation Valid values: 'preempt/none' 'preempt/partition_prio' 'preempt/qos' default='preempt/partition_prio' PreemptExemptTime Slurm documentation Global option for minimum run time for all jobs before they can be considered for preemption. A time of -1 disables the option, equivalent to 0. Acceptable time formats include \"minutes\", \"minutes:seconds\", \"hours:minutes:seconds\", \"days-hours\", \"days-hours:minutes\", and \"days-hours:minutes:seconds\". default='0' type: str SlurmConfOverrides File that will be included at end of slurm.conf to override configuration parameters. This allows you to customize the slurm configuration arbitrarily. This should be used with caution since it can result in errors that make the cluster non-functional. type: str SlurmrestdUid User ID for the slurmrestd daemon. type: int default=901 SlurmRestApiVersion The REST API version. This is automatically set based on the Slurm version being used by the ParallelCluster version. type: str default: ''0.0.39' Head Node AdditionalSecurityGroups Additional security groups that will be added to the head node instance. Head Node AdditionalIamPolicies List of Amazon Resource Names (ARNs) of IAM policies for Amazon EC2 that will be added to the head node instance. InstanceConfig Configure the instances used by the cluster for compute nodes. ParallelCluster is limited to a total of 50 compute resources and we only put 1 instance type in each compute resource. This limits you to a total of 50 instance types per cluster. If you need more instance types than that, then you will need to create multiple clusters. If you configure both on-demand and spot for each instance type, then the limit is effectively 25 instance types because 2 compute resources will be created for each instance type. If you configure more than 50 instance types then the installer will fail with an error. You will then need to modify your configuration to either include fewer instance types or exclude instance types from the configuration. If no Include and Exclude parameters are specified then default EDA instance types will be configured with both On-Demand and Spot Instances configured.. The defaults will include the latest generation instance families in the c, m, r, x, and u families. Older instance families are excluded. Metal instance types are also excluded. Specific instance types are also excluded to keep the total number of instance types under 50. If multiple instance types have the same amount of memory, then the instance types with the highest core counts are excluded. This is because EDA workloads are typically memory limited, not core limited. If any Include or Exclude parameters are specified, then minimal defaults will be used for the parameters that aren't specified. By default, all instance families are included and no specific instance types are included. By default, all instance types with less than 4 GiB of memory are excluded because they don't have enough memory for a Slurm compute node. If no includes or excludes are provided, the defaults are: slurm: InstanceConfig: Exclude: InstanceFamilies: - 'a1' # Graviton 1 - 'c4' # Replaced by c5 - 'd2' # SSD optimized - 'g3' # Replaced by g4 - 'g3s' # Replaced by g4 - 'h1' # SSD optimized - 'i3' # SSD optimized - 'i3en' # SSD optimized - 'm4' # Replaced by m5 - 'p2' # Replaced by p3 - 'p3' - 'p3dn' - 'r4' # Replaced by r5 - 't2' # Replaced by t3 - 'x1' - 'x1e' InstanceTypes: - '.*\\.metal' # Reduce the number of selected instance types to 25. # Exclude larger core counts for each memory size # 2 GB: - 'c7a.medium' - 'c7g.medium' # 4 GB: m7a.medium, m7g.medium - 'c7a.large' - 'c7g.large' # 8 GB: r7a.medium, r7g.medium - 'm5zn.large' - 'm7a.large' - 'm7g.large' - 'c7a.xlarge' - 'c7g.xlarge' # 16 GB: r7a.large, x2gd.medium, r7g.large - 'r7iz.large' - 'm5zn.xlarge' - 'm7a.xlarge' - 'm7g.xlarge' - 'c7a.2xlarge' - 'c7g.2xlarge' # 32 GB: r7a.xlarge, x2gd.large, r7g.xlarge - 'r7iz.xlarge' - 'm5zn.2xlarge' - 'm7a.2xlarge' - 'm7g.2xlarge' - 'c7a.4xlarge' - 'c7g.4xlarge' # 64 GB: r7a.2xlarge, x2gd.xlarge, r7g.2xlarge - 'r7iz.2xlarge' - 'm7a.4xlarge' - 'm7g.4xlarge' - 'c7a.8xlarge' - 'c7g.8xlarge' # 96 GB: - 'm5zn.6xlarge' - 'c7a.12xlarge' - 'c7g.12xlarge' # 128 GB: x2iedn.xlarge, r7iz.4xlarge, x2gd.2xlarge, r7g.4xlarge - 'r7a.4xlarge' - 'm7a.8xlarge' - 'm7g.8xlarge' - 'c7a.16xlarge' - 'c7g.8xlarge' # 192 GB: m5zn.12xlarge, m7a.12xlarge, m7g.12xlarge - 'c7a.24xlarge' # 256 GB: x2iedn.2xlarge, x2iezn.2xlarge, x2gd.4xlarge, r7g.8xlarge - 'r7iz.8xlarge' - 'r7a.8xlarge' - 'm7a.16xlarge' - 'm7g.16xlarge' - 'c7a.32xlarge' # 384 GB: r7iz.12xlarge, r7g.12xlarge - 'r7a.12xlarge' - 'm7a.24xlarge' - 'c7a.48xlarge' # 512 GB: x2iedn.4xlarge, x2iezn.4xlarge, x2gd.8xlarge, r7g.16xlarge - 'r7iz.16xlarge' - 'r7a.16xlarge' - 'm7a.32xlarge' # 768 GB: r7a.24xlarge, x2gd.12xlarge - 'x2iezn.6xlarge' - 'm7a.48xlarge' # 1024 GB: x2iedn.8xlarge, x2iezn.8xlarge, x2gd.16xlarge - 'r7iz.32xlarge' - 'r7a.32xlarge' # 1536 GB: x2iezn.12xlarge, x2idn.24xlarge - 'r7a.48xlarge' # 2048 GB: x2iedn.16xlarge - 'x2idn.32xlarge' # 3072 GB: x2iedn.24xlarge # 4096 GB: x2iedn.32xlarge Include: InstanceFamilies: - 'c7a' # AMD EPYC 9R14 Processor 3.7 GHz - 'c7g' # AWS Graviton3 Processor 2.6 GHz - 'm5zn' # Intel Xeon Platinum 8252 4.5 GHz - 'm7a' # AMD EPYC 9R14 Processor 3.7 GHz - 'm7g' # AWS Graviton3 Processor 2.6 GHz - 'r7a' # AMD EPYC 9R14 Processor 3.7 GHz - 'r7g' # AWS Graviton3 Processor 2.6 GHz - 'r7iz' # Intel Xeon Scalable (Sapphire Rapids) 3.2 GHz - 'x2gd' # AWS Graviton2 Processor 2.5 GHz 1TB - 'x2idn' # Intel Xeon Scalable (Icelake) 3.5 GHz 2 TB - 'x2iedn' # Intel Xeon Scalable (Icelake) 3.5 GHz 4 TB - 'x2iezn' # Intel Xeon Platinum 8252 4.5 GHz 1.5 TB - 'u.*' InstanceTypes: [] UseOnDemand Configure on-demand instances. This sets the default for all included instance types. It can be overridden for included instance families and by instance types. type: bool default: True UseSpot Configure spot instances. This sets the default for all included instance types. It can be overridden for included instance families and by instance types. type: bool default: True DisableSimultaneousMultithreading type: bool default=True Disable SMT on the compute nodes. If true, multithreading on the compute nodes is disabled. This sets the default for all included instance types. It can be overridden for included instance families and by instance types. Not all instance types can disable multithreading. For a list of instance types that support disabling multithreading, see CPU cores and threads for each CPU core per instance type in the Amazon EC2 User Guide for Linux Instances. Update policy: The compute fleet must be stopped for this setting to be changed for an update. ParallelCluster documentation Exclude Instance families and types to exclude. Exclude patterns are processed first and take precedence over any includes. Instance families and types are regular expressions with implicit '^' and '$' at the begining and end. Exclude InstanceFamilies Regular expressions with implicit '^' and '$' at the begining and end. Default: [] Exclude InstanceTypes Regular expressions with implicit '^' and '$' at the begining and end. Default: [] Include Instance families and types to include. Exclude patterns are processed first and take precedence over any includes. Instance families and types are regular expressions with implicit '^' and '$' at the begining and end. Each element in the array can be either a regular expression string or a dictionary where the only key is the regular expression string and that has overrides UseOnDemand , UseSpot , and DisableSimultaneousMultithreading for the matching instance families or instance types. The settings for instance families overrides the defaults, and the settings for instance types override the others. For example, the following configuration defaults to only On-Demand instances with SMT disabled. It includes all of the r7a, r7i, and r7iz instance types. The r7a instances will only have On-Demand instances. The r7i and r7iz instance types will have spot instances except for the r7i.48xlarge which has spot disabled. This allows you to control these attributes of the compute resources with whatever level of granularity that you need. slurm: InstanceConfig: UseOnDemand: true UseSpot: false DisableSimultaneousMultithreading: true Exclude: InstanceTypes: - .*\\.metal Include: InstanceFamilies: - r7a.* - r7i.*: {UseSpot: true} InstanceTypes: - r7i.48xlarge: {UseSpot: false} MaxSizeOnly type: bool default: False If MaxSizeOnly is True then only the largest instance type in a family will be included unless specific instance types are included. Include InstanceFamilies Regular expressions with implicit '^' and '$' at the begining and end. Default: [] Include InstanceTypes Regular expressions with implicit '^' and '$' at the begining and end. Default: [] NodeCounts Configure the number of compute nodes of each instance type. DefaultMinCount type: int default: 0 Minimum number of compute nodes to keep running in a compute resource. If the number is greater than zero then static nodes will be created. DefaultMaxCount type: int The maximum number of compute nodes to create in a compute resource. ComputeResourceCounts Define compute node counts per compute resource. These counts will override the defaults set by DefaultMinCount and DefaultMaxCount . ComputeResourceName Name of the ParallelCluster compute resource. Can be found using sinfo . # Compute Resource MinCount type: int default: 0 # Compute Resource MaxCount type: int Compute Node AdditionalSecurityGroups Additional security groups that will be added to the compute node instances. Compute Node AdditionalIamPolicies List of Amazon Resource Names (ARNs) of IAM policies for Amazon EC2 that will be added to the compute node instances. OnPremComputeNodes Define on-premises compute nodes that will be managed by the ParallelCluster head node. The compute nodes must be accessible from the head node over the network and any firewalls must allow all of the Slurm ports between the head node and compute nodes. ParallelCluster will be configured to allow the neccessary network traffic and the on-premises firewall can be configured to match the ParallelCluster seccurity groups. ConfigFile Configuration file with the on-premises compute nodes defined in Slurm NodeName format as described in the Slurm slurm.conf documentation . The file will be included in the ParallelCluster slurm.conf so it can technically include any Slurm configuration updates including custom partition definitions. NOTE : The syntax of the file isn't checked and syntax errors can result in the slurmctld daemon failing on the head node. On-Premises CIDR The CIDR that contains the on-premises compute nodes. This is to allow egress from the head node to the on-premises nodes. Partition A partition that will contain all of the on-premises nodes. SlurmUid type: int default: 900 The user id of the slurm user. storage ExtraMounts Additional mounts for compute nodes. This can be used so the compute nodes have the same file structure as the remote desktops. This is used to configure ParallelCluster SharedStorage . For example: storage: ExtraMounts: - dest: \"/tools\" StorageType: FsxOpenZfs VolumeId: 'fsvol-abcd1234' src: 'fs-efgh5678.fsx.us-east-1.amazonaws.com:/fsx/' type: nfs4 options: 'nfsvers=4.1' dest The directory where the file system will be mounted. This sets the MountDir . src The source path on the file system export that will be mounted. type The type of mount. For example, nfs3. options Mount options. StorageType The type of file system to mount. Valid values: Efs FsxLustre FsxOntap FsxOpenZfs FileSystemId Specifies the ID of an existing FSx for Lustre or EFS file system. VolumeId Specifies the volume ID of an existing FSx for ONTAP or FSx for OpenZFS file system. Licenses Configure license counts for the scheduler. If the Slurm database is configured then it will be updated with the license counts. Otherwise, the license counts will be added to slurm.conf. LicenseName The name of the license, for example, VCSCompiler_Net or VCSMXRunTime_Net . This is the license name that users specify when submitting a job. It doesn't have to match the license name reported by the license server, although that probably makes the most sense. Count The number of licenses available to Slurm to use to schedule jobs. Once all of the license are used by running jobs, then any pending jobs will remain pending until a license becomes available. Server The license server hosting the licenses. Not currently used. Port The port on the license server used to request licenses. Not currently used. ServerType The type of license server, such as FlexLM. Not currently used. StatusScript A script that queries the license server and dynamically updates the Slurm database with the actual total number of licenses and the number used. Not currently implemented.","title":"Configuraton File Format"},{"location":"config/#configuraton-file-format","text":"This project creates a ParallelCluster configuration file that is documented in the ParallelCluster User Guide . termination_protection : bool StackName : str Region : str SshKeyPair : str VpcId : str CIDR : str SubnetId : str ErrorSnsTopicArn : str TimeZone : str AdditionalSecurityGroupsStackName : str RESStackName : str ExternalLoginNodes : - Tags : - Key: str Values: [ str ] SecurityGroupId: str DomainJoinedInstance : - Tags : - Key: str Values: [ str ] SecurityGroupId: str slurm : ParallelClusterConfig : Version : str ClusterConfig : dict Image : Os : str CustomAmi : str Architecture : str ComputeNodeAmi : str EnableEfa : bool Database : DatabaseStackName : str FQDN : str Port : str AdminUserName : str AdminPasswordSecretArn : str ClientSecurityGroup : SecurityGroupName: SecurityGroupId Slurmdbd : SlurmdbdStackName : str Host : str Port : str ClientSecurityGroup : str Dcv: Enabled : bool Port : int AllowedIps : str LoginNodes : Pools : - Name : str Count : int InstanceType : str GracetimePeriod : int Image : CustomAmi : str Ssh : KeyName : str Networking : SubnetIds : - str SecurityGroups : - str AdditionalSecurityGroups : - str Iam : InstanceRole : str InstanceProfile : str AdditionalIamPolicies : - Policy : str ClusterName : str MungeKeySecret : str SlurmCtl : SlurmdPort : int instance_type : str volume_size : str CloudWatchPeriod : int PreemptMode : str PreemptType : str PreemptExemptTime : str SlurmConfOverrides : str SlurmrestdUid : str AdditionalSecurityGroups : - str AdditionalIamPolicies : - str Imds : Secured : bool InstanceConfig : UseOnDemand : str UseSpot : str DisableSimultaneousMultithreading : str Exclude : InstanceFamilies : - str InstanceTypes : - str Include : MaxSizeOnly : bool InstanceFamilies : - str - str: useOnDemand: bool UseSpot: bool DisableSimultaneousMultithreading: bool InstanceTypes : - str - str: UseOnDemand: bool UseSpot: bool DisableSimultaneousMultithreading: bool NodeCounts : DefaultMinCount : str DefaultMaxCount : str ComputeResourceCounts : str: # ComputeResourceName MinCount : int MaxCount : int AdditionalSecurityGroups : - str AdditionalIamPolicies : - str OnPremComputeNodes : ConfigFile : str CIDR : str Partition : str SlurmUid : int storage : ExtraMounts : - dest : str src : str type : str options : str StorageType : str FileSystemId : str VolumeId : str Licenses : LicenseName : Count : int Server : str Port : str ServerType : StatusScript :","title":"Configuraton File Format"},{"location":"config/#top-level-config","text":"","title":"Top Level Config"},{"location":"config/#termination_protection","text":"Enable Cloudformation Stack termination protection default=True","title":"termination_protection"},{"location":"config/#stackname","text":"The name of the configuration stack that will configure ParallelCluster and deploy it. If you do not specify the ClusterName then it will default to a value based on the StackName. If StackName ends in -config then ClusterName will be the StackName with -config stripped off. Otherwise it will be the StackName with -cl (for cluster) appended. Optional so can be specified on the command-line default='slurm-config'","title":"StackName"},{"location":"config/#region","text":"AWS region where the cluster will be deployed. Optional so can be specified on the command-line","title":"Region"},{"location":"config/#sshkeypair","text":"Default EC2 key pair that will be used for all cluster instances. Optional so can be specified on the command-line","title":"SshKeyPair"},{"location":"config/#vpcid","text":"The ID of the VPC where the cluster will be deployed. Optional so can be specified on the command-line","title":"VpcId"},{"location":"config/#cidr","text":"The CIDR of the VPC. This is used in security group rules.","title":"CIDR"},{"location":"config/#subnetid","text":"The ID of the VPC subnet where the cluster will be deployed. Optional. If not specified then the first private subnet is chosen. If no private subnets exist, then the first isolated subnet is chosen. If no isolated subnets exist, the the first public subnet is chosen. We recommend using a private or isolated subnet.","title":"SubnetId"},{"location":"config/#errorsnstopicarn","text":"The ARN of an existing SNS topic. Errors will be published to the SNS topic. You can subscribe to the topic so that you are notified for things like script or lambda errors. Optional, but highly recommended","title":"ErrorSnsTopicArn"},{"location":"config/#timezone","text":"The time zone to use for all EC2 instances in the cluster. default='US/Central'","title":"TimeZone"},{"location":"config/#additionalsecuritygroupsstackname","text":"If you followed the automated process to create security groups for external login nodes and file systems , then specify the stack name that you deployed and the additional security groups will be configured for the head and compute nodes.","title":"AdditionalSecurityGroupsStackName"},{"location":"config/#resstackname","text":"If you are deploying the cluster to use from Research and Engineering Studio (RES) virtual desktops, then you can specify the stack name for the RES environment to automate the integration. The virtual desktops automatically get configured to use the cluster. This requires you to configure security groups for external login nodes . The Slurm binaries will be compiled for the OS of the desktops and and environment modulefile will be created so that the users just need to load the cluster modulefile to use the cluster.","title":"RESStackName"},{"location":"config/#externalloginnodes","text":"An array of specifications for instances that should automatically be configured as Slurm login nodes. Each array element contains one or more tags that will be used to select login node instances. It also includes the security group id that must be attached to the login node to give it access to the slurm cluster. The tags for a group of instances is an array with the tag name and an array of values. A lambda function processes each login node specification. It uses the tags to select running instances. If the instances do not have the security group attached, then it will attach the security group. It will then run a script each instance to configure it as a login node for the slurm cluster. To use the cluster, users simply load the environment modulefile that is created by the script. For example, to configure RES virtual desktops as Slurm login nodes the following configuration is added. --- ExternalLoginNodes: - Tags: - Key: 'res:EnvironmentName' Values: [ 'res-eda' ] - Key: 'res:NodeType' Values: ['virtual-desktop-dcv-host'] SecurityGroupId: <SlurmLoginNodeSGId>","title":"ExternalLoginNodes"},{"location":"config/#domainjoinedinstance","text":"A specifications for a domain joined instance that will be used to create and update users_groups.json. It also includes the security group id that must be attached to the login node to give it access to the slurm head node so it can mount the slurm configuration file system. The tags for the instance is an array with the tag name and an array of values. A lambda function the specification. It uses the tags to select a running instance. If the instance does not have the security group attached, then it will attach the security group. It will then run a script each instance to configure it to save all of the users and groups into a json file that is used to create local users and groups on compute nodes when they boot. For example, to configure the RES cluster manager, the following configuration is added. --- DomainJoinedInstance: - Tags: - Key: 'Name' Values: [ 'res-eda-cluster-manager' ] - Key: 'res:EnvironmentName' Values: [ 'res-eda' ] - Key: 'res:ModuleName' Values: [ 'cluster-manager' ] - Key: 'res:ModuleId' Values: [ 'cluster-manager' ] - Key: 'app' Values: ['virtual-desktop-dcv-host'] SecurityGroupId: <SlurmLoginNodeSGId>","title":"DomainJoinedInstance"},{"location":"config/#slurm","text":"Slurm configuration parameters.","title":"slurm"},{"location":"config/#parallelclusterconfig","text":"ParallelCluster specific configuration parameters.","title":"ParallelClusterConfig"},{"location":"config/#version","text":"The ParallelCluster version. This is required and cannot be changed after the cluster is created. Updating to a new version of ParallelCluster requires either deleting the current cluster or creating a new cluster.","title":"Version"},{"location":"config/#clusterconfig","text":"type: dict Additional ParallelCluster configuration settings that will be directly added to the configuration without checking. This will will be used to create the initial ParallelCluster configuration and other settings in this configuration file will override values in the dict. This exists to enable further customization of ParallelCluster beyond what this configuration supports. The cluster configuration format is documented in the ParallelCluster User Guide. For example, if you want to change the ScaledownIdletime , you would add the following to your config file. slurm: ParallelClusterConfig: ClusterConfig: Scheduling: SlurmSettings: ScaledownIdletime: 20","title":"ClusterConfig"},{"location":"config/#image","text":"The OS and AMI to use for the head node and compute nodes.","title":"Image"},{"location":"config/#os","text":"See the ParallelCluster docs for the supported OS distributions and versions.","title":"OS"},{"location":"config/#customami","text":"See the ParallelCluster docs for the custom AMI documentation. NOTE : A CustomAmi must be provided for Rocky8 or Rocky9. All other distributions have a default AMI that is provided by ParallelCluster.","title":"CustomAmi"},{"location":"config/#architecture","text":"The CPU architecture to use for the cluster. ParallelCluster doesn't support heterogeneous clusters. All of the instances must have the same CPU architecture and the same OS. The cluster, however, can be accessed from login nodes of any architecture and OS. Valid Values: arm64 x86_64 default: x86_64","title":"Architecture"},{"location":"config/#computenodeami","text":"AMI to use for compute nodes. All compute nodes will use the same AMI. The default AMI is selected by the Image parameters.","title":"ComputeNodeAmi"},{"location":"config/#enableefa","text":"type: bool default: False Recommend to not use EFA unless necessary to avoid insufficient capacity errors when starting new instances in group or when multiple instance types in the group. See https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-cluster","title":"EnableEfa"},{"location":"config/#database","text":"Optional Note : Starting with ParallelCluster 3.10.0, you should use slurm/ParallelClusterConfig/ Slurmdbd instead of slurm/ParallelClusterConfig/Database. You cannot have both parameters. Configure the Slurm database to use with the cluster. This is created independently of the cluster so that the same database can be used with multiple clusters. See Create ParallelCluster Slurm Database on the deployment prerequisites page. If you used the CloudFormation template provided by ParallelCluster , then the easiest way to configure it is to pass the name of the stack in slurm/ParallelClusterConfig/Database/ DatabaseStackName . All of the other parameters will be pulled from the outputs of the stack. See the ParallelCluster documentation .","title":"Database"},{"location":"config/#databasestackname","text":"Name of the ParallelCluster CloudFormation stack that created the database. The following parameters will be set using the outputs of the stack: FQDN Port AdminUserName AdminPasswordSecretArn ClientSecurityGroup","title":"DatabaseStackName"},{"location":"config/#fqdn","text":"Used with the Port to set the Uri of the database.","title":"FQDN"},{"location":"config/#database-port","text":"type: int Database's port.","title":"Database: Port"},{"location":"config/#adminusername","text":"type: str The identity that Slurm uses to connect to the database, write accounting logs, and perform queries. The user must have both read and write permissions on the database. Sets the UserName parameter in ParallelCluster.","title":"AdminUserName"},{"location":"config/#adminpasswordsecretarn","text":"type: str The Amazon Resource Name (ARN) of the AWS Secrets Manager secret that contains the AdminUserName plaintext password. This password is used together with AdminUserName and Slurm accounting to authenticate on the database server. Sets the PasswordSecretArn parameter in ParallelCluster.","title":"AdminPasswordSecretArn"},{"location":"config/#database-clientsecuritygroup","text":"Security group that has permissions to connect to the database. Required to be attached to the head node that is running slurmdbd so that the port connection to the database is allowed.","title":"Database: ClientSecurityGroup"},{"location":"config/#slurmdbd","text":"Note : This is not supported before ParallelCluster 3.10.0. If you specify this parameter then you cannot specify slurm/ParallelClusterConfig/ Database . Optional Configure an external Slurmdbd instance to use with the cluster. The Slurmdbd instance provides access to the shared Slurm database. This is created independently of the cluster so that the same database can be used with multiple clusters. This is created independently of the cluster so that the same slurmdbd instance can be used with multiple clusters. See Create Slurmdbd instance on the deployment prerequisites page. If you used the CloudFormation template provided by ParallelCluster , then the easiest way to configure it is to pass the name of the stack in slurm/ParallelClusterConfig/Database/ SlurmdbdStackName . All of the other parameters will be pulled from the parameters and outputs of the stack. See the ParallelCluster documentation for ExternalSlurmdbd .","title":"Slurmdbd"},{"location":"config/#slurmdbdstackname","text":"Name of the ParallelCluster CloudFormation stack that created the Slurmdbd instance. The following parameters will be set using the outputs of the stack: Host Port ClientSecurityGroup","title":"SlurmdbdStackName"},{"location":"config/#slurmdbd-host","text":"IP address or DNS name of the Slurmdbd instance.","title":"Slurmdbd: Host"},{"location":"config/#slurmdbd-port","text":"Default: 6819 Port used by the slurmdbd daemon on the Slurmdbd instance.","title":"Slurmdbd: Port"},{"location":"config/#slurmdbd-clientsecuritygroup","text":"Security group that has access to use the Slurmdbd instance. This will be added as an extra security group to the head node.","title":"Slurmdbd: ClientSecurityGroup"},{"location":"config/#clustername","text":"Name of the ParallelCluster cluster. Default: If StackName ends with \"-config\" then ClusterName is StackName with \"-config\" stripped off. Otherwise add \"-cl\" to end of StackName.","title":"ClusterName"},{"location":"config/#mungekeysecret","text":"AWS secret with a base64 encoded munge key to use for the cluster. For an existing secret can be the secret name or the ARN. If the secret doesn't exist one will be created, but won't be part of the cloudformation stack so that it won't be deleted when the stack is deleted. Required if your login nodes need to use more than 1 cluster. See Create Munge Key for more details.","title":"MungeKeySecret"},{"location":"config/#slurmctl","text":"Configure the Slurm head node or controller. Required, but can be an empty dict to accept all of the defaults.","title":"SlurmCtl"},{"location":"config/#slurmdport","text":"Port used for the slurmd daemon on the compute nodes. default=6818 type: int","title":"SlurmdPort"},{"location":"config/#instance_type","text":"Instance type of the head node. Must match the architecture of the cluster.","title":"instance_type"},{"location":"config/#volume_size","text":"The size of the EBS root volume on the head node in GB. default=200 type: int","title":"volume_size"},{"location":"config/#cloudwatchperiod","text":"The frequency of CloudWatch metrics in seconds. default=5 type: int","title":"CloudWatchPeriod"},{"location":"config/#preemptmode","text":"Set job preemption policy for the cluster. Jobs can be set to be preemptible when they are submitted. This allows higher priority jobs to preempt a running job when resources are constrained. This policy sets what happens to the preempted jobs. Slurm documentation Valid values: 'OFF' 'CANCEL' 'GANG' 'REQUEUE' 'SUSPEND' default='REQUEUE'","title":"PreemptMode"},{"location":"config/#preempttype","text":"Slurm documentation Valid values: 'preempt/none' 'preempt/partition_prio' 'preempt/qos' default='preempt/partition_prio'","title":"PreemptType"},{"location":"config/#preemptexempttime","text":"Slurm documentation Global option for minimum run time for all jobs before they can be considered for preemption. A time of -1 disables the option, equivalent to 0. Acceptable time formats include \"minutes\", \"minutes:seconds\", \"hours:minutes:seconds\", \"days-hours\", \"days-hours:minutes\", and \"days-hours:minutes:seconds\". default='0' type: str","title":"PreemptExemptTime"},{"location":"config/#slurmconfoverrides","text":"File that will be included at end of slurm.conf to override configuration parameters. This allows you to customize the slurm configuration arbitrarily. This should be used with caution since it can result in errors that make the cluster non-functional. type: str","title":"SlurmConfOverrides"},{"location":"config/#slurmrestduid","text":"User ID for the slurmrestd daemon. type: int default=901","title":"SlurmrestdUid"},{"location":"config/#slurmrestapiversion","text":"The REST API version. This is automatically set based on the Slurm version being used by the ParallelCluster version. type: str default: ''0.0.39'","title":"SlurmRestApiVersion"},{"location":"config/#head-node-additionalsecuritygroups","text":"Additional security groups that will be added to the head node instance.","title":"Head Node AdditionalSecurityGroups"},{"location":"config/#head-node-additionaliampolicies","text":"List of Amazon Resource Names (ARNs) of IAM policies for Amazon EC2 that will be added to the head node instance.","title":"Head Node AdditionalIamPolicies"},{"location":"config/#instanceconfig","text":"Configure the instances used by the cluster for compute nodes. ParallelCluster is limited to a total of 50 compute resources and we only put 1 instance type in each compute resource. This limits you to a total of 50 instance types per cluster. If you need more instance types than that, then you will need to create multiple clusters. If you configure both on-demand and spot for each instance type, then the limit is effectively 25 instance types because 2 compute resources will be created for each instance type. If you configure more than 50 instance types then the installer will fail with an error. You will then need to modify your configuration to either include fewer instance types or exclude instance types from the configuration. If no Include and Exclude parameters are specified then default EDA instance types will be configured with both On-Demand and Spot Instances configured.. The defaults will include the latest generation instance families in the c, m, r, x, and u families. Older instance families are excluded. Metal instance types are also excluded. Specific instance types are also excluded to keep the total number of instance types under 50. If multiple instance types have the same amount of memory, then the instance types with the highest core counts are excluded. This is because EDA workloads are typically memory limited, not core limited. If any Include or Exclude parameters are specified, then minimal defaults will be used for the parameters that aren't specified. By default, all instance families are included and no specific instance types are included. By default, all instance types with less than 4 GiB of memory are excluded because they don't have enough memory for a Slurm compute node. If no includes or excludes are provided, the defaults are: slurm: InstanceConfig: Exclude: InstanceFamilies: - 'a1' # Graviton 1 - 'c4' # Replaced by c5 - 'd2' # SSD optimized - 'g3' # Replaced by g4 - 'g3s' # Replaced by g4 - 'h1' # SSD optimized - 'i3' # SSD optimized - 'i3en' # SSD optimized - 'm4' # Replaced by m5 - 'p2' # Replaced by p3 - 'p3' - 'p3dn' - 'r4' # Replaced by r5 - 't2' # Replaced by t3 - 'x1' - 'x1e' InstanceTypes: - '.*\\.metal' # Reduce the number of selected instance types to 25. # Exclude larger core counts for each memory size # 2 GB: - 'c7a.medium' - 'c7g.medium' # 4 GB: m7a.medium, m7g.medium - 'c7a.large' - 'c7g.large' # 8 GB: r7a.medium, r7g.medium - 'm5zn.large' - 'm7a.large' - 'm7g.large' - 'c7a.xlarge' - 'c7g.xlarge' # 16 GB: r7a.large, x2gd.medium, r7g.large - 'r7iz.large' - 'm5zn.xlarge' - 'm7a.xlarge' - 'm7g.xlarge' - 'c7a.2xlarge' - 'c7g.2xlarge' # 32 GB: r7a.xlarge, x2gd.large, r7g.xlarge - 'r7iz.xlarge' - 'm5zn.2xlarge' - 'm7a.2xlarge' - 'm7g.2xlarge' - 'c7a.4xlarge' - 'c7g.4xlarge' # 64 GB: r7a.2xlarge, x2gd.xlarge, r7g.2xlarge - 'r7iz.2xlarge' - 'm7a.4xlarge' - 'm7g.4xlarge' - 'c7a.8xlarge' - 'c7g.8xlarge' # 96 GB: - 'm5zn.6xlarge' - 'c7a.12xlarge' - 'c7g.12xlarge' # 128 GB: x2iedn.xlarge, r7iz.4xlarge, x2gd.2xlarge, r7g.4xlarge - 'r7a.4xlarge' - 'm7a.8xlarge' - 'm7g.8xlarge' - 'c7a.16xlarge' - 'c7g.8xlarge' # 192 GB: m5zn.12xlarge, m7a.12xlarge, m7g.12xlarge - 'c7a.24xlarge' # 256 GB: x2iedn.2xlarge, x2iezn.2xlarge, x2gd.4xlarge, r7g.8xlarge - 'r7iz.8xlarge' - 'r7a.8xlarge' - 'm7a.16xlarge' - 'm7g.16xlarge' - 'c7a.32xlarge' # 384 GB: r7iz.12xlarge, r7g.12xlarge - 'r7a.12xlarge' - 'm7a.24xlarge' - 'c7a.48xlarge' # 512 GB: x2iedn.4xlarge, x2iezn.4xlarge, x2gd.8xlarge, r7g.16xlarge - 'r7iz.16xlarge' - 'r7a.16xlarge' - 'm7a.32xlarge' # 768 GB: r7a.24xlarge, x2gd.12xlarge - 'x2iezn.6xlarge' - 'm7a.48xlarge' # 1024 GB: x2iedn.8xlarge, x2iezn.8xlarge, x2gd.16xlarge - 'r7iz.32xlarge' - 'r7a.32xlarge' # 1536 GB: x2iezn.12xlarge, x2idn.24xlarge - 'r7a.48xlarge' # 2048 GB: x2iedn.16xlarge - 'x2idn.32xlarge' # 3072 GB: x2iedn.24xlarge # 4096 GB: x2iedn.32xlarge Include: InstanceFamilies: - 'c7a' # AMD EPYC 9R14 Processor 3.7 GHz - 'c7g' # AWS Graviton3 Processor 2.6 GHz - 'm5zn' # Intel Xeon Platinum 8252 4.5 GHz - 'm7a' # AMD EPYC 9R14 Processor 3.7 GHz - 'm7g' # AWS Graviton3 Processor 2.6 GHz - 'r7a' # AMD EPYC 9R14 Processor 3.7 GHz - 'r7g' # AWS Graviton3 Processor 2.6 GHz - 'r7iz' # Intel Xeon Scalable (Sapphire Rapids) 3.2 GHz - 'x2gd' # AWS Graviton2 Processor 2.5 GHz 1TB - 'x2idn' # Intel Xeon Scalable (Icelake) 3.5 GHz 2 TB - 'x2iedn' # Intel Xeon Scalable (Icelake) 3.5 GHz 4 TB - 'x2iezn' # Intel Xeon Platinum 8252 4.5 GHz 1.5 TB - 'u.*' InstanceTypes: []","title":"InstanceConfig"},{"location":"config/#useondemand","text":"Configure on-demand instances. This sets the default for all included instance types. It can be overridden for included instance families and by instance types. type: bool default: True","title":"UseOnDemand"},{"location":"config/#usespot","text":"Configure spot instances. This sets the default for all included instance types. It can be overridden for included instance families and by instance types. type: bool default: True","title":"UseSpot"},{"location":"config/#disablesimultaneousmultithreading","text":"type: bool default=True Disable SMT on the compute nodes. If true, multithreading on the compute nodes is disabled. This sets the default for all included instance types. It can be overridden for included instance families and by instance types. Not all instance types can disable multithreading. For a list of instance types that support disabling multithreading, see CPU cores and threads for each CPU core per instance type in the Amazon EC2 User Guide for Linux Instances. Update policy: The compute fleet must be stopped for this setting to be changed for an update. ParallelCluster documentation","title":"DisableSimultaneousMultithreading"},{"location":"config/#exclude","text":"Instance families and types to exclude. Exclude patterns are processed first and take precedence over any includes. Instance families and types are regular expressions with implicit '^' and '$' at the begining and end.","title":"Exclude"},{"location":"config/#exclude-instancefamilies","text":"Regular expressions with implicit '^' and '$' at the begining and end. Default: []","title":"Exclude InstanceFamilies"},{"location":"config/#exclude-instancetypes","text":"Regular expressions with implicit '^' and '$' at the begining and end. Default: []","title":"Exclude InstanceTypes"},{"location":"config/#include","text":"Instance families and types to include. Exclude patterns are processed first and take precedence over any includes. Instance families and types are regular expressions with implicit '^' and '$' at the begining and end. Each element in the array can be either a regular expression string or a dictionary where the only key is the regular expression string and that has overrides UseOnDemand , UseSpot , and DisableSimultaneousMultithreading for the matching instance families or instance types. The settings for instance families overrides the defaults, and the settings for instance types override the others. For example, the following configuration defaults to only On-Demand instances with SMT disabled. It includes all of the r7a, r7i, and r7iz instance types. The r7a instances will only have On-Demand instances. The r7i and r7iz instance types will have spot instances except for the r7i.48xlarge which has spot disabled. This allows you to control these attributes of the compute resources with whatever level of granularity that you need. slurm: InstanceConfig: UseOnDemand: true UseSpot: false DisableSimultaneousMultithreading: true Exclude: InstanceTypes: - .*\\.metal Include: InstanceFamilies: - r7a.* - r7i.*: {UseSpot: true} InstanceTypes: - r7i.48xlarge: {UseSpot: false}","title":"Include"},{"location":"config/#maxsizeonly","text":"type: bool default: False If MaxSizeOnly is True then only the largest instance type in a family will be included unless specific instance types are included.","title":"MaxSizeOnly"},{"location":"config/#include-instancefamilies","text":"Regular expressions with implicit '^' and '$' at the begining and end. Default: []","title":"Include InstanceFamilies"},{"location":"config/#include-instancetypes","text":"Regular expressions with implicit '^' and '$' at the begining and end. Default: []","title":"Include InstanceTypes"},{"location":"config/#nodecounts","text":"Configure the number of compute nodes of each instance type.","title":"NodeCounts"},{"location":"config/#defaultmincount","text":"type: int default: 0 Minimum number of compute nodes to keep running in a compute resource. If the number is greater than zero then static nodes will be created.","title":"DefaultMinCount"},{"location":"config/#defaultmaxcount","text":"type: int The maximum number of compute nodes to create in a compute resource.","title":"DefaultMaxCount"},{"location":"config/#computeresourcecounts","text":"Define compute node counts per compute resource. These counts will override the defaults set by DefaultMinCount and DefaultMaxCount .","title":"ComputeResourceCounts"},{"location":"config/#computeresourcename","text":"Name of the ParallelCluster compute resource. Can be found using sinfo .","title":"ComputeResourceName"},{"location":"config/#compute-resource-mincount","text":"type: int default: 0","title":"# Compute Resource MinCount"},{"location":"config/#compute-resource-maxcount","text":"type: int","title":"# Compute Resource MaxCount"},{"location":"config/#compute-node-additionalsecuritygroups","text":"Additional security groups that will be added to the compute node instances.","title":"Compute Node AdditionalSecurityGroups"},{"location":"config/#compute-node-additionaliampolicies","text":"List of Amazon Resource Names (ARNs) of IAM policies for Amazon EC2 that will be added to the compute node instances.","title":"Compute Node AdditionalIamPolicies"},{"location":"config/#onpremcomputenodes","text":"Define on-premises compute nodes that will be managed by the ParallelCluster head node. The compute nodes must be accessible from the head node over the network and any firewalls must allow all of the Slurm ports between the head node and compute nodes. ParallelCluster will be configured to allow the neccessary network traffic and the on-premises firewall can be configured to match the ParallelCluster seccurity groups.","title":"OnPremComputeNodes"},{"location":"config/#configfile","text":"Configuration file with the on-premises compute nodes defined in Slurm NodeName format as described in the Slurm slurm.conf documentation . The file will be included in the ParallelCluster slurm.conf so it can technically include any Slurm configuration updates including custom partition definitions. NOTE : The syntax of the file isn't checked and syntax errors can result in the slurmctld daemon failing on the head node.","title":"ConfigFile"},{"location":"config/#on-premises-cidr","text":"The CIDR that contains the on-premises compute nodes. This is to allow egress from the head node to the on-premises nodes.","title":"On-Premises CIDR"},{"location":"config/#partition","text":"A partition that will contain all of the on-premises nodes.","title":"Partition"},{"location":"config/#slurmuid","text":"type: int default: 900 The user id of the slurm user.","title":"SlurmUid"},{"location":"config/#storage","text":"","title":"storage"},{"location":"config/#extramounts","text":"Additional mounts for compute nodes. This can be used so the compute nodes have the same file structure as the remote desktops. This is used to configure ParallelCluster SharedStorage . For example: storage: ExtraMounts: - dest: \"/tools\" StorageType: FsxOpenZfs VolumeId: 'fsvol-abcd1234' src: 'fs-efgh5678.fsx.us-east-1.amazonaws.com:/fsx/' type: nfs4 options: 'nfsvers=4.1'","title":"ExtraMounts"},{"location":"config/#dest","text":"The directory where the file system will be mounted. This sets the MountDir .","title":"dest"},{"location":"config/#src","text":"The source path on the file system export that will be mounted.","title":"src"},{"location":"config/#type","text":"The type of mount. For example, nfs3.","title":"type"},{"location":"config/#options","text":"Mount options.","title":"options"},{"location":"config/#storagetype","text":"The type of file system to mount. Valid values: Efs FsxLustre FsxOntap FsxOpenZfs","title":"StorageType"},{"location":"config/#filesystemid","text":"Specifies the ID of an existing FSx for Lustre or EFS file system.","title":"FileSystemId"},{"location":"config/#volumeid","text":"Specifies the volume ID of an existing FSx for ONTAP or FSx for OpenZFS file system.","title":"VolumeId"},{"location":"config/#licenses","text":"Configure license counts for the scheduler. If the Slurm database is configured then it will be updated with the license counts. Otherwise, the license counts will be added to slurm.conf.","title":"Licenses"},{"location":"config/#licensename","text":"The name of the license, for example, VCSCompiler_Net or VCSMXRunTime_Net . This is the license name that users specify when submitting a job. It doesn't have to match the license name reported by the license server, although that probably makes the most sense.","title":"LicenseName"},{"location":"config/#count","text":"The number of licenses available to Slurm to use to schedule jobs. Once all of the license are used by running jobs, then any pending jobs will remain pending until a license becomes available.","title":"Count"},{"location":"config/#server","text":"The license server hosting the licenses. Not currently used.","title":"Server"},{"location":"config/#port","text":"The port on the license server used to request licenses. Not currently used.","title":"Port"},{"location":"config/#servertype","text":"The type of license server, such as FlexLM. Not currently used.","title":"ServerType"},{"location":"config/#statusscript","text":"A script that queries the license server and dynamically updates the Slurm database with the actual total number of licenses and the number used. Not currently implemented.","title":"StatusScript"},{"location":"custom-amis/","text":"Custom AMIs for ParallelCluster ParallelCluster supports building custom ParallelCluster AMIs for the head and compute nodes . You can specify a custom AMI for the entire cluster (head and compute nodes) and you can also specify a custom AMI for just the compute nodes. By default, ParallelCluster will use pre-built AMIs for the OS that you select. The exception is Rocky 8 and 9, for which ParallelCluster does not provide pre-built AMIs. To use Rocky Linux, you must first build a custom AMI and specify it in your config file at slurm/ParallelClusterConfig/Os/CustomAmi . The easiest way to create a new AMI is to start an EC2 instance with an existing ParallelCluster AMI, update it with your changes, and create a new AMI from that instance. You can find the official ParallelCluster AMIs using the ParallelCluster UI. Click on Images and the list of Official Images will be listed. After you create the a new AMI, you can then add it to your configuration file. ParallelCluster can also automate this process for you using EC2 ImageBuilder. When you build your cluster, example ParallelCluster build configuration files will be created for you and stored on the head node at: /opt/slurm/ ClusterName /config/build-files/parallelcluster- PCVersion -*.yml The build files with eda in the name build an image that installs the packages that are typically used by EDA tools. The build files can be modified for your needs. The build file format is docummented in the ParallelCluster User Guide . For example, you can add your own scripts to run during the AMI build process. The easiest way is to use the ParallelCluster UI to build the AMI using a build config file. Click on Images on the left Click on the Custom tab Click on Build Image Paste the contents of a config file into the window. Copy the image/name value into the Image Id field. It should begin with parallelcluster- Click Build Image The UI will create a cloudformation template that uses EC2 ImageBuilder. While it is being built it will show up as Pending in the UI. When the build is complete the AMI will show up either as Available or Failed . If it fails, the instance used to do the build will be left running. You can connect to it using SSM and lookin in /var/log/messages for error messages. When the build is successful, the stack will be deleted. There is currently a bug where the stack deletion will fail. This doesn't mean that the AMI build failed. Simply select the stack and delete it manually and it should successfully delete. FPGA Developer AMI The build file with fpga in the name is based on the FPGS Developer AMI. The FPGA Developer AMI has the Xilinx Vivado tools that can be used free of additional charges when run on AWS EC2 instances to develop FPGA images that can be run on AWS F1 instances. First subscribe to the FPGA developer AMI in the AWS Marketplace . There are 2 versions, one for CentOS 7 and the other for Amazon Linux 2 . Note : The FPGA Developer AMI hasn't been ported to the latest OS versions, so it will not show up in the build file templates. Deploy or update the Cluster After the AMI is built, add it to the config and create or update your cluster to use the AMI. You can set the AMI for the compute and head nodes using slurm/ParallelClusterConfig/Os/CustomAmi and for the compute nodes only using slurm/ParallelClusterConfig/ComputeNodeAmi . Note : You cannot update the OS of the cluster or the AMI of the head node. If they need to change then you will need to create a new cluster. The config file will look something like the following: slurm: ParallelClusterConfig: Image: Os: rocky8 CustomAmi: ami-abc123 # Rocky linux ComputeNodeAmi: ami-def456 # Rocky linux + EDA packages and EDA file systems","title":"Custom AMIs for ParallelCluster"},{"location":"custom-amis/#custom-amis-for-parallelcluster","text":"ParallelCluster supports building custom ParallelCluster AMIs for the head and compute nodes . You can specify a custom AMI for the entire cluster (head and compute nodes) and you can also specify a custom AMI for just the compute nodes. By default, ParallelCluster will use pre-built AMIs for the OS that you select. The exception is Rocky 8 and 9, for which ParallelCluster does not provide pre-built AMIs. To use Rocky Linux, you must first build a custom AMI and specify it in your config file at slurm/ParallelClusterConfig/Os/CustomAmi . The easiest way to create a new AMI is to start an EC2 instance with an existing ParallelCluster AMI, update it with your changes, and create a new AMI from that instance. You can find the official ParallelCluster AMIs using the ParallelCluster UI. Click on Images and the list of Official Images will be listed. After you create the a new AMI, you can then add it to your configuration file. ParallelCluster can also automate this process for you using EC2 ImageBuilder. When you build your cluster, example ParallelCluster build configuration files will be created for you and stored on the head node at: /opt/slurm/ ClusterName /config/build-files/parallelcluster- PCVersion -*.yml The build files with eda in the name build an image that installs the packages that are typically used by EDA tools. The build files can be modified for your needs. The build file format is docummented in the ParallelCluster User Guide . For example, you can add your own scripts to run during the AMI build process. The easiest way is to use the ParallelCluster UI to build the AMI using a build config file. Click on Images on the left Click on the Custom tab Click on Build Image Paste the contents of a config file into the window. Copy the image/name value into the Image Id field. It should begin with parallelcluster- Click Build Image The UI will create a cloudformation template that uses EC2 ImageBuilder. While it is being built it will show up as Pending in the UI. When the build is complete the AMI will show up either as Available or Failed . If it fails, the instance used to do the build will be left running. You can connect to it using SSM and lookin in /var/log/messages for error messages. When the build is successful, the stack will be deleted. There is currently a bug where the stack deletion will fail. This doesn't mean that the AMI build failed. Simply select the stack and delete it manually and it should successfully delete.","title":"Custom AMIs for ParallelCluster"},{"location":"custom-amis/#fpga-developer-ami","text":"The build file with fpga in the name is based on the FPGS Developer AMI. The FPGA Developer AMI has the Xilinx Vivado tools that can be used free of additional charges when run on AWS EC2 instances to develop FPGA images that can be run on AWS F1 instances. First subscribe to the FPGA developer AMI in the AWS Marketplace . There are 2 versions, one for CentOS 7 and the other for Amazon Linux 2 . Note : The FPGA Developer AMI hasn't been ported to the latest OS versions, so it will not show up in the build file templates.","title":"FPGA Developer AMI"},{"location":"custom-amis/#deploy-or-update-the-cluster","text":"After the AMI is built, add it to the config and create or update your cluster to use the AMI. You can set the AMI for the compute and head nodes using slurm/ParallelClusterConfig/Os/CustomAmi and for the compute nodes only using slurm/ParallelClusterConfig/ComputeNodeAmi . Note : You cannot update the OS of the cluster or the AMI of the head node. If they need to change then you will need to create a new cluster. The config file will look something like the following: slurm: ParallelClusterConfig: Image: Os: rocky8 CustomAmi: ami-abc123 # Rocky linux ComputeNodeAmi: ami-def456 # Rocky linux + EDA packages and EDA file systems","title":"Deploy or update the Cluster"},{"location":"debug/","text":"Debug For ParallelCluster and Slurm issues, refer to the official AWS ParallelCluster Troubleshooting documentation . Slurm Head Node If slurm commands hang, then it's likely a problem with the Slurm controller. Connect to the head node from the EC2 console using SSM Manager or ssh and switch to the root user. sudo su The first thing to do is to ensure that the Slurm controller daemon is running: systemctl status slurmctld If it isn't then first check for errors in the user data script. The following command will show the output: grep cloud-init /var/log/messages | less Then check the controller's logfile. /var/log/slurmctld.log The following command will rerun the user data. /var/lib/cloud/instance/scripts/part-001 Another way to debug the slurmctld daemon is to launch it interactively with debug set high. The first thing to do is get the path to the slurmctld binary. slurmctld=$(cat /etc/systemd/system/slurmctld.service | awk -F '=' '/ExecStart/ {print $2}') Then you can run slurmctld: $slurmctld -D -vvvvv Compute Nodes If there are problems with the compute nodes, connect to them using SSM Manager. Check for cloud-init errors the same way as for the slurmctl instance. The compute nodes do not run ansible; their AMIs are configured using ansible. Also check the slurmd.log . Check that the slurm daemon is running. systemctl status slurmd Log Files Logfile Description /var/log/slurmd.log slurmctld logfile Job Stuck in Pending State You can use scontrol to get detailed information about a job. scontrol show job *jobid* Job Stuck in Completing State When a node starts it reports it's number of cores and free memory to the controller. If the memory is less than in slurm_node.conf then the controller will mark the node as invalid. You can confirm this by searching for the node in /var/log/slurm/slurmctld.log on the controller. If this happens, fix the memory in slurm_nodes.conf and restart slurmctld. systemctl restart slurmctld Then reboot the node. Another cause of this is a hung process on the compute node. To clear this out, connect to the slurm controller and mark the node down, resume, and then idle. scontrol update node NODENAME state=DOWN reason=hung scontrol update node NODENAME state=RESUME scontrol update node NODENAME state=IDLE","title":"Debug"},{"location":"debug/#debug","text":"For ParallelCluster and Slurm issues, refer to the official AWS ParallelCluster Troubleshooting documentation .","title":"Debug"},{"location":"debug/#slurm-head-node","text":"If slurm commands hang, then it's likely a problem with the Slurm controller. Connect to the head node from the EC2 console using SSM Manager or ssh and switch to the root user. sudo su The first thing to do is to ensure that the Slurm controller daemon is running: systemctl status slurmctld If it isn't then first check for errors in the user data script. The following command will show the output: grep cloud-init /var/log/messages | less Then check the controller's logfile. /var/log/slurmctld.log The following command will rerun the user data. /var/lib/cloud/instance/scripts/part-001 Another way to debug the slurmctld daemon is to launch it interactively with debug set high. The first thing to do is get the path to the slurmctld binary. slurmctld=$(cat /etc/systemd/system/slurmctld.service | awk -F '=' '/ExecStart/ {print $2}') Then you can run slurmctld: $slurmctld -D -vvvvv","title":"Slurm Head Node"},{"location":"debug/#compute-nodes","text":"If there are problems with the compute nodes, connect to them using SSM Manager. Check for cloud-init errors the same way as for the slurmctl instance. The compute nodes do not run ansible; their AMIs are configured using ansible. Also check the slurmd.log . Check that the slurm daemon is running. systemctl status slurmd","title":"Compute Nodes"},{"location":"debug/#log-files","text":"Logfile Description /var/log/slurmd.log slurmctld logfile","title":"Log Files"},{"location":"debug/#job-stuck-in-pending-state","text":"You can use scontrol to get detailed information about a job. scontrol show job *jobid*","title":"Job Stuck in Pending State"},{"location":"debug/#job-stuck-in-completing-state","text":"When a node starts it reports it's number of cores and free memory to the controller. If the memory is less than in slurm_node.conf then the controller will mark the node as invalid. You can confirm this by searching for the node in /var/log/slurm/slurmctld.log on the controller. If this happens, fix the memory in slurm_nodes.conf and restart slurmctld. systemctl restart slurmctld Then reboot the node. Another cause of this is a hung process on the compute node. To clear this out, connect to the slurm controller and mark the node down, resume, and then idle. scontrol update node NODENAME state=DOWN reason=hung scontrol update node NODENAME state=RESUME scontrol update node NODENAME state=IDLE","title":"Job Stuck in Completing State"},{"location":"delete-cluster/","text":"Delete Cluster Before deleting the cluster, you should stop the cluster and make sure that no instances are connected to the clusters head node. For example, you should deconfigure external login nodes and instances that are creating and updating the users_groups.json file. If you specified RESEnvironmentName then it will also deconfigure the creation of users_groups.json and also deconfigure the VDI instances so they are no longer using the cluster. If you configured DomanJoinedInstance then the creation of users_groups.json will be automatically deconfigured. If you configured ExternalLoginNodes then they will automatically deconfigured. If you manually did this configuration, then you should manually deconfigure them also before deleting the cluster. Otherwise, the NFS mounts of the head node will hang and file system related commands on the instance may hang. The commands to manually deconfigure can be found in the outputs of the configuration stack. Output Description command10CreateUsersGroupsJsonDeconfigure Deconfigure the creation of users_groups.json command11ExternalLoginNodeDeconfigure Deconfigure external login node To delete the cluster all you need to do is delete the configuration CloudFormation stack. This will delete the ParallelCluster cluster stack and all of the configuration resources. You should not manually delete the ParallelCluster stack. If you do, the deconfiguration of login nodes and such may fail. If you deployed the Slurm database stack then you can keep that and use it for other clusters. If you don't need it anymore, then you can delete the stack. You will also need to manually delete the RDS database. If you deployed the ParallelCluster UI then you can keep it and use it with other clusters. If you don't need it anymore then you can delete the stack.","title":"Delete Cluster"},{"location":"delete-cluster/#delete-cluster","text":"Before deleting the cluster, you should stop the cluster and make sure that no instances are connected to the clusters head node. For example, you should deconfigure external login nodes and instances that are creating and updating the users_groups.json file. If you specified RESEnvironmentName then it will also deconfigure the creation of users_groups.json and also deconfigure the VDI instances so they are no longer using the cluster. If you configured DomanJoinedInstance then the creation of users_groups.json will be automatically deconfigured. If you configured ExternalLoginNodes then they will automatically deconfigured. If you manually did this configuration, then you should manually deconfigure them also before deleting the cluster. Otherwise, the NFS mounts of the head node will hang and file system related commands on the instance may hang. The commands to manually deconfigure can be found in the outputs of the configuration stack. Output Description command10CreateUsersGroupsJsonDeconfigure Deconfigure the creation of users_groups.json command11ExternalLoginNodeDeconfigure Deconfigure external login node To delete the cluster all you need to do is delete the configuration CloudFormation stack. This will delete the ParallelCluster cluster stack and all of the configuration resources. You should not manually delete the ParallelCluster stack. If you do, the deconfiguration of login nodes and such may fail. If you deployed the Slurm database stack then you can keep that and use it for other clusters. If you don't need it anymore, then you can delete the stack. You will also need to manually delete the RDS database. If you deployed the ParallelCluster UI then you can keep it and use it with other clusters. If you don't need it anymore then you can delete the stack.","title":"Delete Cluster"},{"location":"deploy-parallel-cluster/","text":"Deploy AWS ParallelCluster A ParallelCluster configuration will be generated and used to create a ParallelCluster slurm cluster. The first supported ParallelCluster version is 3.6.0. Version 3.7.0 is the recommended minimum version because it supports compute node weighting that is proportional to instance type cost so that the least expensive instance types that meet job requirements are used. The current latest version is 3.9.1. Prerequisites See Deployment Prerequisites page. Create the Cluster To install the cluster run the install script. You can override some parameters in the config file with command line arguments, however it is better to specify all of the parameters in the config file. ./install.sh --config-file <config-file> --cdk-cmd create This will create the ParallelCuster configuration file, store it in S3, and then use a lambda function to create the cluster. If you look in CloudFormation you will see 2 new stacks when deployment is finished. The first is the configuration stack and the second is the cluster. Create users_groups.json NOTE : If you are using RES and specify RESEnvironmentName in your configuration, these steps will automatically be done for you. Before you can use the cluster you must configure the Linux users and groups for the head and compute nodes. One way to do that would be to join the cluster to your domain. But joining each compute node to a domain effectively creates a distributed denial of service (DDOS) attack on the demain controller when the cluster rapidly scales out or in and each node tries to join or leave the domain. This can lead to domain controller timeouts and widespread havoc in your environment. To solve this problem a script runs on a server that is joined to the domain which writes a JSON file with all of the non-privileged users and groups and their respective uids and gids. A script and cron job on the head and compute nodes reads this json file to create local users and groups that match the domain-joined servers. Select the server that you want to use to create and update the JSON file. The outputs of the configuration stack have the commands required. Config Stack Output Description Command01_MountHeadNodeNfs Mounts the Slurm cluster's shared file system at /opt/slurm/{{ClusterName}}. This provides access to the configuration script used in the next step. Command02_CreateUsersGroupsJsonConfigure Create /opt/slurm/{{ClusterName}}/config/users_groups.json and create a cron job to refresh it hourly. Update /etc/fstab with the mount in the previous step. Before deleting the cluster you can undo the configuration by running the commands in the following outputs. Config Stack Output Description command10_CreateUsersGroupsJsonDeconfigure Removes the crontab that refreshes users_groups.json. Now the cluster is ready to be used by sshing into the head node or a login node, if you configured one. If you configured extra file systems for the cluster that contain the users' home directories, then they should be able to ssh in with their own ssh keys. Configure submission hosts to use the cluster NOTE : If you are using RES and specify RESEnvironmentName in your configuration, these steps will automatically be done for you on all running DCV desktops. ParallelCluster was built assuming that users would ssh into the head node or login nodes to execute Slurm commands. This can be undesirable for a number of reasons. First, users shouldn't be given ssh access to a critical infrastructure like the cluster head node. With ParallelCluster 3.7.0 you can configure login nodes, but if you have already provisioned desktop nodes then it's wasteful to have to provision login nodes. Second, it's just inconvenient to have to use ssh to access the cluster and use it. Fortunately, you can configure any server as a submission host so that users can run slurm commands. These commands must be run by an administrator that has root access to the submission host. The commands could also be run to create a custom AMI for user desktops so that they can access the clusters. The commands to configure submission hosts are in the outputs of the configuration CloudFormation stack. Run them in the following order: Config Stack Output Description Command01_MountHeadNodeNfs Mounts the Slurm cluster's shared file system at /opt/slurm/{{ClusterName}}. This provides access to the configuration script used in the next step. Command03_SubmitterConfigure Configure the submission host so it can directly access the Slurm cluster. Update /etc/fstab with the mount in the previous step. The first command simply mounts the head node's NFS file system so you have access to the Slurm commands and configuration. The second command runs an ansible playbook that configures the submission host so that it can run the Slurm commands for the cluster. It will also compile the Slurm binaries for the OS distribution and CPU architecture of your host. It also configures the modulefile that sets up the environment to use the slurm cluster. NOTE : When the new modulefile is created, you need to refresh your shell environment before the modulefile can be used. You can do this by opening a new shell or by sourcing your .profile: source ~/.profile . The clusters have been configured so that a submission host can use more than one cluster by simply changing the modulefile that is loaded. On the submission host just open a new shell and load the modulefile for your cluster and you can access Slurm. Customize the compute node AMI The easiest way to create a custom AMI is to find the default ParallelCluster AMI in the UI. Create an instance using the AMI and make whatever customizations you require such as installing packages and configuring users and groups. Custom file system mounts can be configured in the aws-eda-slurm-cluster config file which will add it to the ParallelCluster config file so that ParallelCluster can manage them for you. When you are done create a new AMI and wait for the AMI to become available. After it is available you can add the custom ami to the aws-eda-slurm-cluster config file. slurm: ParallelClusterConfig: ComputeNodeAmi: ami-0fdb972bda05d2932 Then update your aws-eda-slurm-cluster stack by running the install script again. Run Your First Job Run the following command in a shell to configure your environment to use your slurm cluster. NOTE : When the new modulefile is created, you need to refresh your shell environment before the modulefile can be used. You can do this by opening a new shell or by sourcing your profile: source ~/.bash_profile . module load {{ClusterName}} If you want to get a list of all of the clusters that are available execute the following command. module avail To submit a job run the following command. sbatch /opt/slurm/$SLURM_CLUSTER_NAME/test/job_simple_array.sh To check the status run the following command. squeue To open an interactive shell on a slurm node. srun --pty /bin/bash Slurm Documentation https://slurm.schedmd.com","title":"Deploy AWS ParallelCluster"},{"location":"deploy-parallel-cluster/#deploy-aws-parallelcluster","text":"A ParallelCluster configuration will be generated and used to create a ParallelCluster slurm cluster. The first supported ParallelCluster version is 3.6.0. Version 3.7.0 is the recommended minimum version because it supports compute node weighting that is proportional to instance type cost so that the least expensive instance types that meet job requirements are used. The current latest version is 3.9.1.","title":"Deploy AWS ParallelCluster"},{"location":"deploy-parallel-cluster/#prerequisites","text":"See Deployment Prerequisites page.","title":"Prerequisites"},{"location":"deploy-parallel-cluster/#create-the-cluster","text":"To install the cluster run the install script. You can override some parameters in the config file with command line arguments, however it is better to specify all of the parameters in the config file. ./install.sh --config-file <config-file> --cdk-cmd create This will create the ParallelCuster configuration file, store it in S3, and then use a lambda function to create the cluster. If you look in CloudFormation you will see 2 new stacks when deployment is finished. The first is the configuration stack and the second is the cluster.","title":"Create the Cluster"},{"location":"deploy-parallel-cluster/#create-users_groupsjson","text":"NOTE : If you are using RES and specify RESEnvironmentName in your configuration, these steps will automatically be done for you. Before you can use the cluster you must configure the Linux users and groups for the head and compute nodes. One way to do that would be to join the cluster to your domain. But joining each compute node to a domain effectively creates a distributed denial of service (DDOS) attack on the demain controller when the cluster rapidly scales out or in and each node tries to join or leave the domain. This can lead to domain controller timeouts and widespread havoc in your environment. To solve this problem a script runs on a server that is joined to the domain which writes a JSON file with all of the non-privileged users and groups and their respective uids and gids. A script and cron job on the head and compute nodes reads this json file to create local users and groups that match the domain-joined servers. Select the server that you want to use to create and update the JSON file. The outputs of the configuration stack have the commands required. Config Stack Output Description Command01_MountHeadNodeNfs Mounts the Slurm cluster's shared file system at /opt/slurm/{{ClusterName}}. This provides access to the configuration script used in the next step. Command02_CreateUsersGroupsJsonConfigure Create /opt/slurm/{{ClusterName}}/config/users_groups.json and create a cron job to refresh it hourly. Update /etc/fstab with the mount in the previous step. Before deleting the cluster you can undo the configuration by running the commands in the following outputs. Config Stack Output Description command10_CreateUsersGroupsJsonDeconfigure Removes the crontab that refreshes users_groups.json. Now the cluster is ready to be used by sshing into the head node or a login node, if you configured one. If you configured extra file systems for the cluster that contain the users' home directories, then they should be able to ssh in with their own ssh keys.","title":"Create users_groups.json"},{"location":"deploy-parallel-cluster/#configure-submission-hosts-to-use-the-cluster","text":"NOTE : If you are using RES and specify RESEnvironmentName in your configuration, these steps will automatically be done for you on all running DCV desktops. ParallelCluster was built assuming that users would ssh into the head node or login nodes to execute Slurm commands. This can be undesirable for a number of reasons. First, users shouldn't be given ssh access to a critical infrastructure like the cluster head node. With ParallelCluster 3.7.0 you can configure login nodes, but if you have already provisioned desktop nodes then it's wasteful to have to provision login nodes. Second, it's just inconvenient to have to use ssh to access the cluster and use it. Fortunately, you can configure any server as a submission host so that users can run slurm commands. These commands must be run by an administrator that has root access to the submission host. The commands could also be run to create a custom AMI for user desktops so that they can access the clusters. The commands to configure submission hosts are in the outputs of the configuration CloudFormation stack. Run them in the following order: Config Stack Output Description Command01_MountHeadNodeNfs Mounts the Slurm cluster's shared file system at /opt/slurm/{{ClusterName}}. This provides access to the configuration script used in the next step. Command03_SubmitterConfigure Configure the submission host so it can directly access the Slurm cluster. Update /etc/fstab with the mount in the previous step. The first command simply mounts the head node's NFS file system so you have access to the Slurm commands and configuration. The second command runs an ansible playbook that configures the submission host so that it can run the Slurm commands for the cluster. It will also compile the Slurm binaries for the OS distribution and CPU architecture of your host. It also configures the modulefile that sets up the environment to use the slurm cluster. NOTE : When the new modulefile is created, you need to refresh your shell environment before the modulefile can be used. You can do this by opening a new shell or by sourcing your .profile: source ~/.profile . The clusters have been configured so that a submission host can use more than one cluster by simply changing the modulefile that is loaded. On the submission host just open a new shell and load the modulefile for your cluster and you can access Slurm.","title":"Configure submission hosts to use the cluster"},{"location":"deploy-parallel-cluster/#customize-the-compute-node-ami","text":"The easiest way to create a custom AMI is to find the default ParallelCluster AMI in the UI. Create an instance using the AMI and make whatever customizations you require such as installing packages and configuring users and groups. Custom file system mounts can be configured in the aws-eda-slurm-cluster config file which will add it to the ParallelCluster config file so that ParallelCluster can manage them for you. When you are done create a new AMI and wait for the AMI to become available. After it is available you can add the custom ami to the aws-eda-slurm-cluster config file. slurm: ParallelClusterConfig: ComputeNodeAmi: ami-0fdb972bda05d2932 Then update your aws-eda-slurm-cluster stack by running the install script again.","title":"Customize the compute node AMI"},{"location":"deploy-parallel-cluster/#run-your-first-job","text":"Run the following command in a shell to configure your environment to use your slurm cluster. NOTE : When the new modulefile is created, you need to refresh your shell environment before the modulefile can be used. You can do this by opening a new shell or by sourcing your profile: source ~/.bash_profile . module load {{ClusterName}} If you want to get a list of all of the clusters that are available execute the following command. module avail To submit a job run the following command. sbatch /opt/slurm/$SLURM_CLUSTER_NAME/test/job_simple_array.sh To check the status run the following command. squeue To open an interactive shell on a slurm node. srun --pty /bin/bash","title":"Run Your First Job"},{"location":"deploy-parallel-cluster/#slurm-documentation","text":"https://slurm.schedmd.com","title":"Slurm Documentation"},{"location":"deployment-prerequisites/","text":"Deployment Prerequisites This page shows common prerequisites that need to be done before deployment. Deployment Server/Instance Requirements The deployment process was developed and tested using Amazon Linux 2. It has also been tested on RHEL 8 and RHEL 9. An easy way to create a deployment instance is to use AWS CloudShell. This will give you a code editor IDE and shell environment that you can use to deploy the cluster. If the required packages aren't installed then you will need sudo or root access on the instance. Configure AWS CLI Credentials You will needs AWS credentials that provide admin access to deploy the cluster. Clone or Download the Repository Clone or download the aws-eda-slurm-cluster repository to your system. git clone https://github.com/aws-samples/aws-eda-slurm-cluster.git Create SNS Topic for Error Notifications (Optional but recommended) The Slurm cluster allows you to specify an SNS notification that will be notified when an error is detected. You can provide the ARN for the topic in the config file or on the command line. You can use the SNS notification in various ways. The simplest is to subscribe your email address to the topic so that you get an email when there is an error. You could also use it to trigger a CloudWatch alarm that could be used to trigger a lambda to do automatic remediation or create a support ticket. Make sure using at least python version 3.7 This application requires at least python version 3.7. Many distributions use older versions of python by default such as python 3.6.8 in RHEL 8 and Rocky Linux 8. Newer versions are available, but can't be made the system default without breaking OS tools such as yum. The easiest way to get around this is to create a python virtual environment using a newer version of python. Simply install the newer version and then use it to create and activate a virtual environment. $ python3 --version Python 3.6.8 $ yum -y install python3.11 $ python3.11 -m venv ~/.venv-python3.11 $ source ~/.venv-python3.11/bin/activate $ python3 --version Python 3.11.5 Make sure required packages are installed cd aws-eda-slurm-cluster source setup.sh The setup script assumes that you have sudo access so that you can install or update packages. If you do not, then contact an administrator to help you do the updates. If necessary modify the setup script for your environment. Install Cloud Development Kit (CDK) (Optional) The setup script will attempt to install all of the prerequisites for you. If the install script fails on your system then you can refer to this section for instructions on how to install or update CDK. This cluster uses Cloud Development Kit (CDK) and Python 3 to deploy the cluster. Install the packages used by the installer. sudo yum -y install curl gcc-c++ make nfs-utils python3 tcl unzip wget The following link documents how to setup for CDK. Follow the instructions for Python. https://docs.aws.amazon.com/cdk/v2/guide/getting_started.html#getting_started_prerequisites Note that CDK requires a pretty new version of nodejs which you may have to download from, for example, https://nodejs.org/dist/v16.13.1/node-v16.13.1-linux-x64.tar.xz sudo yum -y install wget wget https://nodejs.org/dist/v16.13.1/node-v16.13.1-linux-x64.tar.xz tar -xf node-v16.13.1-linux-x64.tar.xz ~ Add the nodjs bin directory to your path. https://docs.aws.amazon.com/cdk/v2/guide/getting_started.html#getting_started_install Note that the version of aws-cdk changes frequently. The version that has been tested is in the CDK_VERSION variable in the install script. The install script will try to install the prerequisites if they aren't already installed. Create ParallelCluster UI (optional but recommended) It is highly recommended to create a ParallelCluster UI to manage your ParallelCluster clusters. A different UI is required for each version of ParallelCluster that you are using. The versions are list in the ParallelCluster Release Notes . The minimum required version is 3.6.0 which adds support for RHEL 8 and increases the number of allows queues and compute resources. The suggested version is at least 3.7.0 because it adds configurable compute node weights which we use to prioritize the selection of compute nodes by their cost. The instructions are in the ParallelCluster User Guide . Create Munge Key Munge is a package that Slurm uses to secure communication between servers. The munge service uses a preshared key that must be the same on all of the servers in the Slurm cluster. If you want to be able to use multiple clusters from your submission hosts, such as virtual desktops, then all of the clusters must be using the same munge key. This is done by creating a munge key and storing it in secrets manager. The secret is then passed as a parameter to ParallelCluster so that it can use it when configuring munge on all of the cluster instances. To create the munge key and store it in AWS Secrets Manager, run the following commands. aws secretsmanager create-secret --name SlurmMungeKey --secret-string \"$(dd if=/dev/random bs=1024 count=1 | base64 -w 0)\" Save the ARN of the secret for when you create the Slurmdbd instance and for when you create the configuration file. See the Slurm documentation for authentication for more information. See the ParallelCluster documentation for MungeKeySecretArn . See the MungeKeySecret configuration parameter . Create ParallelCluster Slurm Database The Slurm Database is required for configuring Slurm accounts, users, groups, and fair share scheduling. It you need these and other features then you will need to create a ParallelCluster Slurm Database. You do not need to create a new database for each cluster; multiple clusters can share the same database. Follow the directions in this ParallelCluster tutorial to configure slurm accounting . Create Slurmdbd Instance Note : Before ParallelCluster 3.10.0, the slurmdbd daemon that connects to the data was created on each cluster's head node. The recommended Slurm architecture is to have a shared slurmdbd daemon that is used by all of the clusters. Starting in version 3.10.0, ParallelCluster supports specifying an external slurmdbd instance when you create a cluster and provide a cloud formation template to create it. Follow the directions in this ParallelCluster tutorial to configure slurmdbd . This requires that you have already created the slurm database. Here are some notes on the required parameters and how to fill them out. Parameter Description AmiId You can get this using the ParallelCluster UI. Click on Images and sort on Operating system. Confirm that the version is at least 3.10.0. Select the AMI for alinux2023 and the arm64 architecture. CustomCookbookUrl Leave blank DBMSClientSG Get this from the DatabaseClientSecurityGroup output of the database stack. DBMSDatabaseName This is an arbitrary name. It must be alphanumeric. I use slurmaccounting DBMSPasswordSecretArn Get this from the DatabaseSecretArn output of the database stack DBMSUri Get this from the DatabaseHost output of the database stack. Note that if you copy and paste the link you should delete the https:// prefix and the trailing '/'. DBMSUsername Get this from the DatabaseAdminUser output of the database stack. EnableSlurmdbdSystemService Set to true. Note the warning. If the database already exists and was created with an older version of slurm then the database will be upgraded. This may break clusters using an older slurm version that are still using the cluster. Set to false if you don't want this to happen. InstanceType Choose an instance type that is compatible with the AMI. For example, m7g.large. KeyName Use an existing EC2 key pair. MungeKeySecretArn ARN of an existing munge key secret. See Create Munge Key . PrivateIp Choose an available IP in the subnet. PrivatePrefix CIDR of the instance's subnet. SlurmdbdPort 6819 SubnetId Preferably the same subnet where the clusters will be deployed. VPCId The VPC of the subnet. The stack name will be used in two places. It will be used by the script that creates security groups for you in the following section. It will also be used in the slurm/ParallelClusterConfig/ SlurmdbdStackName configuration parameter when you create your cluster. The stack will only take about 3 minutes to deploy. Shared Security Groups for login nodes and file systems Instances like remote desktops that access the cluster directly are called login nodes. If you want to use your own login nodes to access the cluster, then you must define several security groups that allow connections between the login nodes, the Slurm head node, and the Slurm compute nodes. If you are using shared file servers like FSx file systems, then you also need to configure security groups for the file systems that allows the login and slurm nodes to access the file systems. The details are straightforward, but time consuming, so the process has been automated for you. Simply run the following script which will deploy a CloudFormation stack that creates the required security groups. cd aws-eda-slurm-cluster ./create-slurm-security-groups.sh --region <REGION> --stack-name <STACK_NAME> --VpcId <VPC_ID> Additional script options can be specified if you created an external SlurmDbd instance or have existing security groups for your FSx file systems. Script Option Description --slurmdbd-stack-name Stack name that deployed external slurmdbd instance. --slurmdbd-security-group-id Id of security group attached to the slurmdbd instance. --fsxl-security-group-id Id of security group attached to FSx for Lustre file systems --fsxo-security-group-id Id of security group attached to FSx for NetApp Ontap file systems --fsxz-security-group-id Id of security group attached to FSx for OpenZfs file systems The stack outputs will have the security group ids. Output Name Use SlurmHeadNodeSGId Additional security group for Slurm head node SlurmComputeNodeSGId Additional security group for Slurm compute nodes SlurmSubmitterSGId Additional security group for Slurm login nodes SlurmLustreSGId Security group for FSx for Lustre file systems SlurmOntapSGId Security group for FSx for NetApp Ontap file systems SlurmZfsSGId Security group for FSx for OpenZfs file systems You can pass the name of the stack to the AdditionalSecurityGroupsStackName configuration parameter when you create your cluster and it will get the security groups ids for you and configure the cluster to use them. Create File Systems Most EDA workloads require a high performance shared file system. AWS provides managed file systems that meet the needs of EDA workloads. FSx for NetApp ONTAP, FSx for OpenZfs, and FSx for Lustre are managed file systems that meet the needs of EDA workloads. Create the file systems that you require and use the appropriate security group from the previous section when you create the file system. If the file system already exists, then attach the appropriate security group to the network interfaces of the file systems. Create Exostellar Management Server If you're going to use Exostellar Infrastructure Optimizer (XIO) then you will need to deploy the Exostellar management server. See the XIO page for details. Create Configuration File Before you deploy a cluster you need to create a configuration file. A default configuration file is found in source/resources/config/default_config.yml . You should create a new config file and update the parameters for your cluster. Ideally you should version control this file so you can keep track of changes. The schema for the config file along with its default values can be found in source/cdk/config_schema.py . The schema is defined in python, but the actual config file should be in yaml format. See Configuration File Format for documentation on all of the parameters. The following are key parameters that you will need to update. If you do not have the required parameters in your config file then the installer script will fail unless you specify the --prompt option. You should save your selections in the config file. Parameter Description Valid Values Default StackName The cloudformation stack that will deploy the cluster. I prefer to end the name with \"-config\" . None slurm/ClusterName Name of the Slurm cluster Can't be the same as StackName. If StackName ends in \"-config\" then StackName with \"-config\" stripped off. Otherwise, StackName with \"-cl\" appended. Region Region where VPC is located $AWS_DEFAULT_REGION VpcId The vpc where the cluster will be deployed. vpc-* None SshKeyPair EC2 Keypair to use for instances None ErrorSnsTopicArn ARN of an SNS topic that will be notified of errors arn:aws:sns:{{region}}:{AccountId}:{TopicName} None slurm/InstanceConfig Configure instance types that the cluster can use and number of nodes. See default_config.yml AdditionalSecurityGroupsStackName Name of stack that created security groups for external login nodes and file systems. RESStackName Name of RES environment slurm/storage/ExtraMounts Extra mount points None Configure Slurm Accounting database (slurmdbd) If you created a ParallelCluster Slurm Database and a Slurmdbd Instance then add the following configuration parameters. slurm: ParallelClusterConfig: Slurmdbd: SlurmdbdStackName: <Slurmdbd-Stack-Name> Configure Linux Users and Groups The cluster defines a script that can capture the users and groups from your identity provider (IDP) into a json file. When a new compute node starts, another script creates local Linux users and groups from the json file. The first script gets installed at: /opt/slurm/{{ cluster_name }}/config/bin/create_users_and_groups_json.py This script should be run on an instance that is joined to your IDP. It first tries to use wbinfo -u and if that fails it uses getent passwd to get the list of users and their userids. It uses id to get the uid and gids for the users. The json file gets stored at /opt/slurm/{{ cluster_name }}/config/users_groups.json The compute node calls: /opt/slurm/{{ cluster_name }}/config/bin/create_users_groups.py -i /opt/slurm/{{ cluster_name }}/config/users_groups.json The script calls useradd and groupadd to create local users and groups. To enable this mechanism you must configure the EC2 tags of the domain joined instance that will be used to create the json file. A Lambda function will create the json file and create a lambda that will refresh it hourly. You will also need to provide the security group id of the SlurmExternalLoginNodeSG which will be added to the instance so that it can mount the head node's NFS file system. DomainJoinedInstance: Tags: - Key: Name Value: ClusterManager SecurityGroupId: sg-xxxxxxxx You can provide 1 or more keys and the set will be done on the first instance that matches. Note: You do not have to use this mechanism. ParallelCluster supports using Microsoft Active Directory and you can configure that using the slurm/ParallelClusterConfig/ClusterConfig/DirectoryService parameter . You can also use custom action scripts that run on your compute nodes that configure domains or users and groups to meet your needs. Note: This is automatically configured for you if you specify the RESStackName parameter. Configure the Compute Instances The slurm/InstanceConfig configuration parameter configures the base operating systems, CPU architectures, instance families, and instance types that the Slurm cluster should support. ParallelCluster currently doesn't support heterogeneous clusters; all nodes must have the same architecture and Base OS. Base OS CPU Architectures Amazon Linux 2 x86_64, arm64 CentOS 7 x86_64 RedHat 7 x86_64 RedHat 8 x86_64, arm64 RedHat 9 x86_64, arm64 Rocky 8 x86_64, arm64 Rocky 9 x86_64, arm64 You can exclude instances types by family or specific instance type. By default the InstanceConfig excludes older generation instance families. You can include instances by family or specific instance type. If no includes are specified then all non-excluded instance types will be used. You can also choose to only include the largest instance size within a family. The advantage of using the max instance size is that jobs running on the instance have the highest network bandwidth for that family and fewer instances are required to run the same number of jobs. This may help jobs run faster and allow jobs to wait less time for a new instance to start. The disadvantage is higher cost if the instance is lightly loaded. The default InstanceConfig includes all supported base OSes and architectures and burstable and general purpose instance types. default instance families default instance types default excluded instance families default excluded instance types Note that instance types and families are python regular expressions. slurm: InstanceConfig: Include: InstanceFamilies: - t3.* - m6a.* InstanceTypes: - r6a.large The following InstanceConfig configures instance types recommended for EDA workloads running on CentOS. slurm: InstanceConfig: Include: InstanceFamilies: - c5.* - c6g.* - f1 - m5.* - m6g.* - r5.* - r6g.* - x2gd - z1d If you have reserved instances (RIs) or savings plans then you can configure instances so that they are always on since you are paying for them whether they are running or not. To do this add a MinCount greater than 0 for the compute resources that contain the instance types. slurm: InstanceConfig: NodeCounts: DefaultMinCount: 1 Configure Fair Share Scheduling (Optional) Slurm supports fair share scheduling , but it requires the fair share policy to be configured. By default, all users will be put into a default group that has a low fair share. The configuration file is at source/resources/playbooks/roles/ParallelClusterHeadNode/files/opt/slurm/config/accounts.yml.example in the repository and is deployed to /opt/slurm/{{ClusterName}}/conf/accounts.yml . The file is a simple yaml file that allows you to configure groups, the users that belong to the group, and a fair share weight for the group. Refer to the Slurm documentation for details on how the fair share weight is calculated. The scheduler can be configured so that users who aren't getting their fair share of resources get higher priority. The following shows 3 top level groups. Note that the fairshare weights aren't a percentage. They are just a relative weight. In this example, the projects have 9 times higher weight than the jenkins group. jenkins: fairshare: 10 users: - jenkins project1: fairshare: 90 project2: fairshare: 90 The allocation of top level groups can be further subdivided to control the relative priority of jobs within that group. For example, a project may have design verification (dv), rtl design (rtl), physical design (dv), and formal verification (fv) teams. The following example shows how the project's allocation can be prioritized for the different teams. If a group is using more than it's fair share then its jobs will have lower priority than jobs whose users aren't getting their fair share. project1-dv: parent: project1 fairshare: 80 users: - dvuser1 project1-pd: parent: project1 fairshare: 10 users: - pduser1 project1-rtl: parent: project1 fairshare: 10 users: - rtluser1 project1-fv: parent: project1 fairshare: 10 users: - fvuser1 The scheduler uses the priority/multifactor plugin to calculate job priorities. Fair share is just one of the factors. Read the Multifactor Priority Plugin documentation for the details. This is the default configuration in slurm.conf. The partition weight is set the highest so that jobs in the interactive partition always have the highest priority. Fairshare and QOS are the next highest weighted factors. The next factor is the job age, which means all else being equal the jobs run in FIFO order with the jobs that have been waiting the longest getting higher priority. PriorityType=priority/multifactor PriorityWeightPartition=100000 PriorityWeightFairshare=10000 PriorityWeightQOS=10000 PriorityWeightAge=1000 PriorityWeightAssoc=0 PriorityWeightJobSize=0 These weights can be adjusted based on your needs to control job priorities. Configure Licenses Slurm supports configuring licenses as a consumable resource . It will keep track of how many running jobs are using a license and when no more licenses are available then jobs will stay pending in the queue until a job completes and frees up a license. Combined with the fairshare algorithm, this can prevent users from monopolizing licenses and preventing others from being able to run their jobs. Licenses are configured using the slurm/Licenses configuration variable. If you are using the Slurm database then these will be configured in the database. Otherwise they will be configured in /opt/slurm/{{ClusterName}}/etc/pcluster/custom_slurm_settings_include_file_slurm.conf . The example configuration shows how the number of licenses can be configured. In this example, the cluster will manage 800 vcs licenses and 1 ansys license. Users must request a license using the -L or --licenses options. slurm: Licenses: vcs: Count: 800 ansys: Count: 1 Configure File Systems The Storage/ExtraMounts parameter allows you to configure additional file systems to mount on compute nodes. Note that the security groups for the file systems must allow connections from the compute nodes. Lustre The following example shows how to add an FSx for Lustre file system. The mount information can be found from the FSx console. storage: ExtraMounts - dest: /lustre src: <FileSystemId>.fsx.<Region>.amazonaws.com@tcp:/<MountName> StorageType: FsxLustre FileSystemId: <FileSystemId> type: lustre options: relatime,flock ONTAP The following example shows how to add an FSx for NetApp ONTAP file system. The mount information can be found from the FSx console. storage: ExtraMounts - dest: /ontap src: <SvmId>.<FileSystemId>.fsx.<Region>.amazonaws.com:/vol1 StorageType: FsxOntap FileSystemId: <FileSystemId> VolumeId: <VolumeId> type: nfs options: default ZFS The following example shows how to add an FSx for OpenZFS file system. The mount information can be found from the FSx console. storage: ExtraMounts - dest: /zfs src: <FileSystemId>.fsx.<Region>.amazonaws.com:/fsx StorageType: FsxOpenZfs FileSystemId: <FileSystemId> VolumeId: <VolumeId> type: nfs options: noatime,nfsvers=3,sync,nconnect=16,rsize=1048576,wsize=1048576","title":"Deployment Prerequisites"},{"location":"deployment-prerequisites/#deployment-prerequisites","text":"This page shows common prerequisites that need to be done before deployment.","title":"Deployment Prerequisites"},{"location":"deployment-prerequisites/#deployment-serverinstance-requirements","text":"The deployment process was developed and tested using Amazon Linux 2. It has also been tested on RHEL 8 and RHEL 9. An easy way to create a deployment instance is to use AWS CloudShell. This will give you a code editor IDE and shell environment that you can use to deploy the cluster. If the required packages aren't installed then you will need sudo or root access on the instance.","title":"Deployment Server/Instance Requirements"},{"location":"deployment-prerequisites/#configure-aws-cli-credentials","text":"You will needs AWS credentials that provide admin access to deploy the cluster.","title":"Configure AWS CLI Credentials"},{"location":"deployment-prerequisites/#clone-or-download-the-repository","text":"Clone or download the aws-eda-slurm-cluster repository to your system. git clone https://github.com/aws-samples/aws-eda-slurm-cluster.git","title":"Clone or Download the Repository"},{"location":"deployment-prerequisites/#create-sns-topic-for-error-notifications-optional-but-recommended","text":"The Slurm cluster allows you to specify an SNS notification that will be notified when an error is detected. You can provide the ARN for the topic in the config file or on the command line. You can use the SNS notification in various ways. The simplest is to subscribe your email address to the topic so that you get an email when there is an error. You could also use it to trigger a CloudWatch alarm that could be used to trigger a lambda to do automatic remediation or create a support ticket.","title":"Create SNS Topic for Error Notifications (Optional but recommended)"},{"location":"deployment-prerequisites/#make-sure-using-at-least-python-version-37","text":"This application requires at least python version 3.7. Many distributions use older versions of python by default such as python 3.6.8 in RHEL 8 and Rocky Linux 8. Newer versions are available, but can't be made the system default without breaking OS tools such as yum. The easiest way to get around this is to create a python virtual environment using a newer version of python. Simply install the newer version and then use it to create and activate a virtual environment. $ python3 --version Python 3.6.8 $ yum -y install python3.11 $ python3.11 -m venv ~/.venv-python3.11 $ source ~/.venv-python3.11/bin/activate $ python3 --version Python 3.11.5","title":"Make sure using at least python version 3.7"},{"location":"deployment-prerequisites/#make-sure-required-packages-are-installed","text":"cd aws-eda-slurm-cluster source setup.sh The setup script assumes that you have sudo access so that you can install or update packages. If you do not, then contact an administrator to help you do the updates. If necessary modify the setup script for your environment.","title":"Make sure required packages are installed"},{"location":"deployment-prerequisites/#install-cloud-development-kit-cdk-optional","text":"The setup script will attempt to install all of the prerequisites for you. If the install script fails on your system then you can refer to this section for instructions on how to install or update CDK. This cluster uses Cloud Development Kit (CDK) and Python 3 to deploy the cluster. Install the packages used by the installer. sudo yum -y install curl gcc-c++ make nfs-utils python3 tcl unzip wget The following link documents how to setup for CDK. Follow the instructions for Python. https://docs.aws.amazon.com/cdk/v2/guide/getting_started.html#getting_started_prerequisites Note that CDK requires a pretty new version of nodejs which you may have to download from, for example, https://nodejs.org/dist/v16.13.1/node-v16.13.1-linux-x64.tar.xz sudo yum -y install wget wget https://nodejs.org/dist/v16.13.1/node-v16.13.1-linux-x64.tar.xz tar -xf node-v16.13.1-linux-x64.tar.xz ~ Add the nodjs bin directory to your path. https://docs.aws.amazon.com/cdk/v2/guide/getting_started.html#getting_started_install Note that the version of aws-cdk changes frequently. The version that has been tested is in the CDK_VERSION variable in the install script. The install script will try to install the prerequisites if they aren't already installed.","title":"Install Cloud Development Kit (CDK) (Optional)"},{"location":"deployment-prerequisites/#create-parallelcluster-ui-optional-but-recommended","text":"It is highly recommended to create a ParallelCluster UI to manage your ParallelCluster clusters. A different UI is required for each version of ParallelCluster that you are using. The versions are list in the ParallelCluster Release Notes . The minimum required version is 3.6.0 which adds support for RHEL 8 and increases the number of allows queues and compute resources. The suggested version is at least 3.7.0 because it adds configurable compute node weights which we use to prioritize the selection of compute nodes by their cost. The instructions are in the ParallelCluster User Guide .","title":"Create ParallelCluster UI (optional but recommended)"},{"location":"deployment-prerequisites/#create-munge-key","text":"Munge is a package that Slurm uses to secure communication between servers. The munge service uses a preshared key that must be the same on all of the servers in the Slurm cluster. If you want to be able to use multiple clusters from your submission hosts, such as virtual desktops, then all of the clusters must be using the same munge key. This is done by creating a munge key and storing it in secrets manager. The secret is then passed as a parameter to ParallelCluster so that it can use it when configuring munge on all of the cluster instances. To create the munge key and store it in AWS Secrets Manager, run the following commands. aws secretsmanager create-secret --name SlurmMungeKey --secret-string \"$(dd if=/dev/random bs=1024 count=1 | base64 -w 0)\" Save the ARN of the secret for when you create the Slurmdbd instance and for when you create the configuration file. See the Slurm documentation for authentication for more information. See the ParallelCluster documentation for MungeKeySecretArn . See the MungeKeySecret configuration parameter .","title":"Create Munge Key"},{"location":"deployment-prerequisites/#create-parallelcluster-slurm-database","text":"The Slurm Database is required for configuring Slurm accounts, users, groups, and fair share scheduling. It you need these and other features then you will need to create a ParallelCluster Slurm Database. You do not need to create a new database for each cluster; multiple clusters can share the same database. Follow the directions in this ParallelCluster tutorial to configure slurm accounting .","title":"Create ParallelCluster Slurm Database"},{"location":"deployment-prerequisites/#create-slurmdbd-instance","text":"Note : Before ParallelCluster 3.10.0, the slurmdbd daemon that connects to the data was created on each cluster's head node. The recommended Slurm architecture is to have a shared slurmdbd daemon that is used by all of the clusters. Starting in version 3.10.0, ParallelCluster supports specifying an external slurmdbd instance when you create a cluster and provide a cloud formation template to create it. Follow the directions in this ParallelCluster tutorial to configure slurmdbd . This requires that you have already created the slurm database. Here are some notes on the required parameters and how to fill them out. Parameter Description AmiId You can get this using the ParallelCluster UI. Click on Images and sort on Operating system. Confirm that the version is at least 3.10.0. Select the AMI for alinux2023 and the arm64 architecture. CustomCookbookUrl Leave blank DBMSClientSG Get this from the DatabaseClientSecurityGroup output of the database stack. DBMSDatabaseName This is an arbitrary name. It must be alphanumeric. I use slurmaccounting DBMSPasswordSecretArn Get this from the DatabaseSecretArn output of the database stack DBMSUri Get this from the DatabaseHost output of the database stack. Note that if you copy and paste the link you should delete the https:// prefix and the trailing '/'. DBMSUsername Get this from the DatabaseAdminUser output of the database stack. EnableSlurmdbdSystemService Set to true. Note the warning. If the database already exists and was created with an older version of slurm then the database will be upgraded. This may break clusters using an older slurm version that are still using the cluster. Set to false if you don't want this to happen. InstanceType Choose an instance type that is compatible with the AMI. For example, m7g.large. KeyName Use an existing EC2 key pair. MungeKeySecretArn ARN of an existing munge key secret. See Create Munge Key . PrivateIp Choose an available IP in the subnet. PrivatePrefix CIDR of the instance's subnet. SlurmdbdPort 6819 SubnetId Preferably the same subnet where the clusters will be deployed. VPCId The VPC of the subnet. The stack name will be used in two places. It will be used by the script that creates security groups for you in the following section. It will also be used in the slurm/ParallelClusterConfig/ SlurmdbdStackName configuration parameter when you create your cluster. The stack will only take about 3 minutes to deploy.","title":"Create Slurmdbd Instance"},{"location":"deployment-prerequisites/#shared-security-groups-for-login-nodes-and-file-systems","text":"Instances like remote desktops that access the cluster directly are called login nodes. If you want to use your own login nodes to access the cluster, then you must define several security groups that allow connections between the login nodes, the Slurm head node, and the Slurm compute nodes. If you are using shared file servers like FSx file systems, then you also need to configure security groups for the file systems that allows the login and slurm nodes to access the file systems. The details are straightforward, but time consuming, so the process has been automated for you. Simply run the following script which will deploy a CloudFormation stack that creates the required security groups. cd aws-eda-slurm-cluster ./create-slurm-security-groups.sh --region <REGION> --stack-name <STACK_NAME> --VpcId <VPC_ID> Additional script options can be specified if you created an external SlurmDbd instance or have existing security groups for your FSx file systems. Script Option Description --slurmdbd-stack-name Stack name that deployed external slurmdbd instance. --slurmdbd-security-group-id Id of security group attached to the slurmdbd instance. --fsxl-security-group-id Id of security group attached to FSx for Lustre file systems --fsxo-security-group-id Id of security group attached to FSx for NetApp Ontap file systems --fsxz-security-group-id Id of security group attached to FSx for OpenZfs file systems The stack outputs will have the security group ids. Output Name Use SlurmHeadNodeSGId Additional security group for Slurm head node SlurmComputeNodeSGId Additional security group for Slurm compute nodes SlurmSubmitterSGId Additional security group for Slurm login nodes SlurmLustreSGId Security group for FSx for Lustre file systems SlurmOntapSGId Security group for FSx for NetApp Ontap file systems SlurmZfsSGId Security group for FSx for OpenZfs file systems You can pass the name of the stack to the AdditionalSecurityGroupsStackName configuration parameter when you create your cluster and it will get the security groups ids for you and configure the cluster to use them.","title":"Shared Security Groups for login nodes and file systems"},{"location":"deployment-prerequisites/#create-file-systems","text":"Most EDA workloads require a high performance shared file system. AWS provides managed file systems that meet the needs of EDA workloads. FSx for NetApp ONTAP, FSx for OpenZfs, and FSx for Lustre are managed file systems that meet the needs of EDA workloads. Create the file systems that you require and use the appropriate security group from the previous section when you create the file system. If the file system already exists, then attach the appropriate security group to the network interfaces of the file systems.","title":"Create File Systems"},{"location":"deployment-prerequisites/#create-exostellar-management-server","text":"If you're going to use Exostellar Infrastructure Optimizer (XIO) then you will need to deploy the Exostellar management server. See the XIO page for details.","title":"Create Exostellar Management Server"},{"location":"deployment-prerequisites/#create-configuration-file","text":"Before you deploy a cluster you need to create a configuration file. A default configuration file is found in source/resources/config/default_config.yml . You should create a new config file and update the parameters for your cluster. Ideally you should version control this file so you can keep track of changes. The schema for the config file along with its default values can be found in source/cdk/config_schema.py . The schema is defined in python, but the actual config file should be in yaml format. See Configuration File Format for documentation on all of the parameters. The following are key parameters that you will need to update. If you do not have the required parameters in your config file then the installer script will fail unless you specify the --prompt option. You should save your selections in the config file. Parameter Description Valid Values Default StackName The cloudformation stack that will deploy the cluster. I prefer to end the name with \"-config\" . None slurm/ClusterName Name of the Slurm cluster Can't be the same as StackName. If StackName ends in \"-config\" then StackName with \"-config\" stripped off. Otherwise, StackName with \"-cl\" appended. Region Region where VPC is located $AWS_DEFAULT_REGION VpcId The vpc where the cluster will be deployed. vpc-* None SshKeyPair EC2 Keypair to use for instances None ErrorSnsTopicArn ARN of an SNS topic that will be notified of errors arn:aws:sns:{{region}}:{AccountId}:{TopicName} None slurm/InstanceConfig Configure instance types that the cluster can use and number of nodes. See default_config.yml AdditionalSecurityGroupsStackName Name of stack that created security groups for external login nodes and file systems. RESStackName Name of RES environment slurm/storage/ExtraMounts Extra mount points None","title":"Create Configuration File"},{"location":"deployment-prerequisites/#configure-slurm-accounting-database-slurmdbd","text":"If you created a ParallelCluster Slurm Database and a Slurmdbd Instance then add the following configuration parameters. slurm: ParallelClusterConfig: Slurmdbd: SlurmdbdStackName: <Slurmdbd-Stack-Name>","title":"Configure Slurm Accounting database (slurmdbd)"},{"location":"deployment-prerequisites/#configure-linux-users-and-groups","text":"The cluster defines a script that can capture the users and groups from your identity provider (IDP) into a json file. When a new compute node starts, another script creates local Linux users and groups from the json file. The first script gets installed at: /opt/slurm/{{ cluster_name }}/config/bin/create_users_and_groups_json.py This script should be run on an instance that is joined to your IDP. It first tries to use wbinfo -u and if that fails it uses getent passwd to get the list of users and their userids. It uses id to get the uid and gids for the users. The json file gets stored at /opt/slurm/{{ cluster_name }}/config/users_groups.json The compute node calls: /opt/slurm/{{ cluster_name }}/config/bin/create_users_groups.py -i /opt/slurm/{{ cluster_name }}/config/users_groups.json The script calls useradd and groupadd to create local users and groups. To enable this mechanism you must configure the EC2 tags of the domain joined instance that will be used to create the json file. A Lambda function will create the json file and create a lambda that will refresh it hourly. You will also need to provide the security group id of the SlurmExternalLoginNodeSG which will be added to the instance so that it can mount the head node's NFS file system. DomainJoinedInstance: Tags: - Key: Name Value: ClusterManager SecurityGroupId: sg-xxxxxxxx You can provide 1 or more keys and the set will be done on the first instance that matches. Note: You do not have to use this mechanism. ParallelCluster supports using Microsoft Active Directory and you can configure that using the slurm/ParallelClusterConfig/ClusterConfig/DirectoryService parameter . You can also use custom action scripts that run on your compute nodes that configure domains or users and groups to meet your needs. Note: This is automatically configured for you if you specify the RESStackName parameter.","title":"Configure Linux Users and Groups"},{"location":"deployment-prerequisites/#configure-the-compute-instances","text":"The slurm/InstanceConfig configuration parameter configures the base operating systems, CPU architectures, instance families, and instance types that the Slurm cluster should support. ParallelCluster currently doesn't support heterogeneous clusters; all nodes must have the same architecture and Base OS. Base OS CPU Architectures Amazon Linux 2 x86_64, arm64 CentOS 7 x86_64 RedHat 7 x86_64 RedHat 8 x86_64, arm64 RedHat 9 x86_64, arm64 Rocky 8 x86_64, arm64 Rocky 9 x86_64, arm64 You can exclude instances types by family or specific instance type. By default the InstanceConfig excludes older generation instance families. You can include instances by family or specific instance type. If no includes are specified then all non-excluded instance types will be used. You can also choose to only include the largest instance size within a family. The advantage of using the max instance size is that jobs running on the instance have the highest network bandwidth for that family and fewer instances are required to run the same number of jobs. This may help jobs run faster and allow jobs to wait less time for a new instance to start. The disadvantage is higher cost if the instance is lightly loaded. The default InstanceConfig includes all supported base OSes and architectures and burstable and general purpose instance types. default instance families default instance types default excluded instance families default excluded instance types Note that instance types and families are python regular expressions. slurm: InstanceConfig: Include: InstanceFamilies: - t3.* - m6a.* InstanceTypes: - r6a.large The following InstanceConfig configures instance types recommended for EDA workloads running on CentOS. slurm: InstanceConfig: Include: InstanceFamilies: - c5.* - c6g.* - f1 - m5.* - m6g.* - r5.* - r6g.* - x2gd - z1d If you have reserved instances (RIs) or savings plans then you can configure instances so that they are always on since you are paying for them whether they are running or not. To do this add a MinCount greater than 0 for the compute resources that contain the instance types. slurm: InstanceConfig: NodeCounts: DefaultMinCount: 1","title":"Configure the Compute Instances"},{"location":"deployment-prerequisites/#configure-fair-share-scheduling-optional","text":"Slurm supports fair share scheduling , but it requires the fair share policy to be configured. By default, all users will be put into a default group that has a low fair share. The configuration file is at source/resources/playbooks/roles/ParallelClusterHeadNode/files/opt/slurm/config/accounts.yml.example in the repository and is deployed to /opt/slurm/{{ClusterName}}/conf/accounts.yml . The file is a simple yaml file that allows you to configure groups, the users that belong to the group, and a fair share weight for the group. Refer to the Slurm documentation for details on how the fair share weight is calculated. The scheduler can be configured so that users who aren't getting their fair share of resources get higher priority. The following shows 3 top level groups. Note that the fairshare weights aren't a percentage. They are just a relative weight. In this example, the projects have 9 times higher weight than the jenkins group. jenkins: fairshare: 10 users: - jenkins project1: fairshare: 90 project2: fairshare: 90 The allocation of top level groups can be further subdivided to control the relative priority of jobs within that group. For example, a project may have design verification (dv), rtl design (rtl), physical design (dv), and formal verification (fv) teams. The following example shows how the project's allocation can be prioritized for the different teams. If a group is using more than it's fair share then its jobs will have lower priority than jobs whose users aren't getting their fair share. project1-dv: parent: project1 fairshare: 80 users: - dvuser1 project1-pd: parent: project1 fairshare: 10 users: - pduser1 project1-rtl: parent: project1 fairshare: 10 users: - rtluser1 project1-fv: parent: project1 fairshare: 10 users: - fvuser1 The scheduler uses the priority/multifactor plugin to calculate job priorities. Fair share is just one of the factors. Read the Multifactor Priority Plugin documentation for the details. This is the default configuration in slurm.conf. The partition weight is set the highest so that jobs in the interactive partition always have the highest priority. Fairshare and QOS are the next highest weighted factors. The next factor is the job age, which means all else being equal the jobs run in FIFO order with the jobs that have been waiting the longest getting higher priority. PriorityType=priority/multifactor PriorityWeightPartition=100000 PriorityWeightFairshare=10000 PriorityWeightQOS=10000 PriorityWeightAge=1000 PriorityWeightAssoc=0 PriorityWeightJobSize=0 These weights can be adjusted based on your needs to control job priorities.","title":"Configure Fair Share Scheduling (Optional)"},{"location":"deployment-prerequisites/#configure-licenses","text":"Slurm supports configuring licenses as a consumable resource . It will keep track of how many running jobs are using a license and when no more licenses are available then jobs will stay pending in the queue until a job completes and frees up a license. Combined with the fairshare algorithm, this can prevent users from monopolizing licenses and preventing others from being able to run their jobs. Licenses are configured using the slurm/Licenses configuration variable. If you are using the Slurm database then these will be configured in the database. Otherwise they will be configured in /opt/slurm/{{ClusterName}}/etc/pcluster/custom_slurm_settings_include_file_slurm.conf . The example configuration shows how the number of licenses can be configured. In this example, the cluster will manage 800 vcs licenses and 1 ansys license. Users must request a license using the -L or --licenses options. slurm: Licenses: vcs: Count: 800 ansys: Count: 1","title":"Configure Licenses"},{"location":"deployment-prerequisites/#configure-file-systems","text":"The Storage/ExtraMounts parameter allows you to configure additional file systems to mount on compute nodes. Note that the security groups for the file systems must allow connections from the compute nodes.","title":"Configure File Systems"},{"location":"deployment-prerequisites/#lustre","text":"The following example shows how to add an FSx for Lustre file system. The mount information can be found from the FSx console. storage: ExtraMounts - dest: /lustre src: <FileSystemId>.fsx.<Region>.amazonaws.com@tcp:/<MountName> StorageType: FsxLustre FileSystemId: <FileSystemId> type: lustre options: relatime,flock","title":"Lustre"},{"location":"deployment-prerequisites/#ontap","text":"The following example shows how to add an FSx for NetApp ONTAP file system. The mount information can be found from the FSx console. storage: ExtraMounts - dest: /ontap src: <SvmId>.<FileSystemId>.fsx.<Region>.amazonaws.com:/vol1 StorageType: FsxOntap FileSystemId: <FileSystemId> VolumeId: <VolumeId> type: nfs options: default","title":"ONTAP"},{"location":"deployment-prerequisites/#zfs","text":"The following example shows how to add an FSx for OpenZFS file system. The mount information can be found from the FSx console. storage: ExtraMounts - dest: /zfs src: <FileSystemId>.fsx.<Region>.amazonaws.com:/fsx StorageType: FsxOpenZfs FileSystemId: <FileSystemId> VolumeId: <VolumeId> type: nfs options: noatime,nfsvers=3,sync,nconnect=16,rsize=1048576,wsize=1048576","title":"ZFS"},{"location":"exostellar-infrastructure-optimizer/","text":"Exostellar Infrastructure Optimizer (XIO) Exostellar Infrastructure Optimizer (XIO) runs applications in virtual machines (VMs) on EC2 instances and can dynamically migrate the VMs between instances based on availability and cost. Long-running, stateful jobs are not normally run on spot instances because of the risk of lost work after a spot termination. XIO reduces this risk by predicting spot terminations and migrating the VM to another instance with higher availability. This could be a different spot instance type or an on-demand instance. When spot capacity becomes available again, the VM can be migrated back to a spot instance. This allows you to save up to 90% over on-demand pricing by running on spot when capacity is available. You increase the potential for savings by configuring as many spot capacity pools as possible. This doesn't completely eliminate the risk of the job failing. The job will still fail and need to be restarted from the beginning if a spot termination isn't predicted far enough in advance for the job to be migrated or if a new instance cannot be launched to migrate the job to. Note : Job reliability will be increased by following EC2 Spot best practices such as configuring as many capacity pools and instance types as possible. XIO runs on an Exostellar Management Server (EMS). The EMS runs a web application and launches and manages the instances that run jobs. In response to job requests it launches controller nodes that manage pools of worker nodes. The controller launches workers and then starts one or more VMs on the workers. The controller also determines when VMs need to be migrated, allocates new workers, and manages the VM migrations. You create an XIO Application Environment for each Slurm cluster. The Application Environment contains the URL for the Slurm head node, configures pools of VMs, and configures the path to the Slurm binaries and configuration. The VM pools define the attributes of the instances including the number of CPUs, VM Image, min and max memory, and an associated XIO Profile. You must also create the XIO Profiles that are used by the VM Pools. Each profile configures XIO Controllers and XIO Workers. The Workers run the XIO VMs. The Controller manages the workers and the VMs that run on them. The Worker configuration includes the instance types to use for on-demand and spot instances. It also includes the security groups and tags for the worker instances. You must also create XIO Images that are used to create the VMs. The Images are created from AWS AMIs and are specified in the VM Pools. NOTE: One current restriction of XIO VMs is that they cannot be created from ParallelCluster AMIs. This is because the kernel modules that ParallelCluster installs aren't supported by the XIO hypervisor. XIO Configuration This section will describe the process of configuring XIO to work with ParallelCluster. Refer to Exostellar's documentation to make sure you have the latest instructions. Deploy ParallelCluster without configuring XIO First deploy your cluster without configuring XIO. The cluster deploys ansible playbooks that will be used to create the XIO ParallelCluster AMI. Deploy the Exostellar Management Server (EMS) The next step is to install the Exostellar management server . You must first subscribe to the three Exostellar Infrastructure AMIs in the AWS Marketplace. Exostellar Management Server Exostellar Controller Exostellar Worker Then follow the directions to deploy the CloudFormation template . Verify that the \"az1\" profile exists In the EMS GUI go to Profiles and make sure that the \"az1\" profile exists. I use that as a template to create your new profiles. If it doesn't exist, there was a problem with the EMS deployment and you should contact Exostellar support. Create an XIO ParallelCluster AMI Launch an instance using the base AMI for your OS. For example, launch an instance with a base RHEL 8 or Rocky 8 AMI. Mount the ParallelCluster NFS file system at /opt/slurm. Run the ansible playbook to configure the instance for XIO. /opt/slurm/config/bin/xio-compute-node-ami-configure.sh Do any additional configuration that you require such as configuring file system mounts and installing packages. Create an AMI from the instance and wait for it to become available. After the AMI has been successfully created you can either stop or terminate the instance to save costs. If you may need to do additional customization, then stop it, otherwise terminate it. Add the image id to your configuration as described below. Create XIO Configuration The next step is to plan and configure your XIO deployment. The key decisions that you must make are the instance types that you will use and the AMI that you will use for the XIO VM Images. XIO currently only supports x86_64 instance types and pools cannot mix AMD and Intel instance types. The following XIO configuration for aws-eda-slurm-cluster shows 2 pools that contain Intel and AMD instances. Note that we first define the XIO Profiles with instance types with the same manufacturer, number of cores, and amount of memory. Then we configure pools for the Application Environment that use the profiles. The numbers after the instance type are a priority to bias XIO to use higher priority instance types if they are available. We've chosen to prioritize the latest generation instance types so our jobs run faster and configure older generation instance types at a lower priority to increase the number of capacity pools so that we have a better chance of running on spot and having instances to run our jobs. Refer to Best practices for Amazon EC2 Spot when planning your cluster deployment and creating your configuration. NOTE : XIO currently doesn't support VMs larger than 1 TB. It is highly recommended to use EC2 Spot placement scores when selecting the region and availability zone for your cluster. This will give you an indication of the likelihood of getting desired spot capacity. In the following example I've configured a profile for AMD and Intel instance families. I've included instance families from the last 3 generations of instances to maximize the number of available capacity pools and increase the likelihood of running on spot. Note : The Intel instance families contain more configurations and higher memory instances. They also have high frequency instance types such as m5zn, r7iz, and z1d. They also tend to have more capacity. The AMD instance families include HPC instance types, however, they do not support spot pricing and can only be used for on-demand. Note : This is only an example configuration. You should customize it for your requirements. slurm: Xio: ManagementServerStackName: exostellar-management-server PartitionName: xio AvailabilityZone: us-east-2b Images: - ImageId: ami-xxxxxxxxxxxxxxxxx ImageName: <your-xio-vm-image-name> DefaultImageName: <your-xio-vm-image-name> Profiles: - ProfileName: amd NodeGroupName: amd MaxControllers: 10 InstanceTypes: - c5a:1 - c5ad:1 - c6a:4 - c7a:7 - m5a:1 - m5ad:1 - m6a:4 - m7a:7 - r5a:1 - r5ad:1 - r6a:4 - r7a:7 SpotFleetTypes: - c5a:1 - c5ad:1 - c6a:4 - c7a:7 - m5a:1 - m5ad:1 - m6a:4 - m7a:7 - r5a:1 - r5ad:1 - r6a:4 - r7a:7 EnableHyperthreading: false - ProfileName: intel NodeGroupName: intel MaxControllers: 10 InstanceTypes: - c5n:1 - c5d:1 - c5:1 - c6in:4 - c6id:4 - c6i:4 - c7i:7 - m5:1 - m5d:1 - m5dn:1 - m5n:1 - m5zn:1 - m6i:4 - m6id:4 - m6idn:4 - m6in:4 - m7i:7 - r5:1 - r5b:1 - r5d:1 - r5dn:1 - r5n:1 - r6i:4 - r6id:4 - r6idn:4 - r6in:4 - r7i:7 - r7iz:7 # - x2idn:1 # - x2iedn:1 - z1d:1 SpotFleetTypes: - c5n:1 - c5d:1 - c5:1 - c6in:4 - c6id:4 - c6i:4 - c7i:7 - m5:1 - m5d:1 - m5dn:1 - m5n:1 - m5zn:1 - m6i:4 - m6id:4 - m6idn:4 - m6in:4 - m7i:7 - r5:1 - r5b:1 - r5d:1 - r5dn:1 - r5n:1 - r6i:4 - r6id:4 - r6idn:4 - r6in:4 - r7i:7 - r7iz:7 # - x2idn:1 # - x2iedn:1 - z1d:1 EnableHyperthreading: false Pools: - PoolName: amd-8g-2c ProfileName: amd PoolSize: 10 CPUs: 2 InstanceMemory: 8192 - PoolName: amd-8g-4c ProfileName: amd PoolSize: 10 CPUs: 4 InstanceMemory: 8192 - PoolName: amd-16g-1c ProfileName: amd PoolSize: 10 CPUs: 1 InstanceMemory: 16384 - PoolName: amd-16g-2c ProfileName: amd PoolSize: 10 CPUs: 2 InstanceMemory: 16384 - PoolName: amd-16g-4c ProfileName: amd PoolSize: 10 CPUs: 4 InstanceMemory: 16384 - PoolName: amd-16g-8c ProfileName: amd PoolSize: 10 CPUs: 8 InstanceMemory: 16384 - PoolName: amd-32g-2c ProfileName: amd PoolSize: 10 CPUs: 2 InstanceMemory: 32768 - PoolName: amd-32g-4c ProfileName: amd PoolSize: 10 CPUs: 4 InstanceMemory: 32768 - PoolName: amd-32g-8c ProfileName: amd PoolSize: 10 CPUs: 8 InstanceMemory: 32768 - PoolName: amd-64g-4c ProfileName: amd PoolSize: 10 CPUs: 4 InstanceMemory: 65536 - PoolName: amd-64g-8c ProfileName: amd PoolSize: 10 CPUs: 8 InstanceMemory: 65536 - PoolName: amd-64g-16c ProfileName: amd PoolSize: 10 CPUs: 16 InstanceMemory: 65536 - PoolName: amd-64g-32c ProfileName: amd PoolSize: 10 CPUs: 32 InstanceMemory: 65536 - PoolName: amd-128g-8c ProfileName: amd PoolSize: 10 CPUs: 8 InstanceMemory: 131072 - PoolName: amd-128g-16c ProfileName: amd PoolSize: 10 CPUs: 16 InstanceMemory: 131072 - PoolName: amd-128g-32c ProfileName: amd PoolSize: 10 CPUs: 32 InstanceMemory: 131072 - PoolName: amd-128g-64c ProfileName: amd PoolSize: 10 CPUs: 64 InstanceMemory: 131072 - PoolName: amd-192g-24c ProfileName: amd PoolSize: 10 CPUs: 24 InstanceMemory: 196608 - PoolName: amd-192g-48c ProfileName: amd PoolSize: 10 CPUs: 48 InstanceMemory: 196608 - PoolName: amd-256g-16c ProfileName: amd PoolSize: 10 CPUs: 16 InstanceMemory: 262144 - PoolName: amd-256g-32c ProfileName: amd PoolSize: 10 CPUs: 32 InstanceMemory: 262144 - PoolName: amd-256g-64c ProfileName: amd PoolSize: 10 CPUs: 64 InstanceMemory: 262144 - PoolName: amd-256g-128c ProfileName: amd PoolSize: 10 CPUs: 128 InstanceMemory: 262144 - PoolName: amd-384g-24c ProfileName: amd PoolSize: 10 CPUs: 24 InstanceMemory: 393216 - PoolName: amd-384g-48c ProfileName: amd PoolSize: 10 CPUs: 48 InstanceMemory: 393216 - PoolName: amd-384g-96c ProfileName: amd PoolSize: 10 CPUs: 96 InstanceMemory: 393216 - PoolName: amd-384g-192c ProfileName: amd PoolSize: 10 CPUs: 192 InstanceMemory: 393216 - PoolName: amd-512g-32c ProfileName: amd PoolSize: 10 CPUs: 32 InstanceMemory: 524288 - PoolName: amd-512g-64c ProfileName: amd PoolSize: 10 CPUs: 64 InstanceMemory: 524288 - PoolName: amd-512g-128c ProfileName: amd PoolSize: 10 CPUs: 128 InstanceMemory: 524288 - PoolName: amd-768g-48c ProfileName: amd PoolSize: 10 CPUs: 48 InstanceMemory: 786432 - PoolName: amd-768g-96c ProfileName: amd PoolSize: 10 CPUs: 96 InstanceMemory: 786432 - PoolName: amd-768g-192c ProfileName: amd PoolSize: 10 CPUs: 192 InstanceMemory: 786432 - PoolName: amd-1024g-64c ProfileName: amd PoolSize: 10 CPUs: 64 InstanceMemory: 1048576 - PoolName: amd-1536g-96c ProfileName: amd PoolSize: 10 CPUs: 96 InstanceMemory: 1572864 - PoolName: amd-1536g-192c ProfileName: amd PoolSize: 10 CPUs: 192 InstanceMemory: 1572864 - PoolName: intel-8g-1c ProfileName: intel PoolSize: 10 CPUs: 1 InstanceMemory: 8192 - PoolName: intel-8g-2c ProfileName: intel PoolSize: 10 CPUs: 2 InstanceMemory: 8192 - PoolName: intel-16g-1c ProfileName: intel PoolSize: 10 CPUs: 1 InstanceMemory: 16384 - PoolName: intel-16g-2c ProfileName: intel PoolSize: 10 CPUs: 2 InstanceMemory: 16384 - PoolName: intel-16g-4c ProfileName: intel PoolSize: 10 CPUs: 4 InstanceMemory: 16384 - PoolName: intel-32g-2c ProfileName: intel PoolSize: 10 CPUs: 2 InstanceMemory: 32768 - PoolName: intel-32g-4c ProfileName: intel PoolSize: 10 CPUs: 4 InstanceMemory: 32768 - PoolName: intel-32g-8c ProfileName: intel PoolSize: 10 CPUs: 8 InstanceMemory: 32768 - PoolName: intel-48g-6c ProfileName: intel PoolSize: 10 CPUs: 6 InstanceMemory: 49152 - PoolName: intel-64g-4c ProfileName: intel PoolSize: 10 CPUs: 4 InstanceMemory: 65536 - PoolName: intel-64g-8c ProfileName: intel PoolSize: 10 CPUs: 8 InstanceMemory: 65536 - PoolName: intel-64g-16c ProfileName: intel PoolSize: 10 CPUs: 16 InstanceMemory: 65536 - PoolName: intel-72g-18c ProfileName: intel PoolSize: 10 CPUs: 18 InstanceMemory: 73728 - PoolName: intel-96g-6c ProfileName: intel PoolSize: 10 CPUs: 6 InstanceMemory: 98304 - PoolName: intel-96g-12c ProfileName: intel PoolSize: 10 CPUs: 12 InstanceMemory: 98304 - PoolName: intel-96g-24c ProfileName: intel PoolSize: 10 CPUs: 12 InstanceMemory: 98304 # - PoolName: intel-128g-2c # x2iedn.xlarge # ProfileName: intel # PoolSize: 10 # CPUs: 2 # InstanceMemory: 131072 - PoolName: intel-128g-8c ProfileName: intel PoolSize: 10 CPUs: 8 InstanceMemory: 131072 - PoolName: intel-128g-16c ProfileName: intel PoolSize: 10 CPUs: 16 InstanceMemory: 131072 - PoolName: intel-128g-32c ProfileName: intel PoolSize: 10 CPUs: 32 InstanceMemory: 131072 - PoolName: intel-144g-36c # c5[d].18xlarge ProfileName: intel PoolSize: 10 CPUs: 36 InstanceMemory: 147456 - PoolName: intel-192g-12c ProfileName: intel PoolSize: 10 CPUs: 12 InstanceMemory: 196608 - PoolName: intel-192g-24c ProfileName: intel PoolSize: 10 CPUs: 24 InstanceMemory: 196608 - PoolName: intel-192g-48c ProfileName: intel PoolSize: 10 CPUs: 48 InstanceMemory: 196608 # - PoolName: intel-256g-4c # x2iedn.2xlarge # ProfileName: intel # PoolSize: 10 # CPUs: 4 # InstanceMemory: 262144 - PoolName: intel-256g-16c ProfileName: intel PoolSize: 10 CPUs: 16 InstanceMemory: 262144 - PoolName: intel-256g-32c ProfileName: intel PoolSize: 10 CPUs: 32 InstanceMemory: 262144 - PoolName: intel-256g-64c ProfileName: intel PoolSize: 10 CPUs: 64 InstanceMemory: 262144 - PoolName: intel-384g-24c ProfileName: intel PoolSize: 10 CPUs: 24 InstanceMemory: 393216 - PoolName: intel-384g-48c ProfileName: intel PoolSize: 10 CPUs: 48 InstanceMemory: 393216 - PoolName: intel-384g-96c ProfileName: intel PoolSize: 10 CPUs: 96 InstanceMemory: 393216 # - PoolName: intel-512g-8c # x2iedn.4xlarge # ProfileName: intel # PoolSize: 10 # CPUs: 8 # InstanceMemory: 524288 - PoolName: intel-512g-32c ProfileName: intel PoolSize: 10 CPUs: 32 InstanceMemory: 524288 - PoolName: intel-512g-64c ProfileName: intel PoolSize: 10 CPUs: 64 InstanceMemory: 524288 - PoolName: intel-768g-48c ProfileName: intel PoolSize: 10 CPUs: 48 InstanceMemory: 786432 - PoolName: intel-768g-96c ProfileName: intel PoolSize: 10 CPUs: 96 InstanceMemory: 786432 # - PoolName: intel-1024g-16c # x2iedn.8xlarge # ProfileName: intel # PoolSize: 10 # CPUs: 16 # InstanceMemory: 1048576 # - PoolName: intel-1024g-32c # x2idn.16xlarge # ProfileName: intel # PoolSize: 10 # CPUs: 32 # InstanceMemory: 1048576 - PoolName: intel-1024g-64c ProfileName: intel PoolSize: 10 CPUs: 64 InstanceMemory: 1048576 Update the cluster with the XIO configuration Update the cluster with the XIO configuration. This will update the profiles and environment on the EMS server and configure the cluster for XIO. The only remaining step before you can submit jobs is to create the XIO VM image. This is done before creating an image because the XIO scripts get deployed by this step. Create an XIO Image from the XIO ParallelCluster AMI Connect to the head node and create the XIO Image from the AMI you created. The IMAGE-NAME should be the same that you configured in the Pools. /opt/slurm/etc/exostellar/parse_helper.sh -a <AMI-ID1> -i <IMAGE-NAME> Test launching an XIO VM Connect to the head node and test launching a VM. The pool, profile, and image_name should be from your configuration. The host name doesn't matter. /opt/slurm/etc/exostellar/test_createVm.sh --pool <pool> --profile <profile> -i <image name> -h <host> When this is done, the VM, worker, and controller should all terminate on their own. If they do not, then connect to the EMS and cancel the job that started the controller. Use squeue to list the controller jobs. Use scancel to terminate them. Run a test job using Slurm srun --pty -p xio- Debug UpdateHeadNode resource failed If the UpdateHeadNode resource fails then it is usually because as task in the ansible script failed. Connect to the head node and look for errors in: /var/log/ansible.log Usually it will be a problem with the /opt/slurm/etc/exostellar/configure_xio.py script. When this happens the CloudFormation stack will usually be in UPDATE_ROLLBACK_FAILED status. Before you can update it again you will need to complete the rollback. Go to Stack Actions, select Continue update rollback , expand Advanced troubleshooting , check the UpdateHeadNode resource, anc click Continue update rollback . XIO Controller not starting On EMA, check that a job is running to create the controller. squeue On EMS, check the autoscaling log to see if there are errors starting the instance. `less /var/log/slurm/autoscaling.log`` EMS Slurm partions are at: /xcompute/slurm/bin/partitions.json They are derived from the partition and pool names. Worker instance not starting VM not starting on worker VM not starting Slurm job","title":"Exostellar Infrastructure Optimizer (XIO)"},{"location":"exostellar-infrastructure-optimizer/#exostellar-infrastructure-optimizer-xio","text":"Exostellar Infrastructure Optimizer (XIO) runs applications in virtual machines (VMs) on EC2 instances and can dynamically migrate the VMs between instances based on availability and cost. Long-running, stateful jobs are not normally run on spot instances because of the risk of lost work after a spot termination. XIO reduces this risk by predicting spot terminations and migrating the VM to another instance with higher availability. This could be a different spot instance type or an on-demand instance. When spot capacity becomes available again, the VM can be migrated back to a spot instance. This allows you to save up to 90% over on-demand pricing by running on spot when capacity is available. You increase the potential for savings by configuring as many spot capacity pools as possible. This doesn't completely eliminate the risk of the job failing. The job will still fail and need to be restarted from the beginning if a spot termination isn't predicted far enough in advance for the job to be migrated or if a new instance cannot be launched to migrate the job to. Note : Job reliability will be increased by following EC2 Spot best practices such as configuring as many capacity pools and instance types as possible. XIO runs on an Exostellar Management Server (EMS). The EMS runs a web application and launches and manages the instances that run jobs. In response to job requests it launches controller nodes that manage pools of worker nodes. The controller launches workers and then starts one or more VMs on the workers. The controller also determines when VMs need to be migrated, allocates new workers, and manages the VM migrations. You create an XIO Application Environment for each Slurm cluster. The Application Environment contains the URL for the Slurm head node, configures pools of VMs, and configures the path to the Slurm binaries and configuration. The VM pools define the attributes of the instances including the number of CPUs, VM Image, min and max memory, and an associated XIO Profile. You must also create the XIO Profiles that are used by the VM Pools. Each profile configures XIO Controllers and XIO Workers. The Workers run the XIO VMs. The Controller manages the workers and the VMs that run on them. The Worker configuration includes the instance types to use for on-demand and spot instances. It also includes the security groups and tags for the worker instances. You must also create XIO Images that are used to create the VMs. The Images are created from AWS AMIs and are specified in the VM Pools. NOTE: One current restriction of XIO VMs is that they cannot be created from ParallelCluster AMIs. This is because the kernel modules that ParallelCluster installs aren't supported by the XIO hypervisor.","title":"Exostellar Infrastructure Optimizer (XIO)"},{"location":"exostellar-infrastructure-optimizer/#xio-configuration","text":"This section will describe the process of configuring XIO to work with ParallelCluster. Refer to Exostellar's documentation to make sure you have the latest instructions.","title":"XIO Configuration"},{"location":"exostellar-infrastructure-optimizer/#deploy-parallelcluster-without-configuring-xio","text":"First deploy your cluster without configuring XIO. The cluster deploys ansible playbooks that will be used to create the XIO ParallelCluster AMI.","title":"Deploy ParallelCluster without configuring XIO"},{"location":"exostellar-infrastructure-optimizer/#deploy-the-exostellar-management-server-ems","text":"The next step is to install the Exostellar management server . You must first subscribe to the three Exostellar Infrastructure AMIs in the AWS Marketplace. Exostellar Management Server Exostellar Controller Exostellar Worker Then follow the directions to deploy the CloudFormation template .","title":"Deploy the Exostellar Management Server (EMS)"},{"location":"exostellar-infrastructure-optimizer/#verify-that-the-az1-profile-exists","text":"In the EMS GUI go to Profiles and make sure that the \"az1\" profile exists. I use that as a template to create your new profiles. If it doesn't exist, there was a problem with the EMS deployment and you should contact Exostellar support.","title":"Verify that the \"az1\" profile exists"},{"location":"exostellar-infrastructure-optimizer/#create-an-xio-parallelcluster-ami","text":"Launch an instance using the base AMI for your OS. For example, launch an instance with a base RHEL 8 or Rocky 8 AMI. Mount the ParallelCluster NFS file system at /opt/slurm. Run the ansible playbook to configure the instance for XIO. /opt/slurm/config/bin/xio-compute-node-ami-configure.sh Do any additional configuration that you require such as configuring file system mounts and installing packages. Create an AMI from the instance and wait for it to become available. After the AMI has been successfully created you can either stop or terminate the instance to save costs. If you may need to do additional customization, then stop it, otherwise terminate it. Add the image id to your configuration as described below.","title":"Create an XIO ParallelCluster AMI"},{"location":"exostellar-infrastructure-optimizer/#create-xio-configuration","text":"The next step is to plan and configure your XIO deployment. The key decisions that you must make are the instance types that you will use and the AMI that you will use for the XIO VM Images. XIO currently only supports x86_64 instance types and pools cannot mix AMD and Intel instance types. The following XIO configuration for aws-eda-slurm-cluster shows 2 pools that contain Intel and AMD instances. Note that we first define the XIO Profiles with instance types with the same manufacturer, number of cores, and amount of memory. Then we configure pools for the Application Environment that use the profiles. The numbers after the instance type are a priority to bias XIO to use higher priority instance types if they are available. We've chosen to prioritize the latest generation instance types so our jobs run faster and configure older generation instance types at a lower priority to increase the number of capacity pools so that we have a better chance of running on spot and having instances to run our jobs. Refer to Best practices for Amazon EC2 Spot when planning your cluster deployment and creating your configuration. NOTE : XIO currently doesn't support VMs larger than 1 TB. It is highly recommended to use EC2 Spot placement scores when selecting the region and availability zone for your cluster. This will give you an indication of the likelihood of getting desired spot capacity. In the following example I've configured a profile for AMD and Intel instance families. I've included instance families from the last 3 generations of instances to maximize the number of available capacity pools and increase the likelihood of running on spot. Note : The Intel instance families contain more configurations and higher memory instances. They also have high frequency instance types such as m5zn, r7iz, and z1d. They also tend to have more capacity. The AMD instance families include HPC instance types, however, they do not support spot pricing and can only be used for on-demand. Note : This is only an example configuration. You should customize it for your requirements. slurm: Xio: ManagementServerStackName: exostellar-management-server PartitionName: xio AvailabilityZone: us-east-2b Images: - ImageId: ami-xxxxxxxxxxxxxxxxx ImageName: <your-xio-vm-image-name> DefaultImageName: <your-xio-vm-image-name> Profiles: - ProfileName: amd NodeGroupName: amd MaxControllers: 10 InstanceTypes: - c5a:1 - c5ad:1 - c6a:4 - c7a:7 - m5a:1 - m5ad:1 - m6a:4 - m7a:7 - r5a:1 - r5ad:1 - r6a:4 - r7a:7 SpotFleetTypes: - c5a:1 - c5ad:1 - c6a:4 - c7a:7 - m5a:1 - m5ad:1 - m6a:4 - m7a:7 - r5a:1 - r5ad:1 - r6a:4 - r7a:7 EnableHyperthreading: false - ProfileName: intel NodeGroupName: intel MaxControllers: 10 InstanceTypes: - c5n:1 - c5d:1 - c5:1 - c6in:4 - c6id:4 - c6i:4 - c7i:7 - m5:1 - m5d:1 - m5dn:1 - m5n:1 - m5zn:1 - m6i:4 - m6id:4 - m6idn:4 - m6in:4 - m7i:7 - r5:1 - r5b:1 - r5d:1 - r5dn:1 - r5n:1 - r6i:4 - r6id:4 - r6idn:4 - r6in:4 - r7i:7 - r7iz:7 # - x2idn:1 # - x2iedn:1 - z1d:1 SpotFleetTypes: - c5n:1 - c5d:1 - c5:1 - c6in:4 - c6id:4 - c6i:4 - c7i:7 - m5:1 - m5d:1 - m5dn:1 - m5n:1 - m5zn:1 - m6i:4 - m6id:4 - m6idn:4 - m6in:4 - m7i:7 - r5:1 - r5b:1 - r5d:1 - r5dn:1 - r5n:1 - r6i:4 - r6id:4 - r6idn:4 - r6in:4 - r7i:7 - r7iz:7 # - x2idn:1 # - x2iedn:1 - z1d:1 EnableHyperthreading: false Pools: - PoolName: amd-8g-2c ProfileName: amd PoolSize: 10 CPUs: 2 InstanceMemory: 8192 - PoolName: amd-8g-4c ProfileName: amd PoolSize: 10 CPUs: 4 InstanceMemory: 8192 - PoolName: amd-16g-1c ProfileName: amd PoolSize: 10 CPUs: 1 InstanceMemory: 16384 - PoolName: amd-16g-2c ProfileName: amd PoolSize: 10 CPUs: 2 InstanceMemory: 16384 - PoolName: amd-16g-4c ProfileName: amd PoolSize: 10 CPUs: 4 InstanceMemory: 16384 - PoolName: amd-16g-8c ProfileName: amd PoolSize: 10 CPUs: 8 InstanceMemory: 16384 - PoolName: amd-32g-2c ProfileName: amd PoolSize: 10 CPUs: 2 InstanceMemory: 32768 - PoolName: amd-32g-4c ProfileName: amd PoolSize: 10 CPUs: 4 InstanceMemory: 32768 - PoolName: amd-32g-8c ProfileName: amd PoolSize: 10 CPUs: 8 InstanceMemory: 32768 - PoolName: amd-64g-4c ProfileName: amd PoolSize: 10 CPUs: 4 InstanceMemory: 65536 - PoolName: amd-64g-8c ProfileName: amd PoolSize: 10 CPUs: 8 InstanceMemory: 65536 - PoolName: amd-64g-16c ProfileName: amd PoolSize: 10 CPUs: 16 InstanceMemory: 65536 - PoolName: amd-64g-32c ProfileName: amd PoolSize: 10 CPUs: 32 InstanceMemory: 65536 - PoolName: amd-128g-8c ProfileName: amd PoolSize: 10 CPUs: 8 InstanceMemory: 131072 - PoolName: amd-128g-16c ProfileName: amd PoolSize: 10 CPUs: 16 InstanceMemory: 131072 - PoolName: amd-128g-32c ProfileName: amd PoolSize: 10 CPUs: 32 InstanceMemory: 131072 - PoolName: amd-128g-64c ProfileName: amd PoolSize: 10 CPUs: 64 InstanceMemory: 131072 - PoolName: amd-192g-24c ProfileName: amd PoolSize: 10 CPUs: 24 InstanceMemory: 196608 - PoolName: amd-192g-48c ProfileName: amd PoolSize: 10 CPUs: 48 InstanceMemory: 196608 - PoolName: amd-256g-16c ProfileName: amd PoolSize: 10 CPUs: 16 InstanceMemory: 262144 - PoolName: amd-256g-32c ProfileName: amd PoolSize: 10 CPUs: 32 InstanceMemory: 262144 - PoolName: amd-256g-64c ProfileName: amd PoolSize: 10 CPUs: 64 InstanceMemory: 262144 - PoolName: amd-256g-128c ProfileName: amd PoolSize: 10 CPUs: 128 InstanceMemory: 262144 - PoolName: amd-384g-24c ProfileName: amd PoolSize: 10 CPUs: 24 InstanceMemory: 393216 - PoolName: amd-384g-48c ProfileName: amd PoolSize: 10 CPUs: 48 InstanceMemory: 393216 - PoolName: amd-384g-96c ProfileName: amd PoolSize: 10 CPUs: 96 InstanceMemory: 393216 - PoolName: amd-384g-192c ProfileName: amd PoolSize: 10 CPUs: 192 InstanceMemory: 393216 - PoolName: amd-512g-32c ProfileName: amd PoolSize: 10 CPUs: 32 InstanceMemory: 524288 - PoolName: amd-512g-64c ProfileName: amd PoolSize: 10 CPUs: 64 InstanceMemory: 524288 - PoolName: amd-512g-128c ProfileName: amd PoolSize: 10 CPUs: 128 InstanceMemory: 524288 - PoolName: amd-768g-48c ProfileName: amd PoolSize: 10 CPUs: 48 InstanceMemory: 786432 - PoolName: amd-768g-96c ProfileName: amd PoolSize: 10 CPUs: 96 InstanceMemory: 786432 - PoolName: amd-768g-192c ProfileName: amd PoolSize: 10 CPUs: 192 InstanceMemory: 786432 - PoolName: amd-1024g-64c ProfileName: amd PoolSize: 10 CPUs: 64 InstanceMemory: 1048576 - PoolName: amd-1536g-96c ProfileName: amd PoolSize: 10 CPUs: 96 InstanceMemory: 1572864 - PoolName: amd-1536g-192c ProfileName: amd PoolSize: 10 CPUs: 192 InstanceMemory: 1572864 - PoolName: intel-8g-1c ProfileName: intel PoolSize: 10 CPUs: 1 InstanceMemory: 8192 - PoolName: intel-8g-2c ProfileName: intel PoolSize: 10 CPUs: 2 InstanceMemory: 8192 - PoolName: intel-16g-1c ProfileName: intel PoolSize: 10 CPUs: 1 InstanceMemory: 16384 - PoolName: intel-16g-2c ProfileName: intel PoolSize: 10 CPUs: 2 InstanceMemory: 16384 - PoolName: intel-16g-4c ProfileName: intel PoolSize: 10 CPUs: 4 InstanceMemory: 16384 - PoolName: intel-32g-2c ProfileName: intel PoolSize: 10 CPUs: 2 InstanceMemory: 32768 - PoolName: intel-32g-4c ProfileName: intel PoolSize: 10 CPUs: 4 InstanceMemory: 32768 - PoolName: intel-32g-8c ProfileName: intel PoolSize: 10 CPUs: 8 InstanceMemory: 32768 - PoolName: intel-48g-6c ProfileName: intel PoolSize: 10 CPUs: 6 InstanceMemory: 49152 - PoolName: intel-64g-4c ProfileName: intel PoolSize: 10 CPUs: 4 InstanceMemory: 65536 - PoolName: intel-64g-8c ProfileName: intel PoolSize: 10 CPUs: 8 InstanceMemory: 65536 - PoolName: intel-64g-16c ProfileName: intel PoolSize: 10 CPUs: 16 InstanceMemory: 65536 - PoolName: intel-72g-18c ProfileName: intel PoolSize: 10 CPUs: 18 InstanceMemory: 73728 - PoolName: intel-96g-6c ProfileName: intel PoolSize: 10 CPUs: 6 InstanceMemory: 98304 - PoolName: intel-96g-12c ProfileName: intel PoolSize: 10 CPUs: 12 InstanceMemory: 98304 - PoolName: intel-96g-24c ProfileName: intel PoolSize: 10 CPUs: 12 InstanceMemory: 98304 # - PoolName: intel-128g-2c # x2iedn.xlarge # ProfileName: intel # PoolSize: 10 # CPUs: 2 # InstanceMemory: 131072 - PoolName: intel-128g-8c ProfileName: intel PoolSize: 10 CPUs: 8 InstanceMemory: 131072 - PoolName: intel-128g-16c ProfileName: intel PoolSize: 10 CPUs: 16 InstanceMemory: 131072 - PoolName: intel-128g-32c ProfileName: intel PoolSize: 10 CPUs: 32 InstanceMemory: 131072 - PoolName: intel-144g-36c # c5[d].18xlarge ProfileName: intel PoolSize: 10 CPUs: 36 InstanceMemory: 147456 - PoolName: intel-192g-12c ProfileName: intel PoolSize: 10 CPUs: 12 InstanceMemory: 196608 - PoolName: intel-192g-24c ProfileName: intel PoolSize: 10 CPUs: 24 InstanceMemory: 196608 - PoolName: intel-192g-48c ProfileName: intel PoolSize: 10 CPUs: 48 InstanceMemory: 196608 # - PoolName: intel-256g-4c # x2iedn.2xlarge # ProfileName: intel # PoolSize: 10 # CPUs: 4 # InstanceMemory: 262144 - PoolName: intel-256g-16c ProfileName: intel PoolSize: 10 CPUs: 16 InstanceMemory: 262144 - PoolName: intel-256g-32c ProfileName: intel PoolSize: 10 CPUs: 32 InstanceMemory: 262144 - PoolName: intel-256g-64c ProfileName: intel PoolSize: 10 CPUs: 64 InstanceMemory: 262144 - PoolName: intel-384g-24c ProfileName: intel PoolSize: 10 CPUs: 24 InstanceMemory: 393216 - PoolName: intel-384g-48c ProfileName: intel PoolSize: 10 CPUs: 48 InstanceMemory: 393216 - PoolName: intel-384g-96c ProfileName: intel PoolSize: 10 CPUs: 96 InstanceMemory: 393216 # - PoolName: intel-512g-8c # x2iedn.4xlarge # ProfileName: intel # PoolSize: 10 # CPUs: 8 # InstanceMemory: 524288 - PoolName: intel-512g-32c ProfileName: intel PoolSize: 10 CPUs: 32 InstanceMemory: 524288 - PoolName: intel-512g-64c ProfileName: intel PoolSize: 10 CPUs: 64 InstanceMemory: 524288 - PoolName: intel-768g-48c ProfileName: intel PoolSize: 10 CPUs: 48 InstanceMemory: 786432 - PoolName: intel-768g-96c ProfileName: intel PoolSize: 10 CPUs: 96 InstanceMemory: 786432 # - PoolName: intel-1024g-16c # x2iedn.8xlarge # ProfileName: intel # PoolSize: 10 # CPUs: 16 # InstanceMemory: 1048576 # - PoolName: intel-1024g-32c # x2idn.16xlarge # ProfileName: intel # PoolSize: 10 # CPUs: 32 # InstanceMemory: 1048576 - PoolName: intel-1024g-64c ProfileName: intel PoolSize: 10 CPUs: 64 InstanceMemory: 1048576","title":"Create XIO Configuration"},{"location":"exostellar-infrastructure-optimizer/#update-the-cluster-with-the-xio-configuration","text":"Update the cluster with the XIO configuration. This will update the profiles and environment on the EMS server and configure the cluster for XIO. The only remaining step before you can submit jobs is to create the XIO VM image. This is done before creating an image because the XIO scripts get deployed by this step.","title":"Update the cluster with the XIO configuration"},{"location":"exostellar-infrastructure-optimizer/#create-an-xio-image-from-the-xio-parallelcluster-ami","text":"Connect to the head node and create the XIO Image from the AMI you created. The IMAGE-NAME should be the same that you configured in the Pools. /opt/slurm/etc/exostellar/parse_helper.sh -a <AMI-ID1> -i <IMAGE-NAME>","title":"Create an XIO Image from the XIO ParallelCluster AMI"},{"location":"exostellar-infrastructure-optimizer/#test-launching-an-xio-vm","text":"Connect to the head node and test launching a VM. The pool, profile, and image_name should be from your configuration. The host name doesn't matter. /opt/slurm/etc/exostellar/test_createVm.sh --pool <pool> --profile <profile> -i <image name> -h <host> When this is done, the VM, worker, and controller should all terminate on their own. If they do not, then connect to the EMS and cancel the job that started the controller. Use squeue to list the controller jobs. Use scancel to terminate them.","title":"Test launching an XIO VM"},{"location":"exostellar-infrastructure-optimizer/#run-a-test-job-using-slurm","text":"srun --pty -p xio-","title":"Run a test job using Slurm"},{"location":"exostellar-infrastructure-optimizer/#debug","text":"","title":"Debug"},{"location":"exostellar-infrastructure-optimizer/#updateheadnode-resource-failed","text":"If the UpdateHeadNode resource fails then it is usually because as task in the ansible script failed. Connect to the head node and look for errors in: /var/log/ansible.log Usually it will be a problem with the /opt/slurm/etc/exostellar/configure_xio.py script. When this happens the CloudFormation stack will usually be in UPDATE_ROLLBACK_FAILED status. Before you can update it again you will need to complete the rollback. Go to Stack Actions, select Continue update rollback , expand Advanced troubleshooting , check the UpdateHeadNode resource, anc click Continue update rollback .","title":"UpdateHeadNode resource failed"},{"location":"exostellar-infrastructure-optimizer/#xio-controller-not-starting","text":"On EMA, check that a job is running to create the controller. squeue On EMS, check the autoscaling log to see if there are errors starting the instance. `less /var/log/slurm/autoscaling.log`` EMS Slurm partions are at: /xcompute/slurm/bin/partitions.json They are derived from the partition and pool names.","title":"XIO Controller not starting"},{"location":"exostellar-infrastructure-optimizer/#worker-instance-not-starting","text":"","title":"Worker instance not starting"},{"location":"exostellar-infrastructure-optimizer/#vm-not-starting-on-worker","text":"","title":"VM not starting on worker"},{"location":"exostellar-infrastructure-optimizer/#vm-not-starting-slurm-job","text":"","title":"VM not starting Slurm job"},{"location":"federation/","text":"Federation (legacy) To maximize performance, EDA workloads should run in a single AZ. If you need to run jobs in more than one AZ then you can use the federation feature of Slurm so that you can run jobs on multiple clusters. The config directory has example configuration files that demonstrate how deploy federated cluster into 3 AZs. source/config/slurm_eda_az1.yml source/config/slurm_eda_az2.yml source/config/slurm_eda_az3.yml These clusters should be deployed sequentially. The first cluster creates a cluster and a slurmdbd instance. The other 2 clusters are deployed into their own AZ by configuring the SubnetId of the cluster. They reuse the same slurmdbd instance so that they can reuse a common pool of licenses that is managed by the slurmdbd instance. The config files for the 2nd and 3rd clusters provide the stack names from the others so that the security groups can be updated to allow the required network traffic between the clusters. The following shows an example of the configuration. slurm_eda_az1: Federation: Name: slurmeda FederatedClusterStackNames: [] slurm_eda_az2: Federation: Name: slurmeda FederatedClusterStackNames: - slurmedaaz1 slurm_eda_az3: Federation: Name: slurmeda FederatedClusterStackNames: - slurmedaaz1 - slurmedaaz2","title":"Federation (legacy)"},{"location":"federation/#federation-legacy","text":"To maximize performance, EDA workloads should run in a single AZ. If you need to run jobs in more than one AZ then you can use the federation feature of Slurm so that you can run jobs on multiple clusters. The config directory has example configuration files that demonstrate how deploy federated cluster into 3 AZs. source/config/slurm_eda_az1.yml source/config/slurm_eda_az2.yml source/config/slurm_eda_az3.yml These clusters should be deployed sequentially. The first cluster creates a cluster and a slurmdbd instance. The other 2 clusters are deployed into their own AZ by configuring the SubnetId of the cluster. They reuse the same slurmdbd instance so that they can reuse a common pool of licenses that is managed by the slurmdbd instance. The config files for the 2nd and 3rd clusters provide the stack names from the others so that the security groups can be updated to allow the required network traffic between the clusters. The following shows an example of the configuration. slurm_eda_az1: Federation: Name: slurmeda FederatedClusterStackNames: [] slurm_eda_az2: Federation: Name: slurmeda FederatedClusterStackNames: - slurmedaaz1 slurm_eda_az3: Federation: Name: slurmeda FederatedClusterStackNames: - slurmedaaz1 - slurmedaaz2","title":"Federation (legacy)"},{"location":"implementation/","text":"Implementation Details (legacy) Slurm Infrastructure All hosts in the cluster must share a uniform user and group namespace. The munged service must be running before starting any slurm daemons. Directory Structure All of the configuration files, scripts, and logs can be found under the following directory. /opt/slurm/{{ClusterName}} CloudWatch Metrics CloudWatch metrics are published by the following sources, but the code is all in SlurmPlugin.py . Slurm power saving scripts /opt/slurm/{{ClusterName}}/bin/slurm_ec2_resume.py /opt/slurm/{{ClusterName}}/bin/slurm_ec2_resume_fail.py /opt/slurm/{{ClusterName}}/bin/slurm_ec2_stop.py /opt/slurm/{{ClusterName}}/bin/slurm_ec2_terminate.py Spot monitor running on compute nodes /opt/slurm/{{ClusterName}}/bin/spot_monitor.py Cron jobs running on the Slurm controller /opt/slurm/{{ClusterName}}/bin/slurm_ec2_publish_cw.py /opt/slurm/{{ClusterName}}/bin/terminate_old_instances.py Down Node Handling If a node has a problem running jobs then Slurm can mark it DOWN. This includes if the resume script cannot start an instance for any reason include insufficient EC2 capacity. This can create 2 issues. First, if the compute node is running then it is wasting EC2 costs. Second, the node will be unavailable for scheduling which reduces the configured capacity of the cluster. The cluster is configured to periodically check for DOWN nodes so that they aren't left running and wasting compute costs. This is done by /opt/slurm/{{ClusterName}}/bin/slurm_down_nodes_clean.sh . The script is called every day by a systemd service: /etc/systemd/system/slurm_down_nodes_clean.service This service is run at boot and once a day as defined in /etc/systemd/system/slurm_down_nodes_clean.timer Insufficient Capacity Exception (ICE) Handling When Slurm schedules a powered down node it calls the ResumeScript defined in slurm.conf . This is in /opt/slurm/{{ClusterName}}/bin/slurm_ec2_resume.py . The script will attempt to start an EC2 instance and if it receives and InsufficientCapacityException (ICE) then the node will be marked down and Slurm will requeue the job. However, this is inadequate because if there are a large number of instances of that instance type configured then Slurm will schedule them and try to start them with the same result. Eventually all of the powered down nodes will be marked DOWN and depending on the job requirements the job will be allocated to a node with a different instance type or it will fail. This can take a substantial amount of time so SlurmPlugin.py does the following when it receives an ICE. Mark the node as DRAIN so no new jobs are scheduled on it. Find all other powered down nodes of the same type and mark them DOWN so that they won't be scheduled after this node is marked DOWN. Nodes that are running will be left alone. Requeue jobs on the node that failed to resume because of ICE. Mark the node DOWN. Power down the node. This is so that Slurm knows that the node is powered down so that when it is marked IDLE it will be powered up when a job is scheduled on it. The slurm_down_nodes_clean.service periodically finds all DOWN Slurm nodes, powers them down, and then marks them IDLE so that they can have jobs scheduled on them. This will allow Slurm to attempt to use more nodes of the instance type in the hopes that there is more capacity. If not, then the cycle repeats.","title":"Implementation Details (legacy)"},{"location":"implementation/#implementation-details-legacy","text":"","title":"Implementation Details (legacy)"},{"location":"implementation/#slurm-infrastructure","text":"All hosts in the cluster must share a uniform user and group namespace. The munged service must be running before starting any slurm daemons.","title":"Slurm Infrastructure"},{"location":"implementation/#directory-structure","text":"All of the configuration files, scripts, and logs can be found under the following directory. /opt/slurm/{{ClusterName}}","title":"Directory Structure"},{"location":"implementation/#cloudwatch-metrics","text":"CloudWatch metrics are published by the following sources, but the code is all in SlurmPlugin.py . Slurm power saving scripts /opt/slurm/{{ClusterName}}/bin/slurm_ec2_resume.py /opt/slurm/{{ClusterName}}/bin/slurm_ec2_resume_fail.py /opt/slurm/{{ClusterName}}/bin/slurm_ec2_stop.py /opt/slurm/{{ClusterName}}/bin/slurm_ec2_terminate.py Spot monitor running on compute nodes /opt/slurm/{{ClusterName}}/bin/spot_monitor.py Cron jobs running on the Slurm controller /opt/slurm/{{ClusterName}}/bin/slurm_ec2_publish_cw.py /opt/slurm/{{ClusterName}}/bin/terminate_old_instances.py","title":"CloudWatch Metrics"},{"location":"implementation/#down-node-handling","text":"If a node has a problem running jobs then Slurm can mark it DOWN. This includes if the resume script cannot start an instance for any reason include insufficient EC2 capacity. This can create 2 issues. First, if the compute node is running then it is wasting EC2 costs. Second, the node will be unavailable for scheduling which reduces the configured capacity of the cluster. The cluster is configured to periodically check for DOWN nodes so that they aren't left running and wasting compute costs. This is done by /opt/slurm/{{ClusterName}}/bin/slurm_down_nodes_clean.sh . The script is called every day by a systemd service: /etc/systemd/system/slurm_down_nodes_clean.service This service is run at boot and once a day as defined in /etc/systemd/system/slurm_down_nodes_clean.timer","title":"Down Node Handling"},{"location":"implementation/#insufficient-capacity-exception-ice-handling","text":"When Slurm schedules a powered down node it calls the ResumeScript defined in slurm.conf . This is in /opt/slurm/{{ClusterName}}/bin/slurm_ec2_resume.py . The script will attempt to start an EC2 instance and if it receives and InsufficientCapacityException (ICE) then the node will be marked down and Slurm will requeue the job. However, this is inadequate because if there are a large number of instances of that instance type configured then Slurm will schedule them and try to start them with the same result. Eventually all of the powered down nodes will be marked DOWN and depending on the job requirements the job will be allocated to a node with a different instance type or it will fail. This can take a substantial amount of time so SlurmPlugin.py does the following when it receives an ICE. Mark the node as DRAIN so no new jobs are scheduled on it. Find all other powered down nodes of the same type and mark them DOWN so that they won't be scheduled after this node is marked DOWN. Nodes that are running will be left alone. Requeue jobs on the node that failed to resume because of ICE. Mark the node DOWN. Power down the node. This is so that Slurm knows that the node is powered down so that when it is marked IDLE it will be powered up when a job is scheduled on it. The slurm_down_nodes_clean.service periodically finds all DOWN Slurm nodes, powers them down, and then marks them IDLE so that they can have jobs scheduled on them. This will allow Slurm to attempt to use more nodes of the instance type in the hopes that there is more capacity. If not, then the cycle repeats.","title":"Insufficient Capacity Exception (ICE) Handling"},{"location":"job_preemption/","text":"Job Preemption The cluster is set up with an interactive partition that has a higher priority than all other partitions. All other partitions are configured to allow jobs to be preempted by the interactive queue. When an interactive job is pending because of compute resources then it can preempt another job and use the resources. The preempted job will be requeued so that it will rerun when resources become available. Jobs should rarely pend because of lack of compute resources if you've defined enough compute nodes in your configuration. The more likely reason for a job to pend is if it requires a license and all available licenses are already being used. However, it appears that Slurm doesn't support preemption based on licenses availability so if the reason a job is pending is because of licenses then it will not preempt jobs in a lower priority queue even if doing so would free up a license. Documentation https://slurm.schedmd.com/preempt.html","title":"Job Preemption"},{"location":"job_preemption/#job-preemption","text":"The cluster is set up with an interactive partition that has a higher priority than all other partitions. All other partitions are configured to allow jobs to be preempted by the interactive queue. When an interactive job is pending because of compute resources then it can preempt another job and use the resources. The preempted job will be requeued so that it will rerun when resources become available. Jobs should rarely pend because of lack of compute resources if you've defined enough compute nodes in your configuration. The more likely reason for a job to pend is if it requires a license and all available licenses are already being used. However, it appears that Slurm doesn't support preemption based on licenses availability so if the reason a job is pending is because of licenses then it will not preempt jobs in a lower priority queue even if doing so would free up a license.","title":"Job Preemption"},{"location":"job_preemption/#documentation","text":"https://slurm.schedmd.com/preempt.html","title":"Documentation"},{"location":"onprem/","text":"On-Premises Integration The Slurm cluster can also be configured to manage on-premises compute nodes. The user must configure the on-premises compute nodes and then give the configuration information. Network Requirements The on-prem network must have a CIDR range that doesn't overlap the Slurm cluster's VPC and the two networks need to be connected using VPN or AWS Direct Connect. The on-prem firewall must allow ingress and egress from the VPC. The ports are used to connect to the file systems, slurm controllers, and allow traffic between virtual desktops and compute nodes. DNS Requirements Local network DNS must have an entry for the slurm controller or have a forwarding rule to the AWS provided DNS in the Slurm VPC. File System Requirements All of the compute nodes in the cluster, including the on-prem nodes, must have file system mounts that replicate the same directory structure. This can involve mounting filesystems across VPN or Direct Connect or synchronizing file systems using tools like rsync or NetApp FlexCache or SnapMirror. Performance will dictate the architecture of the file system. The onprem compute nodes must mount the Slurm controller's NFS export so that they have access to the Slurm binaries and configuration file. They must then be configured to run slurmd so that they can be managed by Slurm. Slurm Configuration of On-Premises Compute Nodes The slurm cluster's configuration file allows the configuration of on-premises compute nodes. The Slurm cluster will not provision any of the on-prem nodes, network, or firewall, but it will configure the cluster's resources to be used by the on-prem nodes. All that needs to be configured are the configuration file for the on-prem nodes and the CIDR block. InstanceConfig: OnPremComputeNodes: ConfigFile: 'slurm_nodes_on_prem.conf' CIDR: '10.1.0.0/16' slurm_nodes_on_prem.conf # # ON PREMISES COMPUTE NODES # # Config file with list of statically provisioned on-premises compute nodes that # are managed by this cluster. # # These nodes must be addressable on the network and firewalls must allow access on all ports # required by slurm. # # The compute nodes must have mounts that mirror the compute cluster including mounting the slurm file system # or a mirror of it. NodeName=Default State=DOWN NodeName=onprem-c7-x86-t3-2xl-0 NodeAddr=onprem-c7-x86-t3-2xl-0.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-1 NodeAddr=onprem-c7-x86-t3-2xl-1.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-2 NodeAddr=onprem-c7-x86-t3-2xl-2.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-3 NodeAddr=onprem-c7-x86-t3-2xl-3.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-4 NodeAddr=onprem-c7-x86-t3-2xl-4.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-5 NodeAddr=onprem-c7-x86-t3-2xl-5.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-6 NodeAddr=onprem-c7-x86-t3-2xl-6.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-7 NodeAddr=onprem-c7-x86-t3-2xl-7.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-8 NodeAddr=onprem-c7-x86-t3-2xl-8.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-9 NodeAddr=onprem-c7-x86-t3-2xl-9.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 # # # OnPrem Partition # # The is the default partition and includes all nodes from the 1st OS. # PartitionName=onprem Default=YES PriorityTier=20000 Nodes=\\ onprem-c7-x86-t3-2xl-[0-9] # # Always on partitions # SuspendExcParts=onprem Simulating an On-Premises Network Using AWS Create a new VPC with public and private subnets and NAT gateways. To simulate the latency between an AWS region and on-prem you can create the VPC in a different region in your account. The CIDR must not overlap with the Slurm VPC. Create a VPC peering connection to your Slurm VPC and accept the connection in the Slurm VPC. Create routes in the private subnets for the CIDR of the peered VPC and route it to the vpc peering connection. Add the on-prem VPC to the Slurm VPC's Route53 private local zone. Create a Route53 private hosted zone for the on-prem compute nodes and add it to the onprem VPC and the slurm VPC so that onprem compute nodes can be resolved. Copy the Slurm AMIs to the region of the on-prem VPC. Create an instance using the copied AMI. Connect to the instance and confirm that the mount points mounted correctly. You will probably have to change the DNS names for the file systems to IP addresses. I created A records in the Route53 zone for the file systems so that if the IP addresses ever change in the future I can easily update them in one place without having to create a new AMI or updated any instances. Create a new AMI from the instance. Create compute node instances from the new AMI and run the following commands on them get the slurmd daemon running so they can join the slurm cluster. # Instance specific variables hostname=onprem-c7-x86-t3-2xl-0 # Domain specific variables onprem_domain=onprem.com source /etc/profile.d/instance_vars.sh # munge needs to be running before calling scontrol /usr/bin/cp /opt/slurm/$ClusterName/config/munge.key /etc/munge/munge.key systemctl enable munged systemctl start munged ipaddress=$(hostname -I) $SLURM_ROOT/bin/scontrol update nodename=${hostname} nodeaddr=$ipaddress # Set hostname hostname_fqdn=${hostname}.${onprem_domain} if [ $(hostname) != $hostname_fqdn ]; then hostnamectl --static set-hostname $hostname_fqdn hostnamectl --pretty set-hostname $hostname fi if [ -e /opt/slurm/${ClusterName}/config/users_groups.json ] && [ -e /opt/slurm/${ClusterName}/bin/create_users_groups.py ]; then /opt/slurm/${ClusterName}/bin/create_users_groups.py -i /opt/slurm/${ClusterName}/config/users_groups.json fi # Create directory for slurmd.log logs_dir=/opt/slurm/${ClusterName}/logs/nodes/${hostname} if [[ ! -d $logs_dir ]]; then mkdir -p $logs_dir fi if [[ -e /var/log/slurm ]]; then rm -rf /var/log/slurm fi ln -s $logs_dir /var/log/slurm systemctl enable slurmd systemctl start slurmd # Restart so that log file goes to file system systemctl restart spot_monitor","title":"On-Premises Integration"},{"location":"onprem/#on-premises-integration","text":"The Slurm cluster can also be configured to manage on-premises compute nodes. The user must configure the on-premises compute nodes and then give the configuration information.","title":"On-Premises Integration"},{"location":"onprem/#network-requirements","text":"The on-prem network must have a CIDR range that doesn't overlap the Slurm cluster's VPC and the two networks need to be connected using VPN or AWS Direct Connect. The on-prem firewall must allow ingress and egress from the VPC. The ports are used to connect to the file systems, slurm controllers, and allow traffic between virtual desktops and compute nodes.","title":"Network Requirements"},{"location":"onprem/#dns-requirements","text":"Local network DNS must have an entry for the slurm controller or have a forwarding rule to the AWS provided DNS in the Slurm VPC.","title":"DNS Requirements"},{"location":"onprem/#file-system-requirements","text":"All of the compute nodes in the cluster, including the on-prem nodes, must have file system mounts that replicate the same directory structure. This can involve mounting filesystems across VPN or Direct Connect or synchronizing file systems using tools like rsync or NetApp FlexCache or SnapMirror. Performance will dictate the architecture of the file system. The onprem compute nodes must mount the Slurm controller's NFS export so that they have access to the Slurm binaries and configuration file. They must then be configured to run slurmd so that they can be managed by Slurm.","title":"File System Requirements"},{"location":"onprem/#slurm-configuration-of-on-premises-compute-nodes","text":"The slurm cluster's configuration file allows the configuration of on-premises compute nodes. The Slurm cluster will not provision any of the on-prem nodes, network, or firewall, but it will configure the cluster's resources to be used by the on-prem nodes. All that needs to be configured are the configuration file for the on-prem nodes and the CIDR block. InstanceConfig: OnPremComputeNodes: ConfigFile: 'slurm_nodes_on_prem.conf' CIDR: '10.1.0.0/16' slurm_nodes_on_prem.conf # # ON PREMISES COMPUTE NODES # # Config file with list of statically provisioned on-premises compute nodes that # are managed by this cluster. # # These nodes must be addressable on the network and firewalls must allow access on all ports # required by slurm. # # The compute nodes must have mounts that mirror the compute cluster including mounting the slurm file system # or a mirror of it. NodeName=Default State=DOWN NodeName=onprem-c7-x86-t3-2xl-0 NodeAddr=onprem-c7-x86-t3-2xl-0.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-1 NodeAddr=onprem-c7-x86-t3-2xl-1.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-2 NodeAddr=onprem-c7-x86-t3-2xl-2.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-3 NodeAddr=onprem-c7-x86-t3-2xl-3.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-4 NodeAddr=onprem-c7-x86-t3-2xl-4.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-5 NodeAddr=onprem-c7-x86-t3-2xl-5.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-6 NodeAddr=onprem-c7-x86-t3-2xl-6.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-7 NodeAddr=onprem-c7-x86-t3-2xl-7.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-8 NodeAddr=onprem-c7-x86-t3-2xl-8.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 NodeName=onprem-c7-x86-t3-2xl-9 NodeAddr=onprem-c7-x86-t3-2xl-9.onprem.com CPUs=4 RealMemory=30512 Feature=c7,CentOS_7_x86_64,x86_64,GHz:2.5 Weight=1 # # # OnPrem Partition # # The is the default partition and includes all nodes from the 1st OS. # PartitionName=onprem Default=YES PriorityTier=20000 Nodes=\\ onprem-c7-x86-t3-2xl-[0-9] # # Always on partitions # SuspendExcParts=onprem","title":"Slurm Configuration of On-Premises Compute Nodes"},{"location":"onprem/#simulating-an-on-premises-network-using-aws","text":"Create a new VPC with public and private subnets and NAT gateways. To simulate the latency between an AWS region and on-prem you can create the VPC in a different region in your account. The CIDR must not overlap with the Slurm VPC. Create a VPC peering connection to your Slurm VPC and accept the connection in the Slurm VPC. Create routes in the private subnets for the CIDR of the peered VPC and route it to the vpc peering connection. Add the on-prem VPC to the Slurm VPC's Route53 private local zone. Create a Route53 private hosted zone for the on-prem compute nodes and add it to the onprem VPC and the slurm VPC so that onprem compute nodes can be resolved. Copy the Slurm AMIs to the region of the on-prem VPC. Create an instance using the copied AMI. Connect to the instance and confirm that the mount points mounted correctly. You will probably have to change the DNS names for the file systems to IP addresses. I created A records in the Route53 zone for the file systems so that if the IP addresses ever change in the future I can easily update them in one place without having to create a new AMI or updated any instances. Create a new AMI from the instance. Create compute node instances from the new AMI and run the following commands on them get the slurmd daemon running so they can join the slurm cluster. # Instance specific variables hostname=onprem-c7-x86-t3-2xl-0 # Domain specific variables onprem_domain=onprem.com source /etc/profile.d/instance_vars.sh # munge needs to be running before calling scontrol /usr/bin/cp /opt/slurm/$ClusterName/config/munge.key /etc/munge/munge.key systemctl enable munged systemctl start munged ipaddress=$(hostname -I) $SLURM_ROOT/bin/scontrol update nodename=${hostname} nodeaddr=$ipaddress # Set hostname hostname_fqdn=${hostname}.${onprem_domain} if [ $(hostname) != $hostname_fqdn ]; then hostnamectl --static set-hostname $hostname_fqdn hostnamectl --pretty set-hostname $hostname fi if [ -e /opt/slurm/${ClusterName}/config/users_groups.json ] && [ -e /opt/slurm/${ClusterName}/bin/create_users_groups.py ]; then /opt/slurm/${ClusterName}/bin/create_users_groups.py -i /opt/slurm/${ClusterName}/config/users_groups.json fi # Create directory for slurmd.log logs_dir=/opt/slurm/${ClusterName}/logs/nodes/${hostname} if [[ ! -d $logs_dir ]]; then mkdir -p $logs_dir fi if [[ -e /var/log/slurm ]]; then rm -rf /var/log/slurm fi ln -s $logs_dir /var/log/slurm systemctl enable slurmd systemctl start slurmd # Restart so that log file goes to file system systemctl restart spot_monitor","title":"Simulating an On-Premises Network Using AWS"},{"location":"res_integration/","text":"RES Integration First you will need to deploy RES. The easiest way is to deploy the demo environment which provides all of the prerequisites and completely automates the deployment. If you want to use an existing VPC or Active Directory, then you will need to follow the instructions to deploy the product . RES Setup After you've deployed RES, you need to configure it so that the remote desktops can be used as external login nodes and so that they have access to any file systems that you created. Onboard your file systems RES natively supports EFS, FSx for NetApp Ontap, and FSx for Lustre file systems. It can create them for you or you can onboard existing file systems. Expand Environment Management Click File Systems Click Onboard File System or Create File System Create RES Project Expand Environment Management Click Projects Click Create Project Fill in the required fields. Add any file systems that you created so they will be automatically mounted on the desktops that belong to the project. Expand Advanced Options under Resource Configurations and add the SlurmLoginNodeSG so that it will be attached automatically to the remote desktop so they can access the external file systems and slurm clusters. Add the groups and users that can use the project. Give the project access to software stacks Next, you'll need to give the project access to a Software Stack. You can either create a new Software Stack or update an existing one. Select Software Stacks under Session Management . Select an existing stack like the RHEL 8 stack Select Actions , Edit Stack . Select your project under Projects and enable it to use the stack. Create virtual desktop Now you can create a virtual desktop using the project that you just created. Select My Virtual Desktops under Desktops . Click Launch New Virtual Desktop Give it a descriptive name, select the project, operating system, and software stack. I suggest using a t3 instance for virtual desktops, such as a t3.large. If you need more cores or memory you will use your ParallelCluster compute nodes. I usually increase the storage size to 20GB so I can install additional packages. Click Submit and then wait for the desktop to be provisioned. You may need to refresh the page to update the desktop status. You can switch to the EC2 console to verify that the instance has been launched and that it has the required security group attached. ParallelCluster Configurattion Integration with Research and Engineering Studion (RES) is straightforward. You simply specify the --RESStackName option for the install.sh script or add the RESStackName configuration parameter to your configuration file. The install script will set the following configuration parameters based on your RES environment or check them if you have them set to make sure they are consistent with your RES environment. The intention is to completely automate the deployment of ParallelCluster and set up the RES environment so that it can easily be used. Parameter Description Value VpcId VPC id for the RES cluster vpc-xxxxxx SubnetId Subnet in the RES VPC. subnet-xxxxx slurm/ExternalLoginNodes Information of instances to be configured as external login nodes slurm/DomainJoinedInstance Tags of cluster-manager which will be used to create users_groups.json slurm/storage/ExtraMounts The mount parameters for the /home directory. This is required for access to the home directory. slurm/SlurmCtl/AdditionalSecurityGroups Security group that allows access to EFS /home slurm/InstanceConfig/AdditionalSecurityGroups Security group that allows access to EFS /home You must also create security groups as described in Security Groups for Login Nodes . You must either specify AdditionalSecurityGroupsStackName or specify the SlurmHeadNodeSG in the slurm/SlurmCtl/AdditionalSecurityGroups parameter and the SlurmComputeNodeSG in the slurm/InstanceConfig/AdditionalSecurityGroups parameter. When you specify RESStackName , a lambda function will run SSM commands to create a cron job on a RES domain joined instance to update the users_groups.json file every hour. Another lambda function will also automatically configure all running VDI hosts to use the cluster. The following example shows the configuration parameters for a RES cluster with a stack named res-eda. --- #==================================================================== # EDA Slurm cluster for RES using ParallelCluster # # Defaults and valid configuration options are in source/config_schema.py. # Command line values override values in the config file. #==================================================================== StackName: res-eda-pc-3-9-1-rhel8-x86-config Region: <region> SshKeyPair: <key-name> AdditionalSecurityGroupsStackName: res-eda-SlurmSecurityGroups RESStackName: res-eda ErrorSnsTopicArn: <topic-arn> TimeZone: 'US/Central' slurm: ParallelClusterConfig: Version: '3.10.1' Image: Os: 'rhel8' Architecture: 'x86_64' Slurmdbd: SlurmdbdStackName: pcluster-slurm-dbd-res-eda-3-10-1 SlurmCtl: {} # Configure typical EDA instance types # A partition will be created for each combination of Base OS, Architecture, and Spot InstanceConfig: UseSpot: true NodeCounts: DefaultMaxCount: 10 Connect to the virtual desktop When the cluster deployment finishes you are ready to run jobs from your RES DCV desktop. Create custom AMI for virtual desktops Connect to your virtual desktop and install packages, software, configure ParallelCluster clusters, mount file systems, and whatever else you need for your project. You'll normally require root access to do this. When you are done, remove the following files or else new virtual desktops created from the image will fail to provision. rm /root/bootstrap/semaphore/*.lock","title":"RES Integration"},{"location":"res_integration/#res-integration","text":"First you will need to deploy RES. The easiest way is to deploy the demo environment which provides all of the prerequisites and completely automates the deployment. If you want to use an existing VPC or Active Directory, then you will need to follow the instructions to deploy the product .","title":"RES Integration"},{"location":"res_integration/#res-setup","text":"After you've deployed RES, you need to configure it so that the remote desktops can be used as external login nodes and so that they have access to any file systems that you created.","title":"RES Setup"},{"location":"res_integration/#onboard-your-file-systems","text":"RES natively supports EFS, FSx for NetApp Ontap, and FSx for Lustre file systems. It can create them for you or you can onboard existing file systems. Expand Environment Management Click File Systems Click Onboard File System or Create File System","title":"Onboard your file systems"},{"location":"res_integration/#create-res-project","text":"Expand Environment Management Click Projects Click Create Project Fill in the required fields. Add any file systems that you created so they will be automatically mounted on the desktops that belong to the project. Expand Advanced Options under Resource Configurations and add the SlurmLoginNodeSG so that it will be attached automatically to the remote desktop so they can access the external file systems and slurm clusters. Add the groups and users that can use the project.","title":"Create RES Project"},{"location":"res_integration/#give-the-project-access-to-software-stacks","text":"Next, you'll need to give the project access to a Software Stack. You can either create a new Software Stack or update an existing one. Select Software Stacks under Session Management . Select an existing stack like the RHEL 8 stack Select Actions , Edit Stack . Select your project under Projects and enable it to use the stack.","title":"Give the project access to software stacks"},{"location":"res_integration/#create-virtual-desktop","text":"Now you can create a virtual desktop using the project that you just created. Select My Virtual Desktops under Desktops . Click Launch New Virtual Desktop Give it a descriptive name, select the project, operating system, and software stack. I suggest using a t3 instance for virtual desktops, such as a t3.large. If you need more cores or memory you will use your ParallelCluster compute nodes. I usually increase the storage size to 20GB so I can install additional packages. Click Submit and then wait for the desktop to be provisioned. You may need to refresh the page to update the desktop status. You can switch to the EC2 console to verify that the instance has been launched and that it has the required security group attached.","title":"Create virtual desktop"},{"location":"res_integration/#parallelcluster-configurattion","text":"Integration with Research and Engineering Studion (RES) is straightforward. You simply specify the --RESStackName option for the install.sh script or add the RESStackName configuration parameter to your configuration file. The install script will set the following configuration parameters based on your RES environment or check them if you have them set to make sure they are consistent with your RES environment. The intention is to completely automate the deployment of ParallelCluster and set up the RES environment so that it can easily be used. Parameter Description Value VpcId VPC id for the RES cluster vpc-xxxxxx SubnetId Subnet in the RES VPC. subnet-xxxxx slurm/ExternalLoginNodes Information of instances to be configured as external login nodes slurm/DomainJoinedInstance Tags of cluster-manager which will be used to create users_groups.json slurm/storage/ExtraMounts The mount parameters for the /home directory. This is required for access to the home directory. slurm/SlurmCtl/AdditionalSecurityGroups Security group that allows access to EFS /home slurm/InstanceConfig/AdditionalSecurityGroups Security group that allows access to EFS /home You must also create security groups as described in Security Groups for Login Nodes . You must either specify AdditionalSecurityGroupsStackName or specify the SlurmHeadNodeSG in the slurm/SlurmCtl/AdditionalSecurityGroups parameter and the SlurmComputeNodeSG in the slurm/InstanceConfig/AdditionalSecurityGroups parameter. When you specify RESStackName , a lambda function will run SSM commands to create a cron job on a RES domain joined instance to update the users_groups.json file every hour. Another lambda function will also automatically configure all running VDI hosts to use the cluster. The following example shows the configuration parameters for a RES cluster with a stack named res-eda. --- #==================================================================== # EDA Slurm cluster for RES using ParallelCluster # # Defaults and valid configuration options are in source/config_schema.py. # Command line values override values in the config file. #==================================================================== StackName: res-eda-pc-3-9-1-rhel8-x86-config Region: <region> SshKeyPair: <key-name> AdditionalSecurityGroupsStackName: res-eda-SlurmSecurityGroups RESStackName: res-eda ErrorSnsTopicArn: <topic-arn> TimeZone: 'US/Central' slurm: ParallelClusterConfig: Version: '3.10.1' Image: Os: 'rhel8' Architecture: 'x86_64' Slurmdbd: SlurmdbdStackName: pcluster-slurm-dbd-res-eda-3-10-1 SlurmCtl: {} # Configure typical EDA instance types # A partition will be created for each combination of Base OS, Architecture, and Spot InstanceConfig: UseSpot: true NodeCounts: DefaultMaxCount: 10","title":"ParallelCluster Configurattion"},{"location":"res_integration/#connect-to-the-virtual-desktop","text":"When the cluster deployment finishes you are ready to run jobs from your RES DCV desktop.","title":"Connect to the virtual desktop"},{"location":"res_integration/#create-custom-ami-for-virtual-desktops","text":"Connect to your virtual desktop and install packages, software, configure ParallelCluster clusters, mount file systems, and whatever else you need for your project. You'll normally require root access to do this. When you are done, remove the following files or else new virtual desktops created from the image will fail to provision. rm /root/bootstrap/semaphore/*.lock","title":"Create custom AMI for virtual desktops"},{"location":"rest_api/","text":"Slurm REST API The Slurm REST API give a programmatic way to access the features of Slurm. The REST API can be used, for example, to use a Lambda function to submit jobs to the Slurm cluster. How to use the REST API The following shows how to run a simple REST call. source /opt/slurm/{{ClusterName}}/config/slurm_config.sh unset SLURM_JWT . <(scontrol token) wget --header \"X-SLURM-USER-TOKEN: $SLURM_JWT\" --header \"X-SLURM-USER-NAME: $USER\" -q $SLURMRESTD_URL/slurm/v0.0.38/diag/ -O - The REST API is documented at https://slurm.schedmd.com/rest_api.html . The token returned by scontrol token has a default lifetime of 3600 seconds (1 hour). For automation, a cron job on the Slurm controller creates a new token for the root and slurmrestd users every 30 minutes and stores them in SSM Parameter Store at /{{ClusterName}}/slurmrestd/jwt/{{user_name}} . These tokens can be used by automations such as a Lambda function to access the REST API. An example Lambda function called {{ClusterName}}-CallSlurmRestApiLambda shows how to call various API functions. You can use this as a template to write functions that use your Slurm cluster for automations.","title":"Slurm REST API"},{"location":"rest_api/#slurm-rest-api","text":"The Slurm REST API give a programmatic way to access the features of Slurm. The REST API can be used, for example, to use a Lambda function to submit jobs to the Slurm cluster.","title":"Slurm REST API"},{"location":"rest_api/#how-to-use-the-rest-api","text":"The following shows how to run a simple REST call. source /opt/slurm/{{ClusterName}}/config/slurm_config.sh unset SLURM_JWT . <(scontrol token) wget --header \"X-SLURM-USER-TOKEN: $SLURM_JWT\" --header \"X-SLURM-USER-NAME: $USER\" -q $SLURMRESTD_URL/slurm/v0.0.38/diag/ -O - The REST API is documented at https://slurm.schedmd.com/rest_api.html . The token returned by scontrol token has a default lifetime of 3600 seconds (1 hour). For automation, a cron job on the Slurm controller creates a new token for the root and slurmrestd users every 30 minutes and stores them in SSM Parameter Store at /{{ClusterName}}/slurmrestd/jwt/{{user_name}} . These tokens can be used by automations such as a Lambda function to access the REST API. An example Lambda function called {{ClusterName}}-CallSlurmRestApiLambda shows how to call various API functions. You can use this as a template to write functions that use your Slurm cluster for automations.","title":"How to use the REST API"},{"location":"run_jobs/","text":"Run Jobs This page is to give some basic instructions on how to run and monitor jobs on Slurm. Slurm provides excellent man pages for all of its commands, so if you have questions refer to the man pages. Set Up Load the environment module for Slurm to configure your PATH and Slurm related environment variables. module load {{ClusterName}} The modulefile sets environment variables that control the defaults for Slurm commands. These are documented in the man pages for each command. If you don't like the defaults then you can set them in your environment (for example, your .bashrc) and the modulefile won't change any variables that are already set. The environment variables can always be overridden by the command line options. For example, the SQUEUE_FORMAT2 and SQUEUE_SORT environment variables are set so that the default output format is easier to read and contains useful information that isn't in the default format. Key Slurm Commands The key Slurm commands are Command Description Example salloc Create a compute allocation. salloc -c 1 --mem 1G -C 'spot&GHz:3.1' srun Run a job within an allocation. srun --pty bin/bash sbatch Submit a batch script sbatch -c 1 --mem 1G -C 'spot&GHz:3.1' script squeue Get job status scancel Cancel a job scancel jobid sinfo Get info about Slurm node status sinfo -p all scontrol view or modify Slurm configuration and state scontrol show node nodename sstat Display various status information about a running job/step sshare Tool for listing fair share information sprio View the factors that comprise a job's scheduling priority sacct Display accounting data for jobs sreport Generate reports from the Slurm accounting data. sview Graphical tool for viewing cluster state sbatch The most common options for sbatch are listed here. For more details run man sbatch . Options Description Default -p, --partition= partition-names Select the partition/partitions to run job on. Set by slurm.InstanceConfig.DefaultPartition in config file. -t, --time= time Set a limit on total run time of the job. SBATCH_TIMELIMIT=\"1:0:0\" (1 hour) -c, --cpus-per-task= ncpus Number of cores. Default is 1. --mem= size[units] Amount of memory. Default unit is M. Valid units are [K|M|G|T]. SBATCH_MEM_PER_NODE=100M -L, --licenses= license Licenses used by the job. -a, --array= indexes Submit job array -C, --constraint= list Features required by the job. Multiple constraints can be specified with AND(&) and OR( ). -d, --dependency= dependency-list Don't start the job until the dependencies have been completed. -D, --chdir= directory Set the working directory of the job --wait Do not exit until the job finishes, Exit code of sbatch will be the same as the exit code of the job. --wrap Wrap shell commands in a batch script. Run a simulation build followed by a regression build_jobid=$(sbatch -c 4 --mem 4G -L vcs_build -C 'GHz:4|GHz:4.5' -t 30:0 sim-build.sh) if sbatch -d \"afterok:$build_jobid\" -c 1 --mem 100M --wait submit-regression.sh; then echo \"Regression Passed\" else echo \"Regression Failed\" fi srun The srun is usually used to open a pseudo terminal on a compute node for you to run interactive jobs. It accepts most of the same options as sbatch to request cpus, memory, and node features. To open up a pseudo terminal in your shell on a compute node with 4 cores and 16G of memory, execute the following command. srun -c 4 --mem 8G --pty /bin/bash This will queue a job and when it is allocated to a node and the node runs, the job control will be returned to your shell, but stdin and stdout will be on the compute node. If you set your DISPLAY environment variable and allow external X11 connections you can use this to run interactive GUI jobs on the compute node and have the windows on your instance. xhost + export DISPLAY=$(hostname):$(echo $DISPLAY | cut -d ':' -f 2) srun -c 4 --mem 8G --pty /bin/bash emacs . # Or whatever gui application you want to run. Should open a window. Another way to run interactive GUI jobs is to use srun 's --x11 flag to enable X11 forwarding. srun -c 1 --mem 8G --pty --x11 emacs squeue The squeue command shows the status of jobs. The output format can be customized using the --format or --Format options and you can configure the default output format using the corresponding SQUEUE_FORMAT or SQUEUE_FORMAT2 environment variables. squeue sprio Use sprio to get information about a job's priority. This can be useful to figure out why a job is scheduled before or after another job. sprio -j10,11 sacct Display accounting information about jobs. For example, it can be used to get the requested CPU and memory and see the CPU time and memory actually used. sacct -o JobID,User,JobName,AllocCPUS,State,ExitCode,Elapsed,CPUTime,MaxRSS,MaxVMSize,ReqCPUS,ReqMem,SystemCPU,TotalCPU,UserCPU -j 44 This shows more details. sacct --allclusters --allusers --federation --starttime 1970-01-01 --format 'Submit,Start,End,jobid%15,State%15,user,account,cluster%15,AllocCPUS,AllocNodes,ExitCode,ReqMem,MaxRSS,MaxVMSize,MaxPages,Elapsed,CPUTime,UserCPU,SystemCPU,TotalCPU' | less For more information: man sacct sreport The sreport command can be used to generate report from the Slurm database. Other Slurm Commands Use man command to get information about these less commonly used Slurm commands. Command Description sacctmgr View/modify Slurm account information sattach Attach to a job step sbcast Transmit a file to the nodes allocated to a Slurm job. scrontab Manage slurm crontab files sdiag Diagnostic tool for Slurm. Shows information related to slurmctld execution. seff sgather Transmit a file from the nodes allocated to a Slurm job. sh5util Tool for merging HDF5 files from the acct_gather_profile plugin that gathers detailed data for jobs. sjobexitmod Modify derived exit code of a job strigger Set, get, or clear Slurm trigger information","title":"Run Jobs"},{"location":"run_jobs/#run-jobs","text":"This page is to give some basic instructions on how to run and monitor jobs on Slurm. Slurm provides excellent man pages for all of its commands, so if you have questions refer to the man pages.","title":"Run Jobs"},{"location":"run_jobs/#set-up","text":"Load the environment module for Slurm to configure your PATH and Slurm related environment variables. module load {{ClusterName}} The modulefile sets environment variables that control the defaults for Slurm commands. These are documented in the man pages for each command. If you don't like the defaults then you can set them in your environment (for example, your .bashrc) and the modulefile won't change any variables that are already set. The environment variables can always be overridden by the command line options. For example, the SQUEUE_FORMAT2 and SQUEUE_SORT environment variables are set so that the default output format is easier to read and contains useful information that isn't in the default format.","title":"Set Up"},{"location":"run_jobs/#key-slurm-commands","text":"The key Slurm commands are Command Description Example salloc Create a compute allocation. salloc -c 1 --mem 1G -C 'spot&GHz:3.1' srun Run a job within an allocation. srun --pty bin/bash sbatch Submit a batch script sbatch -c 1 --mem 1G -C 'spot&GHz:3.1' script squeue Get job status scancel Cancel a job scancel jobid sinfo Get info about Slurm node status sinfo -p all scontrol view or modify Slurm configuration and state scontrol show node nodename sstat Display various status information about a running job/step sshare Tool for listing fair share information sprio View the factors that comprise a job's scheduling priority sacct Display accounting data for jobs sreport Generate reports from the Slurm accounting data. sview Graphical tool for viewing cluster state","title":"Key Slurm Commands"},{"location":"run_jobs/#sbatch","text":"The most common options for sbatch are listed here. For more details run man sbatch . Options Description Default -p, --partition= partition-names Select the partition/partitions to run job on. Set by slurm.InstanceConfig.DefaultPartition in config file. -t, --time= time Set a limit on total run time of the job. SBATCH_TIMELIMIT=\"1:0:0\" (1 hour) -c, --cpus-per-task= ncpus Number of cores. Default is 1. --mem= size[units] Amount of memory. Default unit is M. Valid units are [K|M|G|T]. SBATCH_MEM_PER_NODE=100M -L, --licenses= license Licenses used by the job. -a, --array= indexes Submit job array -C, --constraint= list Features required by the job. Multiple constraints can be specified with AND(&) and OR( ). -d, --dependency= dependency-list Don't start the job until the dependencies have been completed. -D, --chdir= directory Set the working directory of the job --wait Do not exit until the job finishes, Exit code of sbatch will be the same as the exit code of the job. --wrap Wrap shell commands in a batch script.","title":"sbatch"},{"location":"run_jobs/#run-a-simulation-build-followed-by-a-regression","text":"build_jobid=$(sbatch -c 4 --mem 4G -L vcs_build -C 'GHz:4|GHz:4.5' -t 30:0 sim-build.sh) if sbatch -d \"afterok:$build_jobid\" -c 1 --mem 100M --wait submit-regression.sh; then echo \"Regression Passed\" else echo \"Regression Failed\" fi","title":"Run a simulation build followed by a regression"},{"location":"run_jobs/#srun","text":"The srun is usually used to open a pseudo terminal on a compute node for you to run interactive jobs. It accepts most of the same options as sbatch to request cpus, memory, and node features. To open up a pseudo terminal in your shell on a compute node with 4 cores and 16G of memory, execute the following command. srun -c 4 --mem 8G --pty /bin/bash This will queue a job and when it is allocated to a node and the node runs, the job control will be returned to your shell, but stdin and stdout will be on the compute node. If you set your DISPLAY environment variable and allow external X11 connections you can use this to run interactive GUI jobs on the compute node and have the windows on your instance. xhost + export DISPLAY=$(hostname):$(echo $DISPLAY | cut -d ':' -f 2) srun -c 4 --mem 8G --pty /bin/bash emacs . # Or whatever gui application you want to run. Should open a window. Another way to run interactive GUI jobs is to use srun 's --x11 flag to enable X11 forwarding. srun -c 1 --mem 8G --pty --x11 emacs","title":"srun"},{"location":"run_jobs/#squeue","text":"The squeue command shows the status of jobs. The output format can be customized using the --format or --Format options and you can configure the default output format using the corresponding SQUEUE_FORMAT or SQUEUE_FORMAT2 environment variables. squeue","title":"squeue"},{"location":"run_jobs/#sprio","text":"Use sprio to get information about a job's priority. This can be useful to figure out why a job is scheduled before or after another job. sprio -j10,11","title":"sprio"},{"location":"run_jobs/#sacct","text":"Display accounting information about jobs. For example, it can be used to get the requested CPU and memory and see the CPU time and memory actually used. sacct -o JobID,User,JobName,AllocCPUS,State,ExitCode,Elapsed,CPUTime,MaxRSS,MaxVMSize,ReqCPUS,ReqMem,SystemCPU,TotalCPU,UserCPU -j 44 This shows more details. sacct --allclusters --allusers --federation --starttime 1970-01-01 --format 'Submit,Start,End,jobid%15,State%15,user,account,cluster%15,AllocCPUS,AllocNodes,ExitCode,ReqMem,MaxRSS,MaxVMSize,MaxPages,Elapsed,CPUTime,UserCPU,SystemCPU,TotalCPU' | less For more information: man sacct","title":"sacct"},{"location":"run_jobs/#sreport","text":"The sreport command can be used to generate report from the Slurm database.","title":"sreport"},{"location":"run_jobs/#other-slurm-commands","text":"Use man command to get information about these less commonly used Slurm commands. Command Description sacctmgr View/modify Slurm account information sattach Attach to a job step sbcast Transmit a file to the nodes allocated to a Slurm job. scrontab Manage slurm crontab files sdiag Diagnostic tool for Slurm. Shows information related to slurmctld execution. seff sgather Transmit a file from the nodes allocated to a Slurm job. sh5util Tool for merging HDF5 files from the acct_gather_profile plugin that gathers detailed data for jobs. sjobexitmod Modify derived exit code of a job strigger Set, get, or clear Slurm trigger information","title":"Other Slurm Commands"},{"location":"security-groups/","text":"Security Groups This page documents the configuration of security groups that will be used by your clusters. Note : This process has been automated and is described on the deployment prerequisites page . You can refer to this page to understand the security groups that are created or if you choose to manually create the security groups yourself. Security Groups for Login Nodes We call instances that can connect to the Slurm cluster a login node. Login nodes can be used to submit and manage jobs. ParallelCluster can be configured to create login nodes that you must SSH into to use the cluster. If you want to allow instances like remote desktops to use the cluster directly, you must configure them as login nodes and give them network permissions to connect to the cluster instances. You must create three security groups that allow connections between the login node, the Slurm head node, and the Slurm compute nodes. You will also need to know the security group id for your external Slurmdbd instance, if you have one. Security Group Name Description SlurmLoginNodeSG Security group that must be attached to login nodes SlurmHeadNodeSG Additional security group for head node SlurmComputeNodeSG Additional security group for compute nodes SlurmdbdSG (Optional) Existing Slurmdbd security group First create these security groups without any security group rules. The reason for this is that the security group rules reference the other security groups so the groups must all exist before any of the rules can be created. After you have created the security groups then create the rules as described below. Slurm Login Node Security Group The LoginNodeSG will be attached to your login nodes, such as your virtual desktops. NOTE : To make this available to Research and Engineering Studio (RES) so that it can be automatically assigned to virtual desktops, you need to add a tag named res:Resource with a value of vdi-security-group . When you create a project, you can select this security group to be added to virtual desktops that use the project. It needs at least the following inbound rules: Type Port range Source Description Details TCP 1024-65535 SlurmHeadNodeSG SlurmHeadNode ephemeral Head node can use ephemeral ports to connect to the login node TCP 1024-65535 SlurmComputeNodeSG SlurmComputeNode ephemeral Compute node will connect to login node using ephemeral ports to manage interactive shells TCP 6000-7024 SlurmComputeNodeSG SlurmComputeNode X11 Compute node can send X11 traffic to login node for GUI applications It needs the following outbound rules. Type Port range Destination Description Details TCP 2049 SlurmHeadNodeSG SlurmHeadNode NFS Mount the slurm NFS file system with binaries and config TCP 6818 SlurmComputeNodeSG SlurmComputeNode slurmd Connect to compute node for interactive jobs TCP 6819 SlurmHeadNodeSG SlurmHeadNode slurmdbd Connect to slurmdbd (accounting database) daemon on head node for versions before 3.10.0. TCP 6819 SlurmdbdSG Slurmdbd Connect to external Slurmdbd instance. For versions starting in 3.10.0. TCP 6820-6829 SlurmHeadNodeSG SlurmHeadNode slurmctld TCP 6830 SlurmHeadNodeSG SlurmHeadNode slurmrestd Slurm Head Node Security Group The SlurmHeadNodeSG will be specified in your configuration file for the slurm/SlurmCtl/AdditionalSecurityGroups parameter. It needs at least the following inbound rules: Type Port range Source Description TCP 2049 SlurmLoginNodeSG SlurmLoginNode NFS TCP 6819 SlurmLoginNodeSG SlurmLoginNode slurmdbd. If not using external Slurmdbd. TCP 6820-6829 SlurmLoginNodeSG SlurmLoginNode slurmctld TCP 6830 SlurmLoginNodeSG SlurmLoginNode slurmrestd It needs the following outbound rules. Type Port range Destination Description TCP 1024-65535 SlurmLoginNodeSG SlurmLoginNode ephemeral Slurm Compute Node Security Group The SlurmComputeNodeSG will be specified in your configuration file for the slurm/InstanceConfig/AdditionalSecurityGroups parameter. It needs at least the following inbound rules: Type Port range Source Description TCP 6818 SlurmLoginNodeSG SlurmLoginNode slurmd It needs the following outbound rules. Type Port range Destination Description TCP 2049 SlurmHeadNodeSG SlurmHeadNode NFS TCP 1024-65535 SlurmLoginNodeSG SlurmLoginNode ephemeral TCP 6000-7024 SlurmLoginNodeSG SlurmLoginNode X11 External Slurmdbd Security Group Note : ParallelCluster 3.10.0 added support for an external Slurmdbd instance. The login node must be able to directly access the Slurmdbd instance on port 6819 when running commands like sacctmgr . You must edit the inbound rules of the Slurmdbd instance's security group to allow the access. Add the following inbound rule. Type Port range Source Description TCP 6819 SlurmLoginNodeSG SlurmLoginNode slurmdbd Security Groups for File Systems You will usually have externally created file systems that should be mounted on the compute nodes and login nodes. You will need to define security groups for the file system network interfaces and modify the Slurm security groups to give them access to the file systems. FSx for Lustre Security Group We'll refer to this group as FSxLustreSG, but you can name it whatever you want. This security group can either be provided when the file system is created, or can be attached to the network interfaces of the file system after it is created. The required security group rules are documented in the FSx documentation . It needs the following inbound rules. Type Port range Source Description Details TCP 988 FSxLustreSG, SlurmHeadNodeSG, SlurmComputeNodeSG, SlurmLoginNodeSG Allows Lustre traffic between FSx for Lustre file servers and Lustre clients TCP 1018-1023 FSxLustreSG, SlurmHeadNodeSG, SlurmComputeNodeSG, SlurmLoginNodeSG Allows Lustre traffic between FSx for Lustre file servers and Lustre clients It needs the following outbound rules. Type Port range Destination Description Details TCP 988 FSxLustreSG, SlurmHeadNodeSG, SlurmComputeNodeSG, SlurmLoginNodeSG Allow Lustre traffic between FSx for Lustre file servers and Lustre clients TCP 1018-1023 FSxLustreSG, SlurmHeadNodeSG, SlurmComputeNodeSG, SlurmLoginNodeSG Allow Lustre traffic between FSx for Lustre file servers and Lustre clients The same inbound and outbound rules need to be added to all 3 of the Slurm security groups too. FSx for NetApp Ontap Security Group We'll refer to this group as FSxOntapSG, but you can name it whatever you want. This security group can either be provided when the file system is created, or can be attached to the network interfaces of the file system after it is created. All the security group rule are documented in the FSx documentation . The minimum set required for mounting the file system are documented below. It needs the following inbound rules. Type Port range Source Description Details TCP, UDP 111 SlurmHeadNodeSG, SlurmComputeNodeSG, SlurmLoginNodeSG Remote procedure call for NFS TCP, UDP 635 SlurmHeadNodeSG, SlurmComputeNodeSG, SlurmLoginNodeSG NFS mount TCP, UDP 2049 SlurmHeadNodeSG, SlurmComputeNodeSG, SlurmLoginNodeSG NFS server daemon TCP, UDP 4045 SlurmHeadNodeSG, SlurmComputeNodeSG, SlurmLoginNodeSG NFS lock daemon TCP, UDP 4046 SlurmHeadNodeSG, SlurmComputeNodeSG, SlurmLoginNodeSG Network status monitor for NFS It needs the following outbound rules. Type Port range Destination Description Details All All The Slurm security groups need to add the following outbound rule to allow mounting using NFS. Type Port range Destination Description Details TCP, UDP 111 FSxOntap Remote procedure call for NFS TCP, UDP 635 FSxOntap NFS mount TCP, UDP 2049 FSxOntap NFS server daemon TCP,UDP 4045 FSxOntap NFS lock daemon TCP, UDP 4046 FSxOntap Network status monitor for NFS FSx for OpenZFS Security Group We'll refer to this group as FSxZfsSG, but you can name it whatever you want. This security group can either be provided when the file system is created, or can be attached to the network interfaces of the file system after it is created. The required security group rule are documented in the FSx documentation . It needs the following inbound rules. Type Port range Source Description Details TCP, UDP 111 SlurmHeadNodeSG, SlurmComputeNodeSG, SlurmLoginNodeSG Remote procedure call for NFS TCP, UDP 2049 SlurmHeadNodeSG, SlurmComputeNodeSG, SlurmLoginNodeSG NFS server daemon TCP, UDP 20001-20003 SlurmHeadNodeSG, SlurmComputeNodeSG, SlurmLoginNodeSG NFS mount, status monitor, and lock daemon Remove all outbound rules. The Slurm security groups need to add the following outbound rule to allow mounting using NFS. Type Port range Destination Description Details TCP, UDP 111 FSxZfs Remote procedure call for NFS TCP, UDP 2049 FSxZfs NFS server daemon TCP, UDP 20001-20003 FSxZfs NFS mount, status monitor, and lock daemon","title":"Security Groups"},{"location":"security-groups/#security-groups","text":"This page documents the configuration of security groups that will be used by your clusters. Note : This process has been automated and is described on the deployment prerequisites page . You can refer to this page to understand the security groups that are created or if you choose to manually create the security groups yourself.","title":"Security Groups"},{"location":"security-groups/#security-groups-for-login-nodes","text":"We call instances that can connect to the Slurm cluster a login node. Login nodes can be used to submit and manage jobs. ParallelCluster can be configured to create login nodes that you must SSH into to use the cluster. If you want to allow instances like remote desktops to use the cluster directly, you must configure them as login nodes and give them network permissions to connect to the cluster instances. You must create three security groups that allow connections between the login node, the Slurm head node, and the Slurm compute nodes. You will also need to know the security group id for your external Slurmdbd instance, if you have one. Security Group Name Description SlurmLoginNodeSG Security group that must be attached to login nodes SlurmHeadNodeSG Additional security group for head node SlurmComputeNodeSG Additional security group for compute nodes SlurmdbdSG (Optional) Existing Slurmdbd security group First create these security groups without any security group rules. The reason for this is that the security group rules reference the other security groups so the groups must all exist before any of the rules can be created. After you have created the security groups then create the rules as described below.","title":"Security Groups for Login Nodes"},{"location":"security-groups/#slurm-login-node-security-group","text":"The LoginNodeSG will be attached to your login nodes, such as your virtual desktops. NOTE : To make this available to Research and Engineering Studio (RES) so that it can be automatically assigned to virtual desktops, you need to add a tag named res:Resource with a value of vdi-security-group . When you create a project, you can select this security group to be added to virtual desktops that use the project. It needs at least the following inbound rules: Type Port range Source Description Details TCP 1024-65535 SlurmHeadNodeSG SlurmHeadNode ephemeral Head node can use ephemeral ports to connect to the login node TCP 1024-65535 SlurmComputeNodeSG SlurmComputeNode ephemeral Compute node will connect to login node using ephemeral ports to manage interactive shells TCP 6000-7024 SlurmComputeNodeSG SlurmComputeNode X11 Compute node can send X11 traffic to login node for GUI applications It needs the following outbound rules. Type Port range Destination Description Details TCP 2049 SlurmHeadNodeSG SlurmHeadNode NFS Mount the slurm NFS file system with binaries and config TCP 6818 SlurmComputeNodeSG SlurmComputeNode slurmd Connect to compute node for interactive jobs TCP 6819 SlurmHeadNodeSG SlurmHeadNode slurmdbd Connect to slurmdbd (accounting database) daemon on head node for versions before 3.10.0. TCP 6819 SlurmdbdSG Slurmdbd Connect to external Slurmdbd instance. For versions starting in 3.10.0. TCP 6820-6829 SlurmHeadNodeSG SlurmHeadNode slurmctld TCP 6830 SlurmHeadNodeSG SlurmHeadNode slurmrestd","title":"Slurm Login Node Security Group"},{"location":"security-groups/#slurm-head-node-security-group","text":"The SlurmHeadNodeSG will be specified in your configuration file for the slurm/SlurmCtl/AdditionalSecurityGroups parameter. It needs at least the following inbound rules: Type Port range Source Description TCP 2049 SlurmLoginNodeSG SlurmLoginNode NFS TCP 6819 SlurmLoginNodeSG SlurmLoginNode slurmdbd. If not using external Slurmdbd. TCP 6820-6829 SlurmLoginNodeSG SlurmLoginNode slurmctld TCP 6830 SlurmLoginNodeSG SlurmLoginNode slurmrestd It needs the following outbound rules. Type Port range Destination Description TCP 1024-65535 SlurmLoginNodeSG SlurmLoginNode ephemeral","title":"Slurm Head Node Security Group"},{"location":"security-groups/#slurm-compute-node-security-group","text":"The SlurmComputeNodeSG will be specified in your configuration file for the slurm/InstanceConfig/AdditionalSecurityGroups parameter. It needs at least the following inbound rules: Type Port range Source Description TCP 6818 SlurmLoginNodeSG SlurmLoginNode slurmd It needs the following outbound rules. Type Port range Destination Description TCP 2049 SlurmHeadNodeSG SlurmHeadNode NFS TCP 1024-65535 SlurmLoginNodeSG SlurmLoginNode ephemeral TCP 6000-7024 SlurmLoginNodeSG SlurmLoginNode X11","title":"Slurm Compute Node Security Group"},{"location":"security-groups/#external-slurmdbd-security-group","text":"Note : ParallelCluster 3.10.0 added support for an external Slurmdbd instance. The login node must be able to directly access the Slurmdbd instance on port 6819 when running commands like sacctmgr . You must edit the inbound rules of the Slurmdbd instance's security group to allow the access. Add the following inbound rule. Type Port range Source Description TCP 6819 SlurmLoginNodeSG SlurmLoginNode slurmdbd","title":"External Slurmdbd Security Group"},{"location":"security-groups/#security-groups-for-file-systems","text":"You will usually have externally created file systems that should be mounted on the compute nodes and login nodes. You will need to define security groups for the file system network interfaces and modify the Slurm security groups to give them access to the file systems.","title":"Security Groups for File Systems"},{"location":"security-groups/#fsx-for-lustre-security-group","text":"We'll refer to this group as FSxLustreSG, but you can name it whatever you want. This security group can either be provided when the file system is created, or can be attached to the network interfaces of the file system after it is created. The required security group rules are documented in the FSx documentation . It needs the following inbound rules. Type Port range Source Description Details TCP 988 FSxLustreSG, SlurmHeadNodeSG, SlurmComputeNodeSG, SlurmLoginNodeSG Allows Lustre traffic between FSx for Lustre file servers and Lustre clients TCP 1018-1023 FSxLustreSG, SlurmHeadNodeSG, SlurmComputeNodeSG, SlurmLoginNodeSG Allows Lustre traffic between FSx for Lustre file servers and Lustre clients It needs the following outbound rules. Type Port range Destination Description Details TCP 988 FSxLustreSG, SlurmHeadNodeSG, SlurmComputeNodeSG, SlurmLoginNodeSG Allow Lustre traffic between FSx for Lustre file servers and Lustre clients TCP 1018-1023 FSxLustreSG, SlurmHeadNodeSG, SlurmComputeNodeSG, SlurmLoginNodeSG Allow Lustre traffic between FSx for Lustre file servers and Lustre clients The same inbound and outbound rules need to be added to all 3 of the Slurm security groups too.","title":"FSx for Lustre Security Group"},{"location":"security-groups/#fsx-for-netapp-ontap-security-group","text":"We'll refer to this group as FSxOntapSG, but you can name it whatever you want. This security group can either be provided when the file system is created, or can be attached to the network interfaces of the file system after it is created. All the security group rule are documented in the FSx documentation . The minimum set required for mounting the file system are documented below. It needs the following inbound rules. Type Port range Source Description Details TCP, UDP 111 SlurmHeadNodeSG, SlurmComputeNodeSG, SlurmLoginNodeSG Remote procedure call for NFS TCP, UDP 635 SlurmHeadNodeSG, SlurmComputeNodeSG, SlurmLoginNodeSG NFS mount TCP, UDP 2049 SlurmHeadNodeSG, SlurmComputeNodeSG, SlurmLoginNodeSG NFS server daemon TCP, UDP 4045 SlurmHeadNodeSG, SlurmComputeNodeSG, SlurmLoginNodeSG NFS lock daemon TCP, UDP 4046 SlurmHeadNodeSG, SlurmComputeNodeSG, SlurmLoginNodeSG Network status monitor for NFS It needs the following outbound rules. Type Port range Destination Description Details All All The Slurm security groups need to add the following outbound rule to allow mounting using NFS. Type Port range Destination Description Details TCP, UDP 111 FSxOntap Remote procedure call for NFS TCP, UDP 635 FSxOntap NFS mount TCP, UDP 2049 FSxOntap NFS server daemon TCP,UDP 4045 FSxOntap NFS lock daemon TCP, UDP 4046 FSxOntap Network status monitor for NFS","title":"FSx for NetApp Ontap Security Group"},{"location":"security-groups/#fsx-for-openzfs-security-group","text":"We'll refer to this group as FSxZfsSG, but you can name it whatever you want. This security group can either be provided when the file system is created, or can be attached to the network interfaces of the file system after it is created. The required security group rule are documented in the FSx documentation . It needs the following inbound rules. Type Port range Source Description Details TCP, UDP 111 SlurmHeadNodeSG, SlurmComputeNodeSG, SlurmLoginNodeSG Remote procedure call for NFS TCP, UDP 2049 SlurmHeadNodeSG, SlurmComputeNodeSG, SlurmLoginNodeSG NFS server daemon TCP, UDP 20001-20003 SlurmHeadNodeSG, SlurmComputeNodeSG, SlurmLoginNodeSG NFS mount, status monitor, and lock daemon Remove all outbound rules. The Slurm security groups need to add the following outbound rule to allow mounting using NFS. Type Port range Destination Description Details TCP, UDP 111 FSxZfs Remote procedure call for NFS TCP, UDP 2049 FSxZfs NFS server daemon TCP, UDP 20001-20003 FSxZfs NFS mount, status monitor, and lock daemon","title":"FSx for OpenZFS Security Group"},{"location":"soca_integration/","text":"SOCA Integration Scale Out Computing on AWS (SOCA) is an AWS solution that was the basis for the Research and Engineering Studion (RES) service. Unless you are already a SOCA user, it is highly recommended that you use RES, which is a fully supported AWS service. Integration with SOCA is straightforward. Set the following parameters in your config file. Parameter Description Value VpcId VPC id for the SOCA cluster vpc-xxxxxx slurm/SlurmCtl/AdditionalSecurityGroups Security group ids that give desktop instances access to the head node and that give the head node access to VPC resources such as file systems. slurm/InstanceConfig/AdditionalSecurityGroups Security group ids that give desktop instances access to the compute nodes and that give compute nodes access to VPC resources such as file systems. ExtraMounts Add the mount parameters for the /apps and /data directories. This is required for access to the home directory. Deploy your slurm cluster. Connect to the SOCA Scheduler instance and follow the instructions to Create users_groups.json . Connect to a remote desktop instance and follow the instructions in Configure submission hosts to use the cluster . If all users need to use the cluster then it is probably best to create a custom AMI that is configured with the configuration commands. You are now ready to run jobs from your SOCA desktop.","title":"SOCA Integration"},{"location":"soca_integration/#soca-integration","text":"Scale Out Computing on AWS (SOCA) is an AWS solution that was the basis for the Research and Engineering Studion (RES) service. Unless you are already a SOCA user, it is highly recommended that you use RES, which is a fully supported AWS service. Integration with SOCA is straightforward. Set the following parameters in your config file. Parameter Description Value VpcId VPC id for the SOCA cluster vpc-xxxxxx slurm/SlurmCtl/AdditionalSecurityGroups Security group ids that give desktop instances access to the head node and that give the head node access to VPC resources such as file systems. slurm/InstanceConfig/AdditionalSecurityGroups Security group ids that give desktop instances access to the compute nodes and that give compute nodes access to VPC resources such as file systems. ExtraMounts Add the mount parameters for the /apps and /data directories. This is required for access to the home directory. Deploy your slurm cluster. Connect to the SOCA Scheduler instance and follow the instructions to Create users_groups.json . Connect to a remote desktop instance and follow the instructions in Configure submission hosts to use the cluster . If all users need to use the cluster then it is probably best to create a custom AMI that is configured with the configuration commands. You are now ready to run jobs from your SOCA desktop.","title":"SOCA Integration"}]}