<!DOCTYPE html>
<html lang="en" data-bs-theme="light">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../img/favicon.ico">
        <title>Exostellar Infrastructure Optimizer (XIO) - EDA SLURM Cluster on AWS</title>
        <link href="../css/bootstrap.min.css" rel="stylesheet">
        <link href="../css/fontawesome.min.css" rel="stylesheet">
        <link href="../css/brands.min.css" rel="stylesheet">
        <link href="../css/solid.min.css" rel="stylesheet">
        <link href="../css/v4-font-face.min.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">
        <link id="hljs-light" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" >
        <link id="hljs-dark" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github-dark.min.css" disabled>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/python.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/yaml.min.js"></script>
        <script>hljs.highlightAll();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-dark">
            <div class="container">
                <a class="navbar-brand" href="..">EDA SLURM Cluster on AWS</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-bs-toggle="collapse" data-bs-target="#navbar-collapse" aria-controls="navbar-collapse" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="nav-item">
                                <a href=".." class="nav-link">AWS EDA Slurm Cluster</a>
                            </li>
                            <li class="nav-item">
                                <a href="../deployment-prerequisites/" class="nav-link">Deployment Prerequisites</a>
                            </li>
                            <li class="nav-item">
                                <a href="../security-groups/" class="nav-link">Security Groups</a>
                            </li>
                            <li class="nav-item">
                                <a href="../deploy-parallel-cluster/" class="nav-link">Deploy AWS ParallelCluster</a>
                            </li>
                            <li class="nav-item">
                                <a href="../config/" class="nav-link">Configuraton File Format</a>
                            </li>
                            <li class="nav-item">
                                <a href="../res_integration/" class="nav-link">RES Integration</a>
                            </li>
                            <li class="nav-item">
                                <a href="../soca_integration/" class="nav-link">SOCA Integration</a>
                            </li>
                            <li class="nav-item">
                                <a href="../exostellar-workload-optimizer/" class="nav-link">Exostellar Workload Optimizer (XWO)</a>
                            </li>
                            <li class="nav-item">
                                <a href="./" class="nav-link active" aria-current="page">Exostellar Infrastructure Optimizer (XIO)</a>
                            </li>
                            <li class="nav-item">
                                <a href="../custom-amis/" class="nav-link">Custom AMIs for ParallelCluster</a>
                            </li>
                            <li class="nav-item">
                                <a href="../run_jobs/" class="nav-link">Run Jobs</a>
                            </li>
                            <li class="nav-item">
                                <a href="../job_preemption/" class="nav-link">Job Preemption</a>
                            </li>
                            <li class="nav-item">
                                <a href="../rest_api/" class="nav-link">Slurm REST API</a>
                            </li>
                            <li class="nav-item">
                                <a href="../onprem/" class="nav-link">On-Premises Integration</a>
                            </li>
                            <li class="nav-item">
                                <a href="../containers/" class="nav-link">Containers</a>
                            </li>
                            <li class="nav-item">
                                <a href="../delete-cluster/" class="nav-link">Delete Cluster</a>
                            </li>
                            <li class="nav-item">
                                <a href="../debug/" class="nav-link">Debug</a>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ms-md-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-bs-toggle="modal" data-bs-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../exostellar-workload-optimizer/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../custom-amis/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                            <li class="nav-item">
                                <a href="https://github.com/aws-samples/aws-eda-slurm-cluster/edit/master/docs/exostellar-infrastructure-optimizer.md" class="nav-link"><i class="fa-brands fa-github"></i> Edit on GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-bs-toggle="collapse" data-bs-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-body-tertiary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-bs-level="1"><a href="#exostellar-infrastructure-optimizer-xio" class="nav-link">Exostellar Infrastructure Optimizer (XIO)</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-bs-level="2"><a href="#xio-configuration" class="nav-link">XIO Configuration</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-bs-level="3"><a href="#deploy-parallelcluster-without-configuring-xio" class="nav-link">Deploy ParallelCluster without configuring XIO</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="3"><a href="#deploy-the-exostellar-management-server-ems" class="nav-link">Deploy the Exostellar Management Server (EMS)</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="3"><a href="#verify-that-the-az1-profile-exists" class="nav-link">Verify that the "az1" profile exists</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="3"><a href="#create-an-xio-parallelcluster-ami" class="nav-link">Create an XIO ParallelCluster AMI</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="3"><a href="#create-xio-configuration" class="nav-link">Create XIO Configuration</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="3"><a href="#update-the-cluster-with-the-xio-configuration" class="nav-link">Update the cluster with the XIO configuration</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="3"><a href="#create-an-xio-image-from-the-xio-parallelcluster-ami" class="nav-link">Create an XIO Image from the XIO ParallelCluster AMI</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="3"><a href="#test-launching-an-xio-vm" class="nav-link">Test launching an XIO VM</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="3"><a href="#run-a-test-job-using-slurm" class="nav-link">Run a test job using Slurm</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#debug" class="nav-link">Debug</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-bs-level="3"><a href="#how-to-connect-to-ems" class="nav-link">How to connect to EMS</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="3"><a href="#how-to-connect-to-controller" class="nav-link">How to connect to Controller</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="3"><a href="#updateheadnode-resource-failed" class="nav-link">UpdateHeadNode resource failed</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="3"><a href="#xio-controller-not-starting" class="nav-link">XIO Controller not starting</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="3"><a href="#worker-instance-not-starting" class="nav-link">Worker instance not starting</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="3"><a href="#vm-not-starting-on-worker" class="nav-link">VM not starting on worker</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="3"><a href="#vm-not-starting-slurm-job" class="nav-link">VM not starting Slurm job</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<h1 id="exostellar-infrastructure-optimizer-xio">Exostellar Infrastructure Optimizer (XIO)</h1>
<p><a href="https://exostellar.io/product/#infrastructureoptimizer">Exostellar Infrastructure Optimizer</a> (XIO) runs applications in virtual machines (VMs) on EC2 instances and can dynamically migrate the VMs between instances based on availability and cost.
Long-running, stateful jobs are not normally run on spot instances because of the risk of lost work after a spot termination.
XIO reduces this risk by predicting spot terminations and migrating the VM to another instance with higher availability.
This could be a different spot instance type or an on-demand instance.
When spot capacity becomes available again, the VM can be migrated back to a spot instance.
This allows you to save up to 90% over on-demand pricing by running on spot when capacity is available.
You increase the potential for savings by configuring as many spot capacity pools as possible.
This doesn't completely eliminate the risk of the job failing.
The job will still fail and need to be restarted from the beginning if a spot termination isn't predicted far enough in advance for the job to be migrated or if a new instance cannot be launched to migrate the job to.</p>
<p><strong>Note</strong>: Job reliability will be increased by following EC2 Spot best practices such as configuring as many capacity pools and instance types as possible.</p>
<p>XIO runs on an Exostellar Management Server (EMS).
The EMS runs a web application and launches and manages the instances that run jobs.
In response to job requests it launches controller nodes that manage pools of worker nodes.
The controller launches workers and then starts one or more VMs on the workers.
The controller also determines when VMs need to be migrated, allocates new workers, and manages the VM migrations.</p>
<p>You create an XIO Application Environment for each Slurm cluster.
The Application Environment contains the URL for the Slurm head node,
configures pools of VMs,
and configures the path to the Slurm binaries and configuration.
The VM pools define the attributes of the instances including the number of CPUs, VM Image, min and max memory, and an associated XIO Profile.</p>
<p>You must also create the XIO Profiles that are used by the VM Pools.
Each profile configures XIO Controllers and XIO Workers.
The Workers run the XIO VMs.
The Controller manages the workers and the VMs that run on them.
The Worker configuration includes the instance types to use for
on-demand and spot instances.
It also includes the security groups and tags for the worker instances.</p>
<p>You must also create XIO Images that are used to create the VMs.
The Images are created from AWS AMIs and are specified in the VM Pools.</p>
<p><strong>NOTE:</strong> One current restriction of XIO VMs is that they cannot be created from ParallelCluster AMIs.
This is because the kernel modules that ParallelCluster installs aren't supported by the XIO hypervisor.</p>
<h2 id="xio-configuration">XIO Configuration</h2>
<p>This section will describe the process of configuring XIO to work with ParallelCluster.</p>
<p>Refer to <a href="https://docs.exostellar.io/latest/Latest/HPC-User/getting-started-installation">Exostellar's documentation</a> to make sure you have the latest instructions.</p>
<h3 id="deploy-parallelcluster-without-configuring-xio">Deploy ParallelCluster without configuring XIO</h3>
<p>First deploy your cluster without configuring XIO.
The cluster deploys ansible playbooks that will be used to create the XIO ParallelCluster AMI.</p>
<h3 id="deploy-the-exostellar-management-server-ems">Deploy the Exostellar Management Server (EMS)</h3>
<p>The next step is to <a href="https://docs.exostellar.io/latest/Latest/HPC-User/installing-management-server">install the Exostellar management server</a>.
You must first subscribe to the three Exostellar Infrastructure AMIs in the AWS Marketplace.</p>
<ul>
<li><a href="https://aws.amazon.com/marketplace/server/procurement?productId=prod-crdnafbqnbnm2">Exostellar Management Server</a></li>
<li><a href="https://aws.amazon.com/marketplace/server/procurement?productId=prod-d4lifqwlw4kja">Exostellar Controller</a></li>
<li><a href="https://aws.amazon.com/marketplace/server/procurement?productId=prod-2smeyk5fuxt7q">Exostellar Worker</a></li>
</ul>
<p>Then follow the <a href="https://docs.exostellar.io/latest/Latest/HPC-User/installing-management-server#v2.4.0.0InstallingwithCloudFormationTemplate(AWS)-Step3:CreateaNewStack">directions to deploy the CloudFormation template</a>.</p>
<h3 id="verify-that-the-az1-profile-exists">Verify that the "az1" profile exists</h3>
<p>In the EMS GUI go to Profiles and make sure that the "az1" profile exists.
I use that as a template to create your new profiles.</p>
<p>If it doesn't exist, there was a problem with the EMS deployment and you should contact Exostellar support.</p>
<h3 id="create-an-xio-parallelcluster-ami">Create an XIO ParallelCluster AMI</h3>
<p>Launch an instance using the base AMI for your OS.
For example, launch an instance with a base RHEL 8 or Rocky 8 AMI.</p>
<p>Mount the ParallelCluster NFS file system at /opt/slurm.</p>
<p>Run the ansible playbook to configure the instance for XIO.</p>
<pre><code>/opt/slurm/config/bin/xio-compute-node-ami-configure.sh
</code></pre>
<p>Do any additional configuration that you require such as configuring file system mounts and installing
packages.</p>
<p>Create an AMI from the instance and wait for it to become available.</p>
<p>After the AMI has been successfully created you can either stop or terminate the instance to save costs.
If you may need to do additional customization, then stop it, otherwise terminate it.</p>
<p>Add the image id to your configuration as described below.</p>
<h3 id="create-xio-configuration">Create XIO Configuration</h3>
<p>The next step is to plan and configure your XIO deployment.
The key decisions that you must make are the instance types that you will use
and the AMI that you will use for the XIO VM Images.</p>
<p>XIO currently only supports x86_64 instance types and pools cannot mix AMD and Intel instance types.
The following XIO configuration for aws-eda-slurm-cluster shows 2 pools that contain Intel and AMD instances.
Note that we first define the XIO Profiles with instance types with the same manufacturer, number of cores, and amount of memory.
Then we configure pools for the Application Environment that use the profiles.
The numbers after the instance type are a priority to bias XIO to use higher priority instance types if they are available.
We've chosen to prioritize the latest generation instance types so our jobs run faster and configure
older generation instance types at a lower priority to increase the number of capacity pools so that
we have a better chance of running on spot and having instances to run our jobs.
Refer to <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-best-practices.html">Best practices for Amazon EC2 Spot</a> when planning your cluster deployment and creating your configuration.</p>
<p><strong>NOTE</strong>: XIO currently doesn't support VMs larger than 1 TB.</p>
<p>It is highly recommended to use <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/work-with-spot-placement-score.html">EC2 Spot placement scores</a> when selecting the region and availability zone for your cluster.
This will give you an indication of the likelihood of getting desired spot capacity.</p>
<p>In the following example I've configured a profile for AMD and Intel instance families.
I've included instance families from the last 3 generations of instances to maximize the number of
available capacity pools and increase the likelihood of running on spot.</p>
<p><strong>Note</strong>: The Intel instance families contain more configurations and higher memory instances. They also have high frequency instance types such as m5zn, r7iz, and z1d. They also tend to have more capacity. The AMD instance families include HPC instance types, however, they do not support spot pricing and can only be used for on-demand.</p>
<p><strong>Note</strong>: This is only an example configuration. You should customize it for your requirements.</p>
<pre><code>slurm:
  Xio:
    ManagementServerStackName: exostellar-management-server
    PartitionName: xio
    AvailabilityZone: us-east-2b

    Images:
      - ImageId: ami-xxxxxxxxxxxxxxxxx
        ImageName: &lt;your-xio-vm-image-name&gt;

    DefaultImageName: &lt;your-xio-vm-image-name&gt;
    Profiles:
      - ProfileName: amd
        NodeGroupName: amd
        MaxControllers: 10
        InstanceTypes:
          - c5a:1
          - c5ad:1
          - c6a:4
          - c7a:7

          - m5a:1
          - m5ad:1
          - m6a:4
          - m7a:7

          - r5a:1
          - r5ad:1
          - r6a:4
          - r7a:7
        SpotFleetTypes:
          - c5a:1
          - c5ad:1
          - c6a:4
          - c7a:7

          - m5a:1
          - m5ad:1
          - m6a:4
          - m7a:7

          - r5a:1
          - r5ad:1
          - r6a:4
          - r7a:7
        EnableHyperthreading: false

      - ProfileName: intel
        NodeGroupName: intel
        MaxControllers: 10
        InstanceTypes:
          - c5n:1
          - c5d:1
          - c5:1
          - c6in:4
          - c6id:4
          - c6i:4
          - c7i:7

          - m5:1
          - m5d:1
          - m5dn:1
          - m5n:1
          - m5zn:1
          - m6i:4
          - m6id:4
          - m6idn:4
          - m6in:4
          - m7i:7

          - r5:1
          - r5b:1
          - r5d:1
          - r5dn:1
          - r5n:1
          - r6i:4
          - r6id:4
          - r6idn:4
          - r6in:4
          - r7i:7
          - r7iz:7

          # - x2idn:1
          # - x2iedn:1

          - z1d:1
        SpotFleetTypes:
          - c5n:1
          - c5d:1
          - c5:1
          - c6in:4
          - c6id:4
          - c6i:4
          - c7i:7

          - m5:1
          - m5d:1
          - m5dn:1
          - m5n:1
          - m5zn:1
          - m6i:4
          - m6id:4
          - m6idn:4
          - m6in:4
          - m7i:7

          - r5:1
          - r5b:1
          - r5d:1
          - r5dn:1
          - r5n:1
          - r6i:4
          - r6id:4
          - r6idn:4
          - r6in:4
          - r7i:7
          - r7iz:7

          # - x2idn:1
          # - x2iedn:1

          - z1d:1
        EnableHyperthreading: false
    Pools:
      - PoolName: amd-8g-2c
        ProfileName: amd
        PoolSize: 10
        CPUs: 2
        InstanceMemory: 8192
      - PoolName: amd-8g-4c
        ProfileName: amd
        PoolSize: 10
        CPUs: 4
        InstanceMemory: 8192
      - PoolName: amd-16g-1c
        ProfileName: amd
        PoolSize: 10
        CPUs: 1
        InstanceMemory: 16384
      - PoolName: amd-16g-2c
        ProfileName: amd
        PoolSize: 10
        CPUs: 2
        InstanceMemory: 16384
      - PoolName: amd-16g-4c
        ProfileName: amd
        PoolSize: 10
        CPUs: 4
        InstanceMemory: 16384
      - PoolName: amd-16g-8c
        ProfileName: amd
        PoolSize: 10
        CPUs: 8
        InstanceMemory: 16384
      - PoolName: amd-32g-2c
        ProfileName: amd
        PoolSize: 10
        CPUs: 2
        InstanceMemory: 32768
      - PoolName: amd-32g-4c
        ProfileName: amd
        PoolSize: 10
        CPUs: 4
        InstanceMemory: 32768
      - PoolName: amd-32g-8c
        ProfileName: amd
        PoolSize: 10
        CPUs: 8
        InstanceMemory: 32768
      - PoolName: amd-64g-4c
        ProfileName: amd
        PoolSize: 10
        CPUs: 4
        InstanceMemory: 65536
      - PoolName: amd-64g-8c
        ProfileName: amd
        PoolSize: 10
        CPUs: 8
        InstanceMemory: 65536
      - PoolName: amd-64g-16c
        ProfileName: amd
        PoolSize: 10
        CPUs: 16
        InstanceMemory: 65536
      - PoolName: amd-64g-32c
        ProfileName: amd
        PoolSize: 10
        CPUs: 32
        InstanceMemory: 65536
      - PoolName: amd-128g-8c
        ProfileName: amd
        PoolSize: 10
        CPUs: 8
        InstanceMemory: 131072
      - PoolName: amd-128g-16c
        ProfileName: amd
        PoolSize: 10
        CPUs: 16
        InstanceMemory: 131072
      - PoolName: amd-128g-32c
        ProfileName: amd
        PoolSize: 10
        CPUs: 32
        InstanceMemory: 131072
      - PoolName: amd-128g-64c
        ProfileName: amd
        PoolSize: 10
        CPUs: 64
        InstanceMemory: 131072
      - PoolName: amd-192g-24c
        ProfileName: amd
        PoolSize: 10
        CPUs: 24
        InstanceMemory: 196608
      - PoolName: amd-192g-48c
        ProfileName: amd
        PoolSize: 10
        CPUs: 48
        InstanceMemory: 196608
      - PoolName: amd-256g-16c
        ProfileName: amd
        PoolSize: 10
        CPUs: 16
        InstanceMemory: 262144
      - PoolName: amd-256g-32c
        ProfileName: amd
        PoolSize: 10
        CPUs: 32
        InstanceMemory: 262144
      - PoolName: amd-256g-64c
        ProfileName: amd
        PoolSize: 10
        CPUs: 64
        InstanceMemory: 262144
      - PoolName: amd-256g-128c
        ProfileName: amd
        PoolSize: 10
        CPUs: 128
        InstanceMemory: 262144
      - PoolName: amd-384g-24c
        ProfileName: amd
        PoolSize: 10
        CPUs: 24
        InstanceMemory: 393216
      - PoolName: amd-384g-48c
        ProfileName: amd
        PoolSize: 10
        CPUs: 48
        InstanceMemory: 393216
      - PoolName: amd-384g-96c
        ProfileName: amd
        PoolSize: 10
        CPUs: 96
        InstanceMemory: 393216
      - PoolName: amd-384g-192c
        ProfileName: amd
        PoolSize: 10
        CPUs: 192
        InstanceMemory: 393216
      - PoolName: amd-512g-32c
        ProfileName: amd
        PoolSize: 10
        CPUs: 32
        InstanceMemory: 524288
      - PoolName: amd-512g-64c
        ProfileName: amd
        PoolSize: 10
        CPUs: 64
        InstanceMemory: 524288
      - PoolName: amd-512g-128c
        ProfileName: amd
        PoolSize: 10
        CPUs: 128
        InstanceMemory: 524288
      - PoolName: amd-768g-48c
        ProfileName: amd
        PoolSize: 10
        CPUs: 48
        InstanceMemory: 786432
      - PoolName: amd-768g-96c
        ProfileName: amd
        PoolSize: 10
        CPUs: 96
        InstanceMemory: 786432
      - PoolName: amd-768g-192c
        ProfileName: amd
        PoolSize: 10
        CPUs: 192
        InstanceMemory: 786432
      - PoolName: amd-1024g-64c
        ProfileName: amd
        PoolSize: 10
        CPUs: 64
        InstanceMemory: 1048576
      - PoolName: amd-1536g-96c
        ProfileName: amd
        PoolSize: 10
        CPUs: 96
        InstanceMemory: 1572864
      - PoolName: amd-1536g-192c
        ProfileName: amd
        PoolSize: 10
        CPUs: 192
        InstanceMemory: 1572864

      - PoolName: intel-8g-1c
        ProfileName: intel
        PoolSize: 10
        CPUs: 1
        InstanceMemory: 8192
      - PoolName: intel-8g-2c
        ProfileName: intel
        PoolSize: 10
        CPUs: 2
        InstanceMemory: 8192
      - PoolName: intel-16g-1c
        ProfileName: intel
        PoolSize: 10
        CPUs: 1
        InstanceMemory: 16384
      - PoolName: intel-16g-2c
        ProfileName: intel
        PoolSize: 10
        CPUs: 2
        InstanceMemory: 16384
      - PoolName: intel-16g-4c
        ProfileName: intel
        PoolSize: 10
        CPUs: 4
        InstanceMemory: 16384
      - PoolName: intel-32g-2c
        ProfileName: intel
        PoolSize: 10
        CPUs: 2
        InstanceMemory: 32768
      - PoolName: intel-32g-4c
        ProfileName: intel
        PoolSize: 10
        CPUs: 4
        InstanceMemory: 32768
      - PoolName: intel-32g-8c
        ProfileName: intel
        PoolSize: 10
        CPUs: 8
        InstanceMemory: 32768
      - PoolName: intel-48g-6c
        ProfileName: intel
        PoolSize: 10
        CPUs: 6
        InstanceMemory: 49152
      - PoolName: intel-64g-4c
        ProfileName: intel
        PoolSize: 10
        CPUs: 4
        InstanceMemory: 65536
      - PoolName: intel-64g-8c
        ProfileName: intel
        PoolSize: 10
        CPUs: 8
        InstanceMemory: 65536
      - PoolName: intel-64g-16c
        ProfileName: intel
        PoolSize: 10
        CPUs: 16
        InstanceMemory: 65536
      - PoolName: intel-72g-18c
        ProfileName: intel
        PoolSize: 10
        CPUs: 18
        InstanceMemory: 73728
      - PoolName: intel-96g-6c
        ProfileName: intel
        PoolSize: 10
        CPUs: 6
        InstanceMemory: 98304
      - PoolName: intel-96g-12c
        ProfileName: intel
        PoolSize: 10
        CPUs: 12
        InstanceMemory: 98304
      - PoolName: intel-96g-24c
        ProfileName: intel
        PoolSize: 10
        CPUs: 12
        InstanceMemory: 98304
      # - PoolName: intel-128g-2c # x2iedn.xlarge
      #   ProfileName: intel
      #   PoolSize: 10
      #   CPUs: 2
      #   InstanceMemory: 131072
      - PoolName: intel-128g-8c
        ProfileName: intel
        PoolSize: 10
        CPUs: 8
        InstanceMemory: 131072
      - PoolName: intel-128g-16c
        ProfileName: intel
        PoolSize: 10
        CPUs: 16
        InstanceMemory: 131072
      - PoolName: intel-128g-32c
        ProfileName: intel
        PoolSize: 10
        CPUs: 32
        InstanceMemory: 131072
      - PoolName: intel-144g-36c # c5[d].18xlarge
        ProfileName: intel
        PoolSize: 10
        CPUs: 36
        InstanceMemory: 147456
      - PoolName: intel-192g-12c
        ProfileName: intel
        PoolSize: 10
        CPUs: 12
        InstanceMemory: 196608
      - PoolName: intel-192g-24c
        ProfileName: intel
        PoolSize: 10
        CPUs: 24
        InstanceMemory: 196608
      - PoolName: intel-192g-48c
        ProfileName: intel
        PoolSize: 10
        CPUs: 48
        InstanceMemory: 196608
      # - PoolName: intel-256g-4c # x2iedn.2xlarge
      #   ProfileName: intel
      #   PoolSize: 10
      #   CPUs: 4
      #   InstanceMemory: 262144
      - PoolName: intel-256g-16c
        ProfileName: intel
        PoolSize: 10
        CPUs: 16
        InstanceMemory: 262144
      - PoolName: intel-256g-32c
        ProfileName: intel
        PoolSize: 10
        CPUs: 32
        InstanceMemory: 262144
      - PoolName: intel-256g-64c
        ProfileName: intel
        PoolSize: 10
        CPUs: 64
        InstanceMemory: 262144
      - PoolName: intel-384g-24c
        ProfileName: intel
        PoolSize: 10
        CPUs: 24
        InstanceMemory: 393216
      - PoolName: intel-384g-48c
        ProfileName: intel
        PoolSize: 10
        CPUs: 48
        InstanceMemory: 393216
      - PoolName: intel-384g-96c
        ProfileName: intel
        PoolSize: 10
        CPUs: 96
        InstanceMemory: 393216
      # - PoolName: intel-512g-8c # x2iedn.4xlarge
      #   ProfileName: intel
      #   PoolSize: 10
      #   CPUs: 8
      #   InstanceMemory: 524288
      - PoolName: intel-512g-32c
        ProfileName: intel
        PoolSize: 10
        CPUs: 32
        InstanceMemory: 524288
      - PoolName: intel-512g-64c
        ProfileName: intel
        PoolSize: 10
        CPUs: 64
        InstanceMemory: 524288
      - PoolName: intel-768g-48c
        ProfileName: intel
        PoolSize: 10
        CPUs: 48
        InstanceMemory: 786432
      - PoolName: intel-768g-96c
        ProfileName: intel
        PoolSize: 10
        CPUs: 96
        InstanceMemory: 786432
      # - PoolName: intel-1024g-16c # x2iedn.8xlarge
      #   ProfileName: intel
      #   PoolSize: 10
      #   CPUs: 16
      #   InstanceMemory: 1048576
      # - PoolName: intel-1024g-32c # x2idn.16xlarge
      #   ProfileName: intel
      #   PoolSize: 10
      #   CPUs: 32
      #   InstanceMemory: 1048576
      - PoolName: intel-1024g-64c
        ProfileName: intel
        PoolSize: 10
        CPUs: 64
        InstanceMemory: 1048576
</code></pre>
<h3 id="update-the-cluster-with-the-xio-configuration">Update the cluster with the XIO configuration</h3>
<p>Update the cluster with the XIO configuration.</p>
<p>This will update the profiles and environment on the EMS server and configure the cluster for XIO.
The only remaining step before you can submit jobs is to create the XIO VM image.</p>
<p>This is done before creating an image because the XIO scripts get deployed by this step.</p>
<h3 id="create-an-xio-image-from-the-xio-parallelcluster-ami">Create an XIO Image from the XIO ParallelCluster AMI</h3>
<p>Connect to the head node and create the XIO Image from the AMI you created.
The IMAGE-NAME should be the same that you configured in the Pools.</p>
<pre><code>/opt/slurm/etc/exostellar/parse_helper.sh -a &lt;AMI-ID1&gt; -i &lt;IMAGE-NAME&gt;
</code></pre>
<h3 id="test-launching-an-xio-vm">Test launching an XIO VM</h3>
<p>Connect to the head node and test launching a VM.
The pool, profile, and image_name should be from your configuration.
The host name doesn't matter.</p>
<pre><code>/opt/slurm/etc/exostellar/test_createVm.sh --pool &lt;pool&gt; --profile &lt;profile&gt; -i &lt;image name&gt; -h &lt;host&gt;
</code></pre>
<p>When this is done, the VM, worker, and controller should all terminate on their own.
If they do not, then connect to the EMS and cancel the job that started the controller.</p>
<p>Use <code>squeue</code> to list the controller jobs. Use <code>scancel</code> to terminate them.</p>
<h3 id="run-a-test-job-using-slurm">Run a test job using Slurm</h3>
<pre><code>srun --pty -p xio-
</code></pre>
<h2 id="debug">Debug</h2>
<h3 id="how-to-connect-to-ems">How to connect to EMS</h3>
<p>Use ssh to connect to the EMS using your EC2 keypair.</p>
<ul>
<li><code>ssh-add private-key.pem</code></li>
<li><code>ssh -A rocky@${EMS_IP_ADDRESS}</code></li>
</ul>
<p>You can <a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/agent-install-rocky.html">install the aws-ssm-agent</a> so that you can connect from the EC2 console using SSM.</p>
<h3 id="how-to-connect-to-controller">How to connect to Controller</h3>
<ul>
<li>First ssh to the EMS.</li>
<li>Get the IP address of the controller from the EC2 console</li>
<li>As root, ssh to the controller</li>
</ul>
<h3 id="updateheadnode-resource-failed">UpdateHeadNode resource failed</h3>
<p>If the UpdateHeadNode resource fails then it is usually because as task in the ansible script failed.
Connect to the head node and look for errors in:</p>
<p><code>/var/log/ansible.log</code></p>
<p>Usually it will be a problem with the <code>/opt/slurm/etc/exostellar/configure_xio.py</code> script.</p>
<p>When this happens the CloudFormation stack will usually be in UPDATE_ROLLBACK_FAILED status.
Before you can update it again you will need to complete the rollback.
Go to Stack Actions, select <code>Continue update rollback</code>, expand <code>Advanced troubleshooting</code>, check the UpdateHeadNode resource, anc click <code>Continue update rollback</code>.</p>
<p>The problem is usually that there is an XWO controller running that is preventing updates to
the profile.
Cancel any XWO jobs and terminate any running workers and controllers and verify that all of the XWO profiles are idle.</p>
<h3 id="xio-controller-not-starting">XIO Controller not starting</h3>
<p>On EMS, check that a job is running to create the controller.</p>
<p><code>squeue</code></p>
<p>On EMS, check the autoscaling log to see if there are errors starting the instance.</p>
<p><code>less /var/log/slurm/autoscaling.log</code></p>
<p>EMS Slurm partitions are at:</p>
<p><code>/xcompute/slurm/bin/partitions.json</code></p>
<p>They are derived from the partition and pool names.</p>
<h3 id="worker-instance-not-starting">Worker instance not starting</h3>
<h3 id="vm-not-starting-on-worker">VM not starting on worker</h3>
<p>Connect to the controller instance and run the following command to get a list of worker instances and VMs.</p>
<pre><code>xspot ps
</code></pre>
<p>Connect to the worker VM using the following command.</p>
<pre><code>xspot console vm-abcd
</code></pre>
<p>This will show the console logs.
If you configured the root password then you can log in as root to do further debug.</p>
<h3 id="vm-not-starting-slurm-job">VM not starting Slurm job</h3>
<p>Connect to the VM as above.</p>
<p>Check /var/log/slurmd.log for errors.</p></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script src="../js/bootstrap.bundle.min.js"></script>
        <script>
            var base_url = "..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../js/base.js"></script>
        <script src="../search/main.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
