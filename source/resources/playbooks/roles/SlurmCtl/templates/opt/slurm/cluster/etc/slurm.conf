# Slurm configuration file
#
# You can create a config file using the configurator at:
# https://slurm.schedmd.com/configurator.html
#
# slurm.conf file generated by configurator.html.
#
# See the slurm.conf man page for more information.
# https://slurm.schedmd.com/slurm.conf.html
#
# This cluster is a high throughput cluster that submits jobs
# very quickly some of which are short.
# The following page documents configuration recommendations for a
# high throughput cluster:
# https://slurm.schedmd.com/high_throughput.html
# Configuration changes on slurmctl
# * /proc/sys/fs/file-max: Recommended limit 0f 32,832. Current value: 39,148,160
# * /proc/sys/net/ipv4/tcp_max_syn_backlog: Current value: 2048. Increased to 4096
# * /proc/sys/net/ipv4/tcp_syncookies: must be set to 1. Is.
# * /proc/sys/net/ipv4/tcp_synack_retries: Set to 5 as recommended
# * /proc/sys/net/core/somaxconn: Defaults to 128. Increase to 4096.
# * ifconfig eth0 txqueuelen 4096 (up from 1000)

ClusterName={{ClusterName}}
{% for i in range(1, NumberOfControllers|int + 1) %}
SlurmctldHost = {{SlurmCtlBaseHostname + i|string}}.{{Domain}}
{% endfor %}

# CommunicationParameters:
# NoAddrCache: Do not assume that nodes will retain their IP addresses. Do not cache node->ip mapping
CommunicationParameters = NoAddrCache
#
Epilog={{SlurmScriptsDir}}/epilog.sh
EpilogSlurmctld={{SlurmScriptsDir}}/slurmctld-epilog.sh
JobRequeue=1
JobSubmitPlugins=defaults
LaunchParameters=enable_nss_slurm
MpiDefault=none
ProctrackType=proctrack/cgroup
Prolog={{SlurmScriptsDir}}/prolog.sh
PrologSlurmctld={{SlurmScriptsDir}}/slurmctld-prolog.sh
# ReturnToService
# 0: Make node available if it registers with a valid configuration regardless of why it is down.
# 1: Only return to service if DOWN due to being non-responsive.
# 2: A DOWN node will become available for use upon registration with a valid configuration.
ReturnToService=2
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmctldPort=6817
SlurmdPidFile=/var/run/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurmd
SlurmUser=slurm
SrunEpilog={{SlurmScriptsDir}}/srun-epilog.sh
SrunProlog={{SlurmScriptsDir}}/srun-prolog.sh
StateSaveLocation={{SlurmVarDir}}/spool
SwitchType=switch/none
TaskEpilog={{SlurmScriptsDir}}/task-epilog.sh
TaskPlugin=task/cgroup
TaskProlog={{SlurmScriptsDir}}/task-prolog.sh
# TreeWidth is set to the maximum for cloud clusters so messages go directly between controller and nodes.
# https://slurm.schedmd.com/elastic_computing.html
TreeWidth = 65533
#
#
#
# TIMERS
InactiveLimit=0
KillWait=30
#
# MessageTimeout
# Prevent error: slurm_receive_msgs: Socket timed out on send/recv operation
# See https://bugs.schedmd.com/show_bug.cgi?id=1806
# Default is 10s. Increase to 30s first and then 60s if that doesn't solve the problem.
# Still get timeouts with 60. Blast it to 180.
MessageTimeout=60
MinJobAge=300
SlurmctldTimeout=300
SlurmdTimeout=600
Waittime=0
#
#
# SCHEDULING
#
# SchedulerType
# * backfill: (default) Augments default FIFO scheduling. Dependent on users specifying job time limits
#      Backfill scheduling is a time consuming operation.
# * builtin: Strict priority order within each partition
# builtin is more light weight, but when using that then job scheduling is strictly FIFO so if a job
# is pending for license, ICE, etc. then lower priority jobs will also pend.
SchedulerType=sched/backfill
#
# SchedulerParameters
# batch_sched_delay: How long, in seconds, scheduling of batch jobs can be delayed.
#     Required in high throughput cluster.
# bf_continue: cause the backfill scheduler to continue processing pending jobs
#     from its original job list after releasing locks even if job or node state changes
# bf_interval: The number of seconds between backfill iterations. Higher values
#     result in less overhead and better responsiveness.
# bf_max_job_test: The maximum number of jobs to attempt backfill scheduling for.
# bf_max_job_user: The maximum number of jobs per user to attempt starting with
#     the backfill scheduler for ALL partitions. One can set this limit to prevent
#     users from flooding the backfill queue with jobs that cannot start and that
#     prevent jobs from other users to start.
#     Set bf_max_job_test to a value much higher than bf_max_job_user.
# bf_max_time: The maximum time in seconds the backfill scheduler can spend.
#     Default: bf_interval
# bf_yield_interval: The backfill scheduler will periodically relinquish locks in
#     order for other pending operations to take place. This specifies the times
#     when the locks are relinquished in microseconds. Smaller values may be helpful
#     for high throughput computing when used in conjunction with the bf_continue option.
#     Default: 2,000,000 (2 sec)
# bf_yield_sleep: The length of time for which the locks are relinquished in microseconds.
# default_queue_depth: The default number of jobs to attempt scheduling (i.e. the queue depth)
#     when a running job completes or other routine actions occur.
#      The full queue will be tested on a less frequent basis as defined by the sched_interval option described below.
#     If set to default of 100 then it will stop scheduling jobs and
#     starting new nodes for them and jobs will get stuck in PENDING state.
# defer: Setting this option will avoid attempting to schedule each job individually at job submit time.
#     This breaks srun.
# max_rpc_cnt: Helps prevent message timeouts
# sched_min_internal: How frequently, in microseconds, the main scheduling loop will execute and test any pending jobs.
#     This is set to 2s because this is a high thoroughput cluster and setting it higher
#     keeps the cluster responsive to incoming requests.
SchedulerParameters=\
batch_sched_delay=10\
,bf_continue\
,bf_interval=30\
,bf_max_job_test=500\
,bf_max_job_user=0\
,bf_yield_interval=1000000\
,default_queue_depth=10000\
,max_rpc_cnt=100\
,preempt_youngest_first\
,sched_min_internal=2000000
#
SelectType = select/cons_res
SelectTypeParameters = CR_CPU_MEMORY
#
# Enable the use of scrontab to submit and manage periodic repeating jobs.
ScronParameters = enable
#
# Preemption
# https://slurm.schedmd.com/preempt.html
# PreemptMode:
#     Values: OFF, CANCEL, GANG, REQUEUE, SUSPEND
PreemptMode = {{PreemptMode}}
PreemptType = {{PreemptType}}
PreemptExemptTime = {{PreemptExemptTime}}
#
# JOB PRIORITY
#
# PriorityType
# Default is priority/basic
# Using multifactor so the interactive partition has priority over the regress partitiion
# https://slurm.schedmd.com/priority_multifactor.html
{% if UseAccountingDatabase %}
# Multifactor uses 9 factors:
# * age
# * association
# * fair-share
# * job size
# * nice
# * partition
# * QOS
# * site
# * TRES
PriorityType=priority/multifactor
PriorityWeightPartition=100000
PriorityWeightFairshare=10000
PriorityWeightQOS=10000
PriorityWeightAge=1000
PriorityWeightAssoc=0
PriorityWeightJobSize=0
{% else %}
PriorityType=priority/basic
{% endif %}
#
#
# LOGGING
#
# SlurmctldDebug: quiet, fatal, error, info, verbose, debug, debug2, debug3, debug4, debug5
SlurmctldDebug=info
SlurmctldLogFile=/var/log/slurm/slurmctld.log
#
# SlurmctldDebug: quiet, fatal, error, info, verbose, debug, debug2, debug3, debug4, debug5
SlurmdDebug=info
SlurmdLogFile=/var/log/slurm/slurmd.log
#
# DebugFlags: Defines specific subsystems which should provide more detailed event logging.
# NO_CONF_HASH: Do not log when the slurm.conf files differ between Slurm daemons
# Script: Debug info regarding the process that runs slurmctld scripts such as PrologSlurmctld and EpilogSlurmctl
DebugFlags=\
NO_CONF_HASH\
,Script\
,Elasticsearch\
,Federation\
,License
#
#
#
# ACCOUNTING
#
{% if UseAccountingDatabase %}
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost={{AccountingStorageHost}}
AccountingStoragePort=6819
#
# AccountingStorageTRES
# Comma-separated list of resources you wish to track on the cluster.
# By default Billing, CPU, Energy, Memory, Node, FS/Disk, Pages and VMem are tracked.
#AccountingStorageTRES=
#
# AccountingStoreFlags
# Comma separated list used to tell the slurmctld to store extra fields that may be more heavy weight than the normal job information.
# job_comment: Include the job's comment field in the job complete message sent to the Accounting Storage database.
# job_env:
# job_script:
AccountingStoreFlags=job_comment
JobAcctGatherType = jobacct_gather/linux
{% else %}
AccountingStorageType=accounting_storage/none
{% endif %}
#
#
# JobCompType: The job completion logging mechanism type.
#     The information captured is redundant with the Accounting database
#     configured by AccountingStorageType.
# jobcomp/none: Upon job completion, a record of the job is purged from the system.
# jobcomp/elasticsearch:
#     Upon job completion, a record of the job should be written to an Elasticsearch
#     server, specified by the JobCompLoc parameter.
#     See https://slurm.schedmd.com/elasticsearch.html
# jobcomp/filetxt: Upon job completion, a record of the job should be written to a
#     text file, specified by the JobCompLoc parameter.
# jobcomp/lua: a record of the job should be processed by the jobcomp.lua scrip
# jobcomp/mysql: A record of the job should be written to a MySQL or MariaDB
#     database, specified by the JobCompLoc parameter.
# jobcomp/script: Upon job completion, a script specified by the JobCompLoc parameter
#     is to be executed with environment variables providing the job information.
JobCompType={{JobCompType}}
{% if JobCompType == "jobcomp/elasticsearch" %}
JobCompLoc = {{JobCompLoc}}
JobCompParams = timeout=5,connect_timeout=5
{% endif %}
{% if JobCompType == "jobcomp/filetxt" %}
JobCompLoc={{SlurmAccountingDir}}/job_accounting.txt
{% endif %}
#

#
# SlurmctldParameters
# cloud_dns: Do not set cloud_dns unless joining the domain and adding nodes to DNS
#     Joining the domain overloads the DCs when lots of nodes are started so
#     not joining the domain or using cloud_dns.
# idle_on_node_suspend: Mark nodes as idle, regardless of current state, when suspending
#     nodes with SuspendProgram so that nodes will be eligible to be resumed at a later time.
# node_reg_mem_percent: Percentage of memory a node is allowed to register with without being marked as invalid with low memory.
#     Use this so can configure nodes with 100% of their memory.
#     Without this option the node will fail because the system uses some of the memory.
SlurmctldParameters=\
idle_on_node_suspend,\
,node_reg_mem_percent=90
#
# Allow users to see state of nodes that are powered down
PrivateData = cloud
#
FederationParameters=fed_display
#
#
# POWER SAVE SUPPORT FOR IDLE NODES
#
{% if SuspendAction == "stop" %}
SuspendProgram = {{SlurmScriptsDir}}/slurm_ec2_stop.py
{% else %}
SuspendProgram = {{SlurmScriptsDir}}/slurm_ec2_terminate.py
{% endif %}
ResumeProgram = {{SlurmScriptsDir}}/slurm_ec2_resume.py
ResumeFailProgram = {{SlurmScriptsDir}}/slurm_ec2_resume_fail.py
# Maximum time between when a node is suspended and when suspend is complete.
# At that time it should be ready to be resumed.
SuspendTimeout = 60
ResumeTimeout = 600
# Number of nodes per minute that can be resumed or suspended
ResumeRate = 300
SuspendRate = 60
# Time that a node has to be idle or down before being suspended
# Should be >= (SuspendTimeout + ResumeTimeout)
SuspendTime = 660

include slurm_licenses.conf
include slurm_nodes.conf
{% if ON_PREM_COMPUTE_NODES_CONFIG is defined %}
include {{ON_PREM_COMPUTE_NODES_CONFIG}}
{% endif %}
include slurm_tres.conf
